% I begin this version on February 3 2012
% on the base of the lecture course of the previous year
% today 1-st February I am working on the version for 2013 year.
% today 31-sr January I am working on the version for 2014 year.
% today 31-sr January I am working on the version for 2015 year.
\def\vare {\varlepsilon}
\def\A {{\bf A}}
\def\c{{\bf c}}
\def\t {\tilde}
\def\a {\alpha}
\def\ac{{\bf a}}
\def\K {{\bf K}}
\def\N {{\bf N}}
\def\V {{\cal V}}
\def\s {{\sigma}}
\def\S {{\Sigma}}
\def\s {{\sigma}}
\def\p{\partial}
\def\pt {{\bf p}}
\def\vare{{\varepsilon}}
\def\Q {{\bf Q}}
\def\D {{\cal D}}
\def\G {{\Gamma}}
\def\C {{\bf C}}
\def\M {{\cal M}}
\def\Z {{\bf Z}}
\def\U  {{\cal U}}
\def\H {{\cal H}}
\def\R  {{\bf R}}
\def\E  {{\bf E}}
\def\l {\lambda}
\def\degree {{\bf {\rm degree}\,\,}}
\def \finish {${\,\,\vrule height1mm depth2mm width 8pt}$}
\def \m {\medskip}
\def\p {\partial}
\def\r {{\bf r}}
\def\v {{\bf v}}
\def\n {{\bf n}}
%\def\t {{\bf t}}
\def\b {{\bf b}}
\def\e{{\bf e}}
\def\f{{\bf f}}
\def\g{{\bf g}}
\def\ac {{\bf a}}
\def \X   {{\bf X}}
\def \Y   {{\bf Y}}
\def \x   {{\bf x}}
\def \y   {{\bf y}}
\def \z   {{\bf z}}
\def \f   {{\bf f}}
\def \l   {{\bf l}}
\def\w{{\omega}}
\def \k   {{\bf \kappa}}
\def\la {\langle}
\def \ra {\rangle}
\def \( {\langle}
\def \) {\rangle}




\documentclass[12pt]{article}
\usepackage{amsmath,amsthm}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
%\theoremstyle{theorem}
%\newtheorem{thm}{Khimera}



\numberwithin{equation}{section}

\title{Introduction to  Geometry}
\date{}
\begin{document}
\maketitle

  \centerline {it is a draft of lecture notes of H.M. Khudaverdian.}

  \centerline { Manchester, 12 February  2015}

\tableofcontents
\pagenumbering{roman}

\newpage
%\setcounter{page}{3}
\pagenumbering{arabic}
\section {Euclidean space}

We recall important notions from linear algebra.


\subsection {Vector space.}


Vector space $V$ on real numbers is a set of vectors with operations
$"+"$---addition of vector and $"\cdot"$---multiplication of vector
Lon real number (sometimes called coefficients, scalars). These
operations obey the following axioms


\begin{itemize}
 \item  $\forall \ac,\b\in V, \ac+\b\in V$,

 \item $\forall \lambda\in \R, \forall \ac\in V, \lambda \ac\in V$.

\item
  $\forall \ac,\b \ac+\b=\b+\ac$  (commutativity)
\item
$\forall \ac,\b,\c, \,\,\ac+(\b+\c)=(\ac+\b)+\c$ (associativity)

\item  $\exists\,\, {\mathbf 0}$ such that $\forall \ac$,
 $\ac+{\mathbf 0}=\ac$

\item  $\forall \ac$ there exists a vector $-\ac$ such that $\ac+(-\ac)=0$.

\item

  $\forall\lambda\in \R, \lambda(\ac+\b)=\lambda \ac+\lambda \b$

  \item

    $\forall\lambda,\mu\in \R (\lambda+\mu)\ac=\lambda\ac+\mu\ac$

    \item $(\lambda\mu)\ac=\lambda(\mu\ac)$

    \item $1\ac=\ac$

\end{itemize}
It follows from these axioms  that in particularly $\mathbf 0$ is unique and $-\ac$ is uniquely defined
by $\ac$. (Prove it.)


\smallskip

{\bf Remark} We denote by $0$ real number  $0$ and {\it vector }
$\bf 0$. Sometimes we have to be careful to distinguish
between zero  vector $\bf 0$ and number zero.

\smallskip

 Examples of vector spaces\ldots
 Consider now just one non-trivial example:
a space of polynomials of order $\leq 2$:
            $$
   V=\{ax^2+bx+c, a,b,c\in \R\}\,.
            $$
It is easy to see that polynomials are `vectors' with respect to 
operation of addition and multiplication on numbers.

 Consider {\bf conterexample}: 
a space of polynomials of order $2$ such that leading coefficient 
is equal to $1$:
            $$
   V=\{x^2+bx+c, a,b,c\in \R\}\,.
            $$
This is not vcto space: why? since the for any two polynomials $f,g$ from
thsi space the polynomials $f-g$, $f+g$ 
{\it does not belong to this space.}

\subsection {Basic example of ($n$-dimensional) vector space---$\R^n$}

 A basic example of vector space (over real numbers) is a space
  of ordered $n$-tuples of real numbers.


\noindent  $\R^2$ is a space of pairs of real numbers. $\R^2=\{(x,y),\,\,x,y\in \R \}$

\noindent    $\R^3$ is a space of triples  of real numbers. $\R^3=\{(x,y,z),\,\,x,y,z\in \R \}$

\noindent    $\R^4$ is a space of quadruples  of real numbers. $\R^4=\{(x,y,z,t),\,\,x,y,z,t,\in \R \}$

  \centerline {and so on...}
  $\R^n$---is a space of $n$-typles of real numbers:
                    \begin{equation}\label{basisexampleofvectorspace}
         \R^n=\{(x^1,x^2,\dots,x^n),\,\,x^1,\dots,,x^n\in \R \}
                     \end{equation}
  If  $\x,\y\in \R^n$ are two vectors, $\x=(x^1,\dots,x^n)$, $\y=(y^1,\dots,y^n)$
then             $$
        \x+\y=(x_1+y_1,\dots,x_n+y_n)\,.
                 $$ and
multiplication on scalars is defined as
           $$
\lambda\x=\lambda \cdot (x^1,\dots,x^n)=(\lambda x^1,\dots,\lambda x^n)\,,\quad (\lambda\in \R)\,.
           $$
($\lambda\in \R$).

\m
{\bf Remark}  Why $\R^n$ is 
$n$-dimensional vector space? We see it later in the subsection \ref{basis}

\subsection{Linear dependence of vectors}



We often consider linear combinations  in vector space:
           \begin{equation}\label{linearcombination}
            \sum_i\lambda_i\x_i=\lambda_1\x_1+\lambda_2\x_2+\dots+\lambda_m\x_m\,,
           \end{equation}
          where $\lambda_1,\lambda_2,\dots,\lambda_m$ are coefficients (real numbers),
          $\x_1,\x_2,\dots,\x_m$ are vectors from vector space $V$.
We say that linear combination \eqref{linearcombination} is {\it \,trivial\,} if all coefficients $\lambda_1,\lambda_2,\dots,\lambda_m$ are equal to zero.
              $$
               \lambda_1=\lambda_2=\dots=\lambda_m=0\,.
               $$
We say that linear combination \eqref{linearcombination} is {\it not trivial} if at least one of  coefficients $\lambda_1,\lambda_2,\dots,\lambda_m$ is not equal to zero:
                    $$
         \lambda_1\not=0, {\rm or}\lambda_2\not=0,{\rm or}\dots {\rm or} \lambda_m\not=0\,.
                    $$

Recall definition of linearly dependent and linearly independent vectors:

{\bf Definition} The vectors $\{\x_1,\x_2,\dots,\x_m\}$ in vector space $V$ are {\it linearly dependent\,}
if there exists a non-trivial linear combination of these vectors such that it is equal to zero.

In other words we say that the vectors $\{\x_1,\x_2,\dots,\x_m\}$ in vector space $V$ are {\it linearly dependent\,} if there exist
coefficients $\mu_1,\mu_2,\dots,\mu_m$ such that at least one of these coefficients is not equal to zero and
               \begin{equation}\label{defoflindepend}
             \mu_1\x_1+\mu_2\x_2+\dots+\mu_m\x_m=0\,.
                 \end{equation}

Respectively  vectors $\{\x_1,\x_2,\dots,\x_m\}$ are {\it linearly independent\,} if they are not linearly dependent.
  This means that an arbitrary linear combination of these vectors which is equal  zero is trivial.

In other words
vectors $\{\x_1,\x_2,\x_m\}$ are {\it linearly independent} if the condition
                      $$
                      \mu_1\x_1+\mu_2\x_2+\dots+\mu_m\x_m=0
                      $$
implies that $\mu_1=\mu_2=\dots=\mu_m=0$.



Very useful and workable

{\bf Proposition }
{\it Vectors $\{\x_1,\x_2,\dots,\x_m\}$ in vector space $V$ are {\it linearly dependent\,}
if and only if at least one of these vectors is expressed via linear combination of other vectors:}

               \begin{equation*}\label{lineardependant}
             \x_i=\sum_{j\not =i}\lambda_j\x_j\,.
               \end{equation*}
{\footnotesize
{\sl Proof}. If the condition \eqref{lineardependant} is obeyed then $x_i-\sum_{j\not =i}\lambda_j\x_j=0$. This non-trivial linear combination is equal to zero. Hence vectors $\{x_1,\dots,\x_m\}$ are linearly dependent.

   Now suppose that vectors $\{\x_1,\dots,\x_m\}$ are linearly dependent. This means that
   there exist
coefficients $\mu_1,\mu_2,\dots,\mu_m$ such that at least one of these coefficients is not equal to zero and
   the sum \eqref{defoflindepend} equals to zero. WLOG suppose that $\mu_1\not=0$. We see that
   to             $$
   \x_1=-{\mu_2\over \mu_1}\x_2-{\mu_3\over \mu_1}\x_3-\dots-{\mu_m\over \mu_1}\x_m\,,
                  $$
    i.e. vector $\x_1$ is expressed as linear combination of vectors $\{\x_2,\x_3,\dots,\x_m\}$\finish.

\m
}

   \subsection {Dimension of vector space. Basis in vector space.}\label{basis}


{\bf Definition }  Vector space $V$ has a dimension $n$  if there exist $n$  linearly independent vectors
in this vector space, and  any $n+1$ vectors in $V$ are  linearly dependent.

{\footnotesize In the
case if in the vector space $V$ for an arbitrary $N$
there exist $N$
linearly independent vectors
then the space $V$ is {\it infinite-dimensional}.
An example of infinite-dimensional vector space is a space $V$
of all polynomials of an arbitrary order. One can see that
for an arbitrary $N$ polynomials
           $$
\{1,x,x^2,x^3,\dots,x^N\}
           $$
are linearly idependent. (Try to prove it!).
This implies $V$ is infinite-dimensional vector space.
}

\m


  {\it Basis}

 {\bf Definition}
 Let $V$ be $n$-dimensional vector space.  
The ordered set $\{\e_1,\e_2,\dots,\e_n\}$
 of $n$  linearly independent vectors in $V$ is called a 
basis of the vector space
 $V$.
\m

 {\bf Proposition 1}
 {\it Let $\{\e_1,\dots,\e_n\}$ be an arbitrary basis 
in $n$-dimensional vector space $V$.
 Then any vector $\x\in V$ can be expressed as a 
linear combination of vectors
 $\{\e_1,\dots,\e_n\}$ in a unique way, i.e.
  for every vector $\x\in V$ there exists an 
ordered set of coefficients $\{x^1,\dots,a^n\}$ such that
                            \begin{equation}\label{expansionofvector}
      \x=x^1\e_1+\dots+x^n\e_n
                            \end{equation}
 and if

                      \begin{equation}\label{uniquenessofexpansion}
      \x=a^1\e_1+\dots+a^n\e_n=b^1\e_1+\dots+b^n\e_n\,,
                            \end{equation}
 then $a^1=b^1, a^2=b^2,\dots,a^n=b^n$. In other words
for any vector $\x\in V$ there exists an ordered $n$-tuple 
$(x^1,\dots,x^n)$ of coefficients
  such that  $\x=\sum_{i=1}^nx^i\e_i$  and this 
$n$-tuple is unique.}

%\end{document} % 31 January
\m

 {\sl Proof} Let $\x$ be an arbitrary vector in vector space $V$. 
The dimension of vector space $V$
  equals to $n$. Hence $n+1$ vectors 
$(\e_1,\dots,\e_n,\x)$ are  linearly dependent:
  $\lambda_1\e_1+\dots+\lambda_n\e_n+\lambda_{n+1}\x=0$ 
and this combination is non-trivial.
  If $\lambda_{n+1}=0$ then  $\lambda_1\e_1+\dots+\lambda_n\e_n=0$ 
and this combination is non-trivial,
  i.e. vectors $(\e_1,\dots,\e_n$ are  linearly dependent. Contradiction. Hence $\lambda_{n+1}\not=0$, i.e.
  vector $\x$ can be expressed via vectors $(\e_1,\dots,\e_n)$:
                   $\x=x^1\e_1+\dots x^n\e_n$ where $x^i=-{\lambda_i\over \lambda_{n+1}}$.
  We proved that any vector can be expressed via vectors of basis. 
Prove now the uniqueness of this expansion.
Namely, if \eqref{uniquenessofexpansion} holds then
  $(a^1-b^1)\e_1+(a^2-b^2)\e_2+\dots+(a^n-b^n)\e_n=0$. 
Due to linear independence of basis vectors
this means that $(a^1-b^1)=(a^2-b^2)=\dots=(a^n-b^n)=0$, 
i.e. $a^1=b^1, a^2=b^2,\dots,a^n=b^n$\finish


In other words:

 {\bf Basis is a set of  linearly independent
 vectors in vector space $V$ which span (generate) vector space   $V$.}

(Recall that we say that vector space $V$  is {\it spanned} by vectors $\{\x_1,\dots,\x_n\}$
(or vectors vectors $\{\x_1,\dots,\x_n\}$ {\it span} vector space $V$ ) if any vector $\ac\in V$
can be expresses as a linear combination of vectors $\{\x_1,\dots,\x_n\}$.


\m

 {\bf Definition}
 Coefficients $\{a^1,\dots,a^n\}$ are called {\it components of the vector $\x$ in the basis $\{\e_1,\dots,\e_n\}$}
 or just shortly {\it components of the vector $\x$}.

 \m

  Another very useful and workable statement



  {\bf Proposition 2}  Let  $\{\e_1,\dots,\e_m\}$ be an ordered set of vectors in vector space $V$ such that
  an arbitrary vector $\x\in V$ can be expressed as a linear combination of vectors
 $\{\e_1,\dots,\e_n\}$ in a unique way (see \eqref{expansionofvector} and \eqref{uniquenessofexpansion} above).
 Then

 \begin{itemize}

\item  $V$ is a finite-dimensional space of dimension $m$.

\item $\{\e_1,\dots,\e_m\}$ is a basis in this space.

\end{itemize}

This is very practical statement: it can be often used to find a 
dimension of vector space.


 {\bf Remark} We say "a basis" not "the basis", since 
there are many bases in the vector space $V$.
 (See also Homeworks 1 and 2).





{\bf Remark} Basis is a maximal set of  linearly independent vectors
in a linear space $V$.

\m

{\footnotesize  This leads to definition of
a basis  in infinite-dimensional space. We have to note that in infinite-dimensional space more useful becomes the conception of {\it topological basis}
when infinite sums are considered.}



{\it Canonical basis  in $\R^n$}

\m

We considered above  the basic example of $n$-dimensional vector space---a space
of ordered $n$-tuples of real numbers: $\R^n=\{(x^1,x^2,\dots,x^n),  x^i\in \R\}$
(see the subsection 1.2).
What is the meaning of letter `$n$' in the definition of $\R^n$?

Consider vectors $\e_1,\e_2,\dots,\e_n\in \R^n$:
                  \begin{equation}\label{basicvectors}
                  \begin{array}{cc}
                    \e_1=& (1,0,0\dots,0,0) \\
                    \e_2=& (0,1,0\dots,0,0) \\
                    \dots & \dots \\
                    \e_n=& (0,0,0\dots,0,1) \\
                  \end{array}
                  \end{equation}
Then for an arbitrary vector   $\R^n\ni \ac=(a^1,a^2,a^3,\dots,a^n)$
                     $$
            \ac=(a^1,a^2,a^3,\dots,a^n)=
                      $$
                      $$
                      a^1(1,0,0\dots,0,0)+a^2(0,1,0\dots,0,0)+
            a^3(0,0,1,0\dots,0,0)+\dots+a^n(0,1,0\dots,0,1)=
                         $$
                         $$
            =\sum_{i=1}^m a^i\e_i=a^i\e_i \qquad \hbox{(we will use sometimes condensed notations}\,\, \x= x^i\e_i)
                            $$
Thus we see that for every vector $\ac\in \R^n$ we have 
unique expansion via the vectors
 \eqref{basicvectors}.

{\bf Remark} One can find
   another basis in $\R^n$--just take an arbitrary ordered set of $n$  linearly independent vectors.
   (See exercises 7 and 8 in  Homework 1).
 The basis \eqref{basicvectors} is distinguished. 
Sometimes it is called {\it canonical basis in $\R^n$.}



\m


{\bf Remark} One can consider a set of ordered $n$-tuples in $\R^n$ as the set of
points.
   Two points
   ${\bf a,b}\in \R^n$ define a vector:
   if $\bf a$ $=(a^1,\dots,a^n)$, $\bf b$ $=(b^1,\dots,b^n)$, then
   the vector $\bf ab$ attached to the point $\bf a$
   has coordinates $=(b^1-a^1,b^2-a^2\dots,b^n-a^n)$
   \footnote{$\R^n$ considered as a set of points is called affine space}.

%\end{document} % 31 January 2015

\subsection {Scalar product. Euclidean space}

In vector space one have additional structure: {\it scalar product of vectors}.

{\bf Definition} Scalar product in a vector space $V$ is  a function $B(\x,\y)$ on a pair of
 vectors which takes real values and satisfies the  the following conditions:
               \begin{equation}\label{scalarproductproperties}
              \begin{array}  {cc}
                B(\x,\y)=B(\y,\x) \quad \hbox  {(symmetricity condition)}\\
                   B(\lambda\x+\mu\x',\y)=
                   \lambda B(\x,\y)+\mu B(\x',\y)\quad   \hbox  {(linearity condition)}\\
                 B( \x,\x)\geq 0\,\, , B(\x,\x)= 0\Leftrightarrow  \x=0\,\,
                 \hbox{(positive-definiteness condition)}
                   \end{array}
               \end{equation}
{\bf Definition } Euclidean space is a vector space equipped with a scalar product.

\m


  One can easy to see that the function $B(\x,\y)$ is bilinear function, i.e.
  it is linear function with respect to the second argument also\footnote{Here and later we will denote scalar
   product $B(\x,\y)$ just by $(\x,\y)$. Scalar product sometimes is called inner product. Sometimes it is called
    dot product.}.
  This follows from previous axioms:
                   $$
   B(\x,\lambda\y+\mu\y')\underbrace{=}_{\hbox {symm.}}B(\lambda\y+\mu\y',\x)
   \underbrace{=}_{\hbox {linear. }}\lambda B(\y,\x)+\mu B(\y',\x)
   \underbrace{=}_{\hbox {symm. }}\lambda B(\x,\y)+\mu B(\x,\y')\,.
                   $$

{\footnotesize A bilinear function  $B(\x,\y)$ on pair of vectors is called sometimes {\it bilinear form} on
vector space. Bilinear form $B(\x,\y)$  which satisfies the symmetricity condition is called
{\it symmetric bilinear form}.  Scalar product is nothing but symmetric bilinear form on vectors
which is positive-definite: $B(\x,\x)\geq 0$) and is non-degenerate ($(\x,\x)= 0\Rightarrow  \x=0$}.

\medskip



\medskip


{\bf Example} We considered the vector space $\R^n$, the space of $n$-tuples (see the subsection 1.2).
 One can consider the vector space $\R^n$ as Euclidean space provided by the scalar product
         \begin{equation}\label{scalarproductexample1}
                    B(\x,\y)=x^1y^1+\dots+x^ny^n
                \end{equation}
This scalar product sometimes is called {\it canonical scalar product}.


{\bf Exercise}  Check that it is indeed scalar product.

\smallskip

{\bf Example} We consider in $2$-dimensional 
vector space $V$ with basis $\{\e_1,\e_2\}$ and
$B(\X,\Y)$ such that $B(\e_1,\e_1)=3$,  $B(\e_2,\e_2)=5$ and  $B(\e_1,\e_2)=0$.
Then for every two vectors $\X=x^1\e_1+x^2\e_2$ and ${\bf Y}=y^1\e_1+y^2\e_2$ we have that
                        $$
     B(\X,\Y) =\left(\X,\Y\right)=\left(x^1\e_1+x^2\e_2,y^1\e_1+y^2\e_2\right)=
              $$
              $$
   x^1y^1(\e_1,\e_1)+x^1y^2(\e_1,\e_2)+
   x^2y^1(\e_2,\e_1)+
   x^2y^2(\e_2,\e_2)=
      3x^1y^1+5x^2y^2\,.
                        $$
One can see that all axioms are obeyed.

\centerline {\it Notations!}

Scalar product sometimes is called "inner" product
 or "dot" product.
Later on we will use for scalar product $B(\x,\y)$ just shorter notation
$(\x,\y)$ (or $\langle\x,\y\rangle$).
Sometimes it is used for scalar product a notation $\x \cdot \y$.
Usually this notation is reserved only for
the canonical case $\eqref{scalarproductexample1}$.

\smallskip



{\bf Counterexample}  Consider  again $2$-dimensional 
vector space $V$ with basis $\{\e_1,\e_2\}$.


Show that operation such that $(\e_1,\e_1)=(\e_2,\e_2)=0$ and $(\e_1,\e_2)=1$
does not define scalar product.
{\it Solution}. For every two vectors $\X=x^1\e_1+x^2\e_2$ and 
${\bf Y}=y^1\e_1+y^2\e_2$ we have that
                        $$
\left(\X,\Y\right)=\left(x^1\e_1+x^2\e_2,y^1\e_1+y^2\e_2\right)=x^1y^2+x^2y^1
              $$
hence for vector $\X=(1,-1)$ $(\X,\X)=-2<0$. Positive-definiteness is not fulfilled.

Another {\bf Counterexample}  
Show that operation $(\X,\Y)=x^1y^1-x^2y^2$ does not define scalar product.
{\it Solution}. Take $\X=(0,-1)$. Then $(\X,\X)=-1$. The condition of positive-definiteness is not fulfilled.
(See also exercises in Homework 2.)


%\end {document}  % 5.02.2014


\subsection {Orthonormal basis in Euclidean space}

One can see that for scalar product  \eqref{scalarproductexample1}
and for the basis $\{\e_1,\dots,\e_n\}$ defined by the relation \eqref{basicvectors}
the following relations hold:
\begin{equation}\label{orthonormalbasis}
    (\e_i,\e_j)=\delta_{ij}= \begin {cases}
                  1\quad {\rm if}\quad i=j\\
              0\quad {\rm if}\quad i\not=j
                         \end{cases}
\end{equation}

   Let  $\{\e_1,\e_2,\dots,\e_n\}$ be an ordered set of $n$ vectors in $n$-dimensional
   Euclidean space which obeys the conditions
 \eqref{orthonormalbasis}. One can see that this ordered set is a basis
 \footnote
 {Indeed prove that conditions \eqref{orthonormalbasis} imply that these $n$ vectors are linear independent.
 Suppose that $\lambda_1\e_1+\lambda_2\e_2+\dots+\lambda_n\e_n=0$.
 For an arbitrary $i$ multiply the left and right hand sides of this relation on
 a vector $\e_i$. We come to condition $\lambda_i=0$.
 Hence vectors $(\e_1,\e_2,\dots,\e_n)$ are linearly dependent.}.

{\bf Definition-Proposition}   The ordered set of vectors
$\{\e_1,\e_2,\dots,\e_n\}$ in $n$-dimensional Euclidean space which obey the conditions
 \eqref{orthonormalbasis} is a basis.  This basis  is called {\it an orthonormal basis}.



\smallskip

 One can prove that every (finite-dimensional)  Euclidean
 space possesses orthonormal basis.

 Later by default we consider only orthonormal bases in Euclidean spaces.
 Respectively scalar product will be defined by the formula \eqref{scalarproductexample1}.
 Indeed let $\{\e_1,\e_2,\dots,\e_n\}$ be an orthonormal basis in Euclidean space. Then for an arbitrary two vectors
 $\x,\y$, such that $\x=\sum x^i{\e_i}$, $\y=\sum y^j{\e_j}$ we have:
    \begin{equation*}\label{scalarproduct4}
    (\x,\y)=\left(\sum x^i{\e_i},\sum y^j{\e_j}\right)=
    \sum_{i,j=1}^nx^iy^j(\e_i,\e_j)=\sum_{i,j=1}^nx^iy^j\delta_{ij}=
    \sum_{i=1}^nx^iy^i
\end{equation*}
 We come to the canonical  scalar product \eqref{scalarproductexample1}.
Later on we usually will consider scalar product defined by the formula
  \eqref{scalarproductexample1} 
i.e. scalar product in orthonormal basis.

\smallskip

{\bf Remark} We consider here general definition of scalar product
then came  to conclusion that in a special basis, 
({\it orthonormal basis}),
this is nothing but usual `dot' product 
\eqref{scalarproductexample1}. 

\smallskip


 \centerline {\it Geometrical properties of scalar product: length of the vectors, angle between vectors}

The scalar product of vector on itself defines the  {\it length of the vector}:
\begin{equation}\label{length}
\hbox {Length of the vector $\x$}=|\x|=\sqrt {(\x,\x)}=\sqrt {(x^1)^2+\dots+(x^n)^2}
\end{equation}

  If we consider Euclidean space $\bf E^n$  as the set of points then the distance between two points
  $\x,\y$  is
    the length of corresponding vector:
\begin{equation*}\label{distance}
 \hbox{distance between points $\x,\y$}= |{\bf x}-{\bf y}|=\sqrt {\left(y^1-x^1\right)^2
 +\dots+\left(y^{n}-x^n\right)^2}
\end{equation*}



We recall very important formula how scalar (inner) product is
related with the angle between vectors:
 \begin{equation*}\label{scalarproductandangle}
  ({\bf x},{\bf y})=x^1y^1+x^2y^2=|{\bf x}||{\bf y}|\cos\varphi
\end{equation*}
where $\varphi$ is an angle between vectors $\x$ and $\y$ in $\E^2$.

This formula is valid also in the three-dimensional case and any $n$-dimensional case for $n\geq 1$.
It gives as a tool to calculate angle between two vectors:
\begin{equation}\label{scalarproductandangleinndimensions}
  ({\bf x},{\bf y})=x^1y^1+x^2y^2+\dots+x^ny^n=|{\bf x}||{\bf y}|\cos\varphi
\end{equation}
In particulary it follows from this formula
 that
                \begin{equation}\label{angleacuteorobtuse}
                \begin {array}{cc}
 \hbox {\it angle between vectors $\x,\y$ is acute  if scalar product $(\x,\y)$ is positive}\\
\hbox {\it angle between vectors $\x,\y$ is obtuse  if scalar product $(\x,\y)$ is negative}\\
\hbox {\it vectors $\x,\y$ are perpendicular  if scalar product $(\x,\y)$ is equal to zero}
          \end{array}
          \end{equation}



{\footnotesize {\bf Remark} Geometrical intuition  tells us that cosinus of the angle between two vectors has to be
 less or equal to one and it is equal to one if and only if vectors $\x,\y$ are collinear.
 Comparing with \eqref{scalarproductandangleinndimensions} we come to the inequality:
                  \begin{equation}\label{cauchybuniakovsky}
                  \begin{array}{cc}
            (\x,\y)^2=\left(x^1y^1+\dots+x^ny^n\right)^2\leq \left((x^1)^2+\dots+(x^n)^2\right)
             \left((y^1)^2+(\dots+(y^n)^2\right)=(\x,\x)(\y,\y)\\
           {\rm and} (\x,\y)^2=(\x,\x)(\y,\y)\quad \hbox {if vectors are colienar, i.e. $x^i=\lambda y^i$}
             \end{array}
                  \end{equation}
This is famous Cauchy--Buniakovsky--Schwarz inequality, one of most
important inequalities in mathematics. (See for more details
Homework 2)}

%\end{document} % February

\subsection {Transition matrices. Orthogonal bases and orthogonal matrices}

One can consider different  bases in vector space.

Let $A$ be $n\times n$ matrix with real entries, $A=||a_{ij}||$, $i,j=1,2,\dots,n$:
             $$
          A=\begin{pmatrix}
   a_{11} &a_{12}\dots &a_{1n}\cr
   a_{21} &a_{22}\dots &a_{2n}\cr
 a_{31} &a_{32}\dots &a_{3n}\cr
 \dots &\dots\dots &\dots\cr
 a_{(n-1)\, 1} &a_{(n-1) 2}\dots &a_{(n-1)n}\cr
 a_{n\,1} &a_{n 2}\dots &a_{n n}\cr
\end{pmatrix}
                $$
Let $\{\e_1,\e_2,\dots,\e_n\}$ be an arbitrary  basis in $n$-dimensional vector space $V$.


The basis $\{\e_1,\e_2,\dots,\e_n\}$ can be considered as row of vectors,
or $1\times n$ matrix with entries--vectors.


Multiplying  $1\times n$ matrix $\{\e_1,\e_2,\dots,\e_n\}$ on matrix $A$ we come to new row of vectors
$\{\e'_1,\e'_2,\dots,\e'_n\}$ such that
 \begin{equation}\label{transformoforthogbasis0}
  \{\e'_1,\e'_2,\dots,\e'_n\}=\{\e_1,\e_2,\dots,\e_n\}A=
 \end{equation}
    \begin{equation}\label{transformoforthogbasis}
\{\e'_1,\e'_2,\dots,\e'_n\}=\{\e_1,\e_2,\dots,\e_n\}
\begin{pmatrix}
   a_{11} &a_{12}\dots &a_{1n}\cr
   a_{21} &a_{22}\dots &a_{2n}\cr
 a_{31} &a_{32}\dots &a_{3n}\cr
 \dots &\dots\dots &\dots\cr
 a_{(n-1)\, 1} &a_{(n-1) 2}\dots &a_{(n-1)n}\cr
 a_{n\,1} &a_{n 2}\dots &a_{n n}\cr
\end{pmatrix}
\end{equation},
            $$
        \begin{cases}
        \e'_1=a_{11}\e_1+a_{21}\e_2+a_{31}\e_3+\dots+a_{(n-1)\, 1}\e_{n-1}+a_{n\, 1}\e_n\cr
        \e'_1=a_{12}\e_1+a_{22}\e_2+a_{32}\e_3+\dots+a_{(n-1)\, 2}\e_{n-1}+a_{n\, 2}\e_n\cr
        \e'_1=a_{13}\e_1+a_{23}\e_2+a_{33}\e_3+\dots+a_{(n-1)\, 3}\e_{n-1}+a_{n\, 1}\e_n\cr
        \dots=\dots\dots+\dots\dots+\dots\dots+\dots+\dots\dots \dots\dots\cr
        \e'_n=a_{1n}\e_1+a_{2n}\e_2+a_{3n}\e_3+\dots+a_{(n-1)\, n}\e_{n-1}+a_{n\, n}\e_n\cr
        \end{cases}
          $$
         or shortly:
        \begin{equation}\label{transformoforthogbasis1}
               \e_i'=\sum_{k=1}^n \e_k a_{ki}\,.
         \end{equation}
 {\bf Definition } Matrix $A$ which transforms a 
  basis $\{\e_1,\e_2,\dots,\e_n\}$
  to the row of vectors  $\{\e'_1,\e'_2,\dots,\e'_n\}$ 
(see equation \eqref{transformoforthogbasis1})
  is  {\it transition matrix} from the basis 
$\{\e_1,\e_2,\dots,\e_n\}$ to the row
  $\{\e'_1,\e'_2,\dots,\e'_n\}$.


 What is the condition that  the row $\{\e'_1,\e'_2,\dots,\e'_n\}$ 
is a basis too?
 The row, ordered set of vectors,
 $\{\e'_1,\e'_2,\dots,\e'_n\}$ is a basis if and only if vectors $(\e'_1,\e'_2,\dots,\e'_n)$
are  linearly independent. Thus  we come to

{\bf Proposition 1} {\it Let $\{\e_1,\e_2,\dots,\e_n\}$ be a basis in 
$n$-dimensional
 vector space $V$, and let $A$ be an
$n\times n$ matrix with real entries.
Then
                      \begin{equation}\label{transitionmatrix}
                      \{\e'_1,\e'_2,\dots,\e'_n\}=\{\e_1,\e_2,\dots,\e_n\}A
                       \end{equation}
                       is a basis if and only if the transition
matrix $A$ has rank $n$, i.e. it is non-degenerate (invertible) matrix.}

Recall that $n\times$ matrix $A$ is nondegenerate (invertible)
$\Leftrightarrow$ $\det A\not=0$.


% 7 february

%\end{document}   % 6 February 2015 
\m

{\bf Remark}  Recall that  the condition that $n\times n$ matrix $A$ is non-degenerate
(has rank $n$) is equivalent to the condition that it is invertible matrix,
or to the condition that $\det A\not=0$.

Now suppose that  $\{\e_1,\e_2,\dots,\e_n\}$ is orthonoromal basis 
in $n$-dimensional Euclidean
vector space.   What is the condition that  the new basis
$\{\e'_1,\e'_2,\dots,\e'_n\}=\{\e_1,\e_2,\dots,\e_n\}A$ is 
an orthonormal  basis too?

\m

{\bf Definition}  We say that $n\times n$ matrix is orthogonal matrix 
if its product on transposed matrix is equal to unity matrix:
                \begin{equation}\label{defofofthogonal}
                    A^{^T}A=I\,.
                \end{equation}

 {\bf Exercise}. Prove that determinant of orthogonal matrix is 
equal to $\pm 1$:
 \begin{equation}\label{defofofthogonal2}
                    A^{^T}A=I\Rightarrow \det A=\pm 1\,.
                \end{equation}

 {\sl Solution}  $A^TA=I$. Hence $\det (A^TA)=\det A^T\det A=(\det A)^2=\det I=1$. Hence $\det A=\pm 1$.
   We see that in particular orthogonal matrix is non-degenerate ($\det A\not=0$). Hence it is a transition matrix from one basis to another.      The following  Proposition is valid:

{\bf Proposition 2}  Let $\{\e_1,\e_2,\dots,\e_n\}$ be  an 
orthonormal basis in $n$-dimensional Euclidean
vector space.  Then the new basis  $\{\e'_1,\e'_2,\dots,\e'_n\}=\{\e_1,\e_2,\dots,\e_n\}A$ is orthonormal basis if and only if the transition matrix $A$ is orthogonal matrix.

{\sl Proof}
  The basis   $\{\e'_1,\e'_2,\dots,\e'_n\}$ is orthonormal  
means that $(\e'_i,\e'_j)=\delta_{ij}$. We have:
                        $$
\delta_{ij}=(\e'_i,\e'_j)=\left(\sum_{m=1}^n \e_m A_{mi},\e'_j=\sum_{n=1}^n \e_n A_{nj}\right)=
        \sum_{m,n=1}^n A_{mi}A_{nj}(\e_m,\e_n)=
                        $$
                        \begin{equation}\label{orthogonmatr1}
\sum_{m,n=1}^n A_{mi}A_{nj}\delta_{mn}=\sum_{m=1}^n A_{mi}A_{mj}=
\sum_{m=1}^n A^T_{im}A_{mj}=(A^TA)_{ij}\,,
\end{equation}
Hence $(A^TA)_{ij}=\delta_{ij}$, i.e. $A^TA=I$.







\m

{\footnotesize   We know that determinant of orthogonal matrix equals to $\pm 1$.
It is very useful to consider the following groups:

 \begin{itemize}

 \item  The group $O(n)$---group of  orthogonal $n\times n$ matrices:
             \begin{equation}\label{orthog.group}
    O(n)=\{A\colon \quad A^T A=I\}\,.
\end{equation}


\item  The group $SO(n)$ special orthogonal group of  $n\times n$ matrices:
             \begin{equation}\label{orthog.group}
    SO(n)=\{A\colon \quad A^T A=I, \det A=1 \}\,.
\end{equation}

\end{itemize}
}

\subsection {Linear operators.}

Recall here facts about linear operators in vector space

Let $P$ be a linear operator in vector space $V$:
              \begin{equation*}
 P\colon \, V\to V,\qquad P(\lambda \x+\mu \y)=\lambda P(\x)+\mu P(\y).
              \end{equation*}
Let $\{\e_1,\dots,\e_n\}$ be an arbitrary  basis in $n$-dimensional vector space $V$.
Consider the action of operator $P$ on basis vectors: $\e'_i=P(\e_i)$:
                \begin{equation}\label{transitionmatrixoflinearoperator}
                  \begin{matrix}
 \e'_1=P(\e_1)=\e_1 p_{11}+\e_2 p_{21}+\e_3 p_{31}+\dots+\e_n p_{n1}\cr
 \e'_2 =P(\e_2)=\e_1 p_{12}+\e_2 p_{22}+\e_3 p_{32}+\dots+\e_n p_{n2}\cr
 \e'_3 =P(\e_3)=\e_1 p_{13}+\e_2 p_{23}+\e_3 p_{31}+\dots+\e_n p_{n3}\cr
     \ldots\cr
  \e'_n = P(\e_n)=\e_1 p_{1n}+\e_2 p_{2n}+\e_3 p_{3n}+\dots+\e_n p_{nn}\cr                         \end{matrix}
                 \end{equation}
   In the case if linear operator $P$ is non-degenerate (invertible) then
  vectors  ${\e'_1,\e'_2,\e'_3,\dots,\e'_n}$,
 where $\e'_i=P(\e_i)=\sum \e_kp_{ki}$ form a basis.
  The matrix $P=||p_{ik}||$ is the transition matrix
from the basis $\{\e_i\}$ to the basis $\{\e'_i=P(\e_i)\}$. 

{\bf Definition}  Let $\{\e_i\}$ be a basis. Then the 
transition matrix $p_{ik}$ defined by 
relation \eqref{transitionmatrixoflinearoperator} 
 is called {\it matrix of operator $P$ in the basis $\{\e_i\}$.}

\smallskip
   How matrix of linear operatot changes if we change the basis?
    Consider a new basis $\{\f_1,\dots,\f_n\}$ in the linear space $V$.
  Let $A$ be transition matrix from the basis $\{\e_1,\dots,\e_n\}$
to the new basis $\{\f_1,\dots,\f_n\}$:
               \begin{equation*}\label{changingofbasis2}
\{\f_1,\dots,\f_n\}=\{\e_1,\dots,\e_n\}A,\,\,{\rm i.e.}
      f_i=\sum_k\e_k a_{ki}
               \end{equation*}
 (see equation \eqref{transformoforthogbasis1}).
Then the action of operator $P$ in the new basis is given by the formula
       $\f'_i=P(\f_i)$. According to the formulae \eqref{changingofbasis2} and
\eqref{transitionmatrixoflinearoperator} we have
          $$
   \f'_i=P(\f_i)=P\left(\sum_q\e_q a_{qi}\right)=
     \sum_q a_{qi}\left(\sum_r\e_r p_{rq}\right)=
     \sum_{q,r}\e_r p_{rq}a_{qi}=\sum_{r}\e_r (PA)_{ri}=
         $$
         $$
    \sum_{r,k}\f_{k}(A^{-1})_{kr}(PA)_{ri}=\sum_k \f_k(A^{-1}PA)_{ki}\,.
      $$
We see that in the new basis $\{\f_i\}$
a matrix of linear operator is $A^{-1}PA$:
         \begin{equation}\label{changingoftransitionmatrix}
{\rm If\,}
  \{\e'_1,\dots,\e'_n\}=\{\e_1,\dots,\e_n\}P,\,\,
{\rm then\,\,}
  \{\f'_1,\dots,\f'_n\}=\{\f_1,\dots,\f_n\}A^{-1}PA,\,\,,
      \end{equation}
where $A$ is transition matrix from the basis $\{\e_1,\dots,\e_n\}$
to the basis $\{\f_1,\dots,\f_n\}$,

\subsubsection{Determinant and Trace of linear operator}
We recall the definition of determinant and explain what is the trace
of linear operator,

   {\bf Definition-Proposition}
Let $P$ be a linear operator in vector space $V$ and
let $P_{ik}=||p_{ik}||$ be transition matrix of this operator in
an arbitrary basis in $V$ (see construction
\eqref{transitionmatrixoflinearoperator}.) Then
determinant of linear operator $P$ equals to
determinant of transition matrix of this operator.
           $$
  \det P= \det \left(p_{ik}\right)
            $$
In the same way we define trace of operator via trace of matrix:
            \begin{equation}\label{traceofoperator}
      {\rm Tr\,}P={\rm Tr\,}\left(||p_{ik}||\right)=
    p_{11}+p_{22}+p_{33}+\dots+p_{nn}\,.
             \end{equation}
Determinant and trace of operator are well-defined. since
due to \eqref{changingoftransitionmatrix}
determinant and trace of transition matrice do not change
if we change the basis in spite of the  fact that transition matrix changes:
$P\mapsto A^{-1}PA$, but
              $$
              \det \left(A^{-1}PA\right)=
              \det A^{-1}\det P\det A=(\det A)^{-1}\det P \det A=\det P\,.
              $$
{\footnotesize In the same way one can see that trace is invariant too:
             $$
   {\rm Tr\,}(A^{-1}PA)=\sum_i(A^{-1}PA)_{ii}=
\sum_{i,k,p}\left(A^{-1}\right)_{ik}p_{kp}=
\sum_{i,k,p}A_{pi}\left(A^{-1}\right)_{ik}p_{kp}=
          $$
          $$
\sum_{p,k}\left(A\cdot A^{-1}\right)_{pk}p_{kp}=
\sum_{p,k}\delta_{kp}p_{kp}=\sum_k p_{kk}={\rm Tr\,}P\,.
              $$
Trace of linear operator is an infinitesimal version of its determinant:
       $$
   \det(1+tP)=1+t{\rm Tr\,}P+O(t^2)\,.
       $$
This is infinitesimal version for the followiong
famous formula which relates trace and det of linear
operator:
        \begin{equation}\label{Liovilleformula}
         \det e^{tA}=e^{t{\rm Tr\,}A}\,.
         \end{equation}
where $e^{tA}=\sum{t^nA^n\over n!}$. E.g. if
$A=\begin{pmatrix}0 &-1\cr 1 &0\end{pmatrix}$,
then $e^{tA}=\begin{pmatrix} \cos t&-\sin t\cr \sin t &\cos t\end{pmatrix}$,
$\det e^{tA}=1$ and $e^{t{\rm Tr\,}A}=e^0=1$. }



%\end{document}
 \subsubsection {Orthogonal linear operators}

   Now we study geometrical meaning of
orthogonal linear operators in Euclidean space.



    Recall that linear operator $P$ in Euclidean space $\E^n$
is called orthogonal operator if it preserves scalar product:
        \begin{equation}\label{orthogonaldef}
        (P\x, P\y)=(\x,\y), \hbox{ for arbitrary vectors $\x,\y$}\,
          \end{equation}

In particular if $\{\e_i\}$ is orthonormal basis in Euclidean space then
due to \eqref{orthogonaldef} the new basis $\{\e'_i=P(\e_i)\}$ is orthonormal too. Thus we see that matrix of orthogonal operator $P$ in a given orthogonal
basis is orthogonal matrix:
         \begin{equation}\label{orthogonallinearoperator}
                   P^T\cdot P=I
                   \end{equation}
 (see \eqref{defofofthogonal} in subsection 1.7).
   In particular we see that for orthogonal linear operator
     $\det P=\pm 1$ (compare with \eqref{defofofthogonal2}).
     If
   $\det P=1$ then it preserves orientation and if $\det P=-1$
   then it changes orientation.
Remember we already studied orthogonal matrices in
$2$-dimensional Euclidean space (see subsection 1.8).



\subsection{Orthogonal operators in $\E^2$---Rotations and reflections}


We show that an orthogonal operator `rotates the space' 
or makes a `reflection'.

   Let $A$ be an arothogonal operator acting in Euclidean space  $\E^2$:
   $(A\x,A\y)=(\x,\y)$. Let $\{\e,\f\}$ be an orthonormal 
basis in $2$-dimensional Euclidean space $\E^2$:
$(\e,\e)=(\f,\f)=1$ (i.e. $|\e|=|\f|=1$) and $(\e,\f)=0$--vectors $\e,\f$ have unit length and are orthogonal to each other.



Consider a new basis $\{\e',\f'\}$, an image of basis ${\e,\f}$
under action of $A$: $\e'=A(\e)$, $\f'=A(\f)$. Let $ \begin{pmatrix}
      \a & \beta \\
      \gamma & \delta\\
       \end{pmatrix}$  be matrix of operator $A$ in the basis ${\e,\f}$,
(see
 equation \eqref{transitionmatrixoflinearopera} and defintion 
after this equation):
             $$
    \{\e',\f'\}=\{\e,\f\}A=  \{\e,\f\}
   \begin{pmatrix}
      \a & \beta \\
      \gamma & \delta\\
       \end{pmatrix},
\, {\rm i.e.}\,\e'=\a \e+\gamma \f,\, \f'=\beta \e+\delta \f
                $$
New basis is orthonormal basis also,
 $(\e',\e')=(\f',\f')=1\,,\quad (\e',\f')=0$.

Operator $A$ is orthogonal operator, and its matrix is orthogonal matrix:
      \begin{equation*}    
A^TA=\begin{pmatrix}
      \a & \beta \\
      \gamma & \delta\\
       \end{pmatrix}^t
       \begin{pmatrix}
      \a & \beta \\
      \gamma & \delta\\
       \end{pmatrix}=
       \begin{pmatrix}
      \a & \gamma \\
      \beta & \delta\\
       \end{pmatrix}
       \begin{pmatrix}
      \a & \beta \\
      \gamma & \delta\\
       \end{pmatrix}=
       \begin{pmatrix}
      \a^2+\gamma^2 & \a\beta+\gamma\delta \\
      \a\beta+\gamma\delta & \beta^2+\delta^2\\
       \end{pmatrix}=
       \begin{pmatrix}
      1 & 0 \\
      0& 1\\
       \end{pmatrix}\,.
\end{equation*}

{\bf Remark} Whith some abuse of notation,
 (if it not a reason of confusion)
we sometimes use the same letter
for linear operator and the matrix of this operator in orthonormal basis.


 We have  $\a^2+\gamma^2=1$, $\a\beta+\gamma\delta=0$ and $\beta^2+\delta^2=1$.  Hence one can choose angles
  $\varphi,\psi \colon 0\leq 2\pi$ such that
               $\a=\cos\varphi,\,\,\gamma=\sin\varphi,\,\,\,\beta=\sin\psi,\,\,\delta=\cos\psi$.
 The condition  $\a\beta+\gamma\delta=$ means that
                 $$
            \cos\varphi\sin\psi+ \sin\varphi\cos\psi=\sin(\varphi+\psi)=0
                 $$
Hence $\sin \varphi=-\sin \psi$, $\cos\varphi=\cos\psi$ ($\varphi+\psi=0$) or
$\sin \varphi=\sin \psi$, $\cos\varphi=-\cos\psi$ ($\varphi+\psi=\pi$)


The first case:  $\sin \varphi=-\sin \psi$, $\cos\varphi=\cos\psi$,
   \begin{equation}\label{orthogonalrotation}
    A_\varphi=\begin{pmatrix}
      \a & \beta \\
      \gamma & \delta\\
       \end{pmatrix}=\begin{pmatrix}
      \cos\varphi & -\sin\varphi \\
      \sin\varphi & \cos\varphi\\
       \end{pmatrix}\,\,\,(\det A_\varphi=1)
\end{equation}\

 The second case: $\sin \varphi=\sin \psi$, $\cos\varphi=-\cos\psi$,
   \begin{equation}\label{orthogonalrotation+reflection}
    \tilde A_\varphi=\begin{pmatrix}
      \a & \beta \\
      \gamma & \delta\\
       \end{pmatrix}=\begin{pmatrix}
      \cos\varphi & \sin\varphi \\
      \sin\varphi & -\cos\varphi\\
       \end{pmatrix}\,\,\, (\det \tilde A_\varphi=-1)
\end{equation}


\m


 In the first case matrix of operator $A_\varphi$ is 
defined by the relation \eqref{orthogonalrotation}.
In this case the new basis is:
    \begin{equation}\label{rotationofbasisontheangle}
   (\e',\f')=
   (\e,\f)
     A_\varphi=
     (\e,\f)
   \begin{pmatrix}
      \cos\varphi & -\sin\varphi \\
      \sin\varphi & \cos\varphi \\
    \end{pmatrix}
                 ,\quad
            \begin{matrix}
   \e'=A_\varphi(\e)= \cos\varphi \,\e+\sin\varphi \,\f\cr
   \f'=A_\varphi(\f) -\sin\varphi\, \e+\cos\varphi \,\f\cr
            \end{matrix}
            \end{equation}
For an arbitrary vector $\x=x\e+y\f$
$\x\to A_\varphi(\x)=A_\varphi(x\e+y\f)=x'\e+y'\f$,
             \begin{equation}
        \begin{pmatrix}
           x'\cr y'\cr
        \end{pmatrix} =
          \begin{pmatrix}
      \cos\varphi & -\sin\varphi \\
      \sin\varphi & \cos\varphi \\
             \end{pmatrix}
           \begin{pmatrix}
           x\cr y\cr
        \end{pmatrix}=
           \begin{pmatrix}
           x\cos\varphi-y\sin\varphi\cr 
        \sin\varphi+y\cos\varphi\cr
        \end{pmatrix}\,.
             \end{equation}
{\bf  Operator $A_\varphi$ rotates 
basis vectors $\e,\f$ and arbitrary vector $\x$ on an angle
$\varphi$ }


\m

 In the second case a matrix of operator
$\tilde A_\varphi$ is defined by the relation 
\eqref{orthogonalrotation+reflection}.
One can see that
                  \begin{equation}\label{secondcase-rotation}
    \tilde A_\varphi=\begin{pmatrix}
      \cos\varphi & \sin\varphi \\
      \sin\varphi & -\cos\varphi\\
       \end{pmatrix}=\begin{pmatrix}
      \cos\varphi & -\sin\varphi \\
      \sin\varphi & \cos\varphi\\
       \end{pmatrix}\begin{pmatrix}
      1 & 0 \\
      0 & -1\\
       \end{pmatrix}=A_\varphi R
\end{equation}
where we denote by $R=\begin{pmatrix}
      1 & 0 \\
      0 & -1\\
       \end{pmatrix}$ a transition matrix from the basis $\{\e,\f\}$ to the basis $\{\e,-\f\}$---``reflection''l.


       We see that in the second case the orthogonal  operator
  $\tilde A_\varphi$
is composition of rotation and reflection:
       $ \{\e,\f\}{\buildrel \tilde A_\varphi=A_\varphi R\over \longrightarrow}\{\tilde \e,\tilde \f\}$:
                           \begin{equation}\label{rotation+reflectioninaction}
    \{\e,\f\}{\buildrel A_\varphi\over \longrightarrow}
    \{\e'=\cos\varphi \,\e+\sin\varphi \l,\f,\f'=-\sin\varphi\, \e+\cos\varphi \,\f\}
    {\buildrel R\over \longrightarrow}\{\tilde \e=\e',\tilde f=-\f\}
\end{equation}


  We come to proposition

        \m

        {\bf Proposition}. {\it Let $A$ be an arbitrary 
$2\times 2$ orthogonal linear transformation, $A^TA=1$,
        and in particularly $\det A=\pm 1$. 
(As usual we consider matrix of orthogonal operator 
in the orthonormal basis.)

         If $\det A=1$
      then there exists an angle $\varphi\in [0,2\pi)$ such that
      $A=A_\varphi$ is an operator which rotates basis vectors
and any vector \eqref{orthogonalrotation}
     on the  angle  $\varphi$.

      If $\det A=-1$ then there exists an angle $\varphi\in [0,2\pi)$ 
such that $A=\tilde A_\varphi$
      is a composition of rotation and 
reflection (see \eqref{rotation+reflectioninaction}).}

      \m

{\footnotesize
      {\bf Remark} One can show that orthogonal operator
       $\tilde A_\varphi$ is a reflection with respect
        to the axis which have the angle $\varphi\over 2$ with $x$-axis.

     Consider just examples:
                   $$
     a) \varphi=0, \,\, \tilde A_\varphi=
                    \begin{pmatrix}
      \cos\varphi & \sin\varphi \\
      \sin\varphi & -\cos\varphi\\
       \end{pmatrix}=\begin{pmatrix}
      1 & 0 \\
      0 & -1\\
       \end{pmatrix},\,\,
       \quad \begin{pmatrix}\e\cr \f\end{pmatrix}
       \mapsto
       \begin{pmatrix}\e\cr -\f\end{pmatrix}
     $$
     (reflection with respect to $x$-axis)

        $$
             b) \varphi=\pi, \,\, \tilde A_\varphi=
                    \begin{pmatrix}
      \cos\varphi & \sin\varphi \\
      \sin\varphi & -\cos\varphi\\
       \end{pmatrix}=\begin{pmatrix}
      -1 & 0 \\
      0 & 1\\
       \end{pmatrix},\,\,
       \quad \begin{pmatrix}\e\cr \f\end{pmatrix}
       \mapsto
       \begin{pmatrix}-\e\cr \f\end{pmatrix}
        $$
     (reflection with respect to $y$-axis)

     $$
             b) \varphi={\pi\over 2}, \,\, \tilde A_\varphi=
                    \begin{pmatrix}
      \cos\varphi & \sin\varphi \\
      \sin\varphi & -\cos\varphi\\
       \end{pmatrix}=\begin{pmatrix}
      0 & 1 \\
      1 & 0\\
       \end{pmatrix},\,\,
       \quad \begin{pmatrix}\e\cr \f\end{pmatrix}
       \mapsto
       \begin{pmatrix}\f\cr \e\end{pmatrix}
     $$
(reflection with respect to axis $y=x$ (``swapping'' of basis vectors))

Try to do it in general case.
}


%\end{document}     % 13-14 february

   \bigskip




\m


\subsection {Orientation in vector space}

    You heard  words ``orientation...'', ``''

    You heard expressions like: A basis  $\{\ac,\b,\c\}$ have the same orientation as the basis $\{\ac',\b',\c'\}$
     if they both obey right hand rule or if they both obey left hand rule.
     In the other case we say that these bases have opposite orientation...


    Try to give the exact meaning to these words.

   Note that in  three-dimensional Euclidean space except scalar (inner) product, one can consider
   another important operation: vector product.  The conception of 
orientation is indispensable for
   defining this operation.


   \bigskip
Consider the set of {\it all} bases in the  given vector space  $V$.


 Let  $(\e_1,\dots\e_n)$, $(\e'_1,\dots\e'_n)$ be two arbitrary bases in the vector space $V$
 and let $T$ be transition matrix
  which transforms the basis $\{\e_i\}$ to the new  basis $\{\e'_i\}$:
\begin{equation}\label{transitionmatrix1}
    \{\e'_1,\dots\e'_n\}=\{\e_1,\dots\e_n\}T\,,\qquad (\e'_i=\sum_{k=1}^n\e_k t_{ki})
\end{equation}
(see also \eqref{transformoforthogbasis}).




  {\bf Definition}
   We say that two bases
 $\{\e_1,\dots\e_n\}$ and $\{\e'_1,\dots\e'_n\}$ in $V$
   have {\it the same orientation}
  if  the determinant of transition matrix \eqref{transitionmatrix1} from the first basis to the second one is positive:
  $\det T>0$.

\smallskip

  \noindent We say that the basis $\{\e_1,\dots\e_n\}$ has an orientation
  {\it opposite to the orientation} of the basis $\{\e'_1,\dots\e'_n\}$
  (or in other words these two bases have opposite orientation)
  if the determinant of transition matrix from the first basis to the second one is negative: $\det T<0$.

{\bf Remark} Transition matrix from basis to basis is non-degenerate, hence its determinant cannot be equal to zero.
 It can be or positive or negative.

\m

\end{document}     % 12 February

  One can see that orientation establishes  the equivalence relation in the set of all bases. Denote this relation by ``$\sim$'':
                $
                \{\e_1,\dots\e_n\}\sim \{\e'_1,\dots\e'_n\}\,,
                $
  if two bases $\{\e_1,\dots\e_n\}$ and $\{\e'_1,\dots\e'_n\}$ have the same orientation, i.e.
  $\det T>0$ for transition matrix.

  Show that ``$\sim$'' is an equivalence relation, i.e. this
  relation is reflexive, symmetric and transitive.


     Check it:
   \begin{itemize}
   \item it is reflexive, i.e. for every basis $\{\e_1,\dots\e_n\}$
   \begin{equation}\label{reflex}
     \{\e_1,\dots,\e_n\}\sim \{\e_1,\dots,\e_n\}\,,
\end{equation}
     because in this case transition matrix $T=I$ and $det I=1>0$.
\item it is symmetric, i.e.

If  $\{\e_1,\dots,\e_n\}\sim \{\e'_1,\dots,\e'_n\}$ then $\{\e'_1,\dots,\e'_n)\sim \{\e_1,\dots,\e_n\}$,


because if $T$ is transition matrix from the first basis $\{\e_1,\dots,\e_n\}$ to the second
basis $\{\e'_1,\dots,\e'_n\}$: $\{\e'_1,\dots,\e'_n\}=\{\e_1,\dots,\e_n\}T$,

    \noindent then the transition matrix from the second basis $\{\e'_1,\dots,\e'_n\}$
    to the first basis $\{\e_1,\dots,\e_n\}$ is the inverse matrix $T^{-1}$:
    $\{\e_1,\dots,\e_n\}=\{\e'_1,\dots,\e'_n\}T^{-1}$.
    Hence $\det T^{-1}={1\over \det T}>0$ if $\det T>0$.

\item  Is transitive, i.e. if $\{\e_1,\dots,\e_n\}\sim \{\e'_1,\dots,\e'_n\}$ 
and $\{\e'_1,\dots,\e'_n)\sim\{\tilde \e_1,\dots, \tilde\e_n\}$, 
then one can see that
$\{\e_1,\dots,\e_n\}\sim \{\tilde \e_1,\dots, \tilde\e_n\}$.  

  Do it in detail.
For convenience call a basis $\{\e_1,\dots,\e_n\}$ the `I-st' basis,
 call a basis $\{\e'_1,\dots,\e'_n\}$ the `II-nd' basis and
  call a basis $\{\tilde \e_1,\dots, \tilde\e_n\}$ the `III-rd' basis.
   Let $T_{12}$ be a transition matrix from the I-st basis to the II-nd basis,
  $T_{13}$ be a transition matrix from the I-st basis 
 to the III-rd basis and $T_{23}$ be a transition matrix 
from the II-nd basis to the III-rd basis:
                        \begin{equation}\label{threetransitionmatrices}
                        \begin{matrix}
  \{\e'_1,\dots,\e'_n\}&=\{\e_1,\dots,\e_n\}T_{12}\cr
  \{\tilde\e_1,\dots,\tilde\e_n\}&=\{\e_1,\dots,\e_n\}T_{13}\cr
  \{\tilde\e_1,\dots,\tilde\e_n\}&=\{\e'_1,\dots,\e'_n\}T_{23},
                        \end{matrix}
                         \end{equation}
Hence $\{\tilde\e_1,\dots,\tilde\e_n\}=\{\e'_1,\dots,\e'_n\}T_{23}=$
                        $$
      \left(\{\e_1,\dots,\e_n\}T_{12}\right)T_{23}=
\{\e_1,\dots,\e_n\}T_{12}\circ T_{23}=\{\e_1,\dots,\e_n\}T_{13}.
                      $$
          We see that  $\underbrace{T_{13}}_{\hbox{I-st $\to$ III-rd}}=
 \underbrace{T_{12}}_{\hbox{I-st $\to$ II-nd}}\circ
  \underbrace{T_{23}}_{\hbox{II-nd $\to$ II-rd}}$:

                    \begin{equation}\label{relationbetweentransitionmatrices}
 T_{13}=T_{12} \circ T_{23}
            \Rightarrow
 \det T_{13}=\det (T_{12}\circ T_{23})=\det T_{12}\cdot \det T_{23}\,.
        \end{equation}
Transitivity immediately follows from this relation:
if I-st $\sim$ II and II-nd $\sim$ III-rd,
 then determinants of matrices $T_{12}$
and $T_{23}$ are positive. Hence according to relation
\eqref{relationbetweentransitionmatrices}
$\det T_{13}$ is positive too, i.e. I-st $\sim$ III-rd. 

  

   \end{itemize}

Since it is equivalence relation the set of all bases is 
a union if disjoint equivalence classes.
Two bases are in the same equivalence class if and only if 
they have the same orientation.



One can see that there are exactly two equivalence classes.

\m

{\bf Proposition}   {\it Let two bases $\{\e_1,\dots,\e_n\}$ and
 $\{\e'_1,\dots,\e'_n\}$ in vector space $V$ have opposite orientation.
  Let $\{\tilde\e_1,\dots,\tilde\e_n\}$ be an arbitrary basis in $V$.
 Then the basis $\{\tilde\e_1,\dots,\tilde\e_n\}$ and the first basis
  $\{\e_1,\dots,\e_n\}$
have the same orientation or the basis $\{\tilde\e_1,\dots,\tilde\e_n\}$
 and the second basis $\{\e'_1,\dots,\e'_n\}$ have the same orientation.
  In other words
 if $\{\e_1,\dots,\e_n\}$, $\{\e'_1,\dots,\e'_n\}$ and
 $\{\tilde\e_1,\dots,\tilde\e_n\}$
are three bases in vector space $V$ such that
 $\{\e_1,\dots,\e_n\}\not\sim\{\e'_1,\dots,\e'_n\}$ then
             \begin{equation}\label{twoorientations}
 \{\tilde\e_1,\dots,\tilde\e_n\}\sim \{\e_1,\dots,\e_n\}\,\,{\rm or}\,\,
 \{\tilde\e_1,\dots,\tilde\e_n\}\sim \{\e'_1,\dots,\e'_n\}\,.
                 \end{equation}
 There are two equivalence classes of bases with respect to orientation.
 An arbitrary basis belongs
to the equivalence class of the basis $\{\e_1,\e_2\dots,\e_n\}$,
or it belongs to the to the equivalence class of the basis
$\{\e'_1,\e_2\dots,\e'_n\}$ (in the case if bases
 $\{\tilde\e'_1,\dots,\tilde\e'_n\}$, $\{\tilde\e_1,\dots,\tilde\e_n\}$
have opposite orientation).}

Proof of the statement immediately follows from equations
 \eqref{threetransitionmatrices} 
and \eqref{relationbetweentransitionmatrices}. 
   In the same way like in these equations   
 we call a  basis $\{\e_1,\e_2\dots,\e_n\}$ the "I-st basis",
     a  basis  $\{\e'_1,\e'_2\dots,\e'_n\}$ the "II-nd basis" and
    a  basis  $\{\tilde\e_1,\tilde\e_2\dots,\tilde\e_n\}$ the "III-rd basis".
   Determinant of transition matrix  $T_{12}$ is negative since I-st and 
II-nd bases have opposite orientation. Then it follows from relation
\eqref{relationbetweentransitionmatrices} that determinants of
transition matrices $T_{13}$ and $T_{23}$ have opposite signs.
Hence $\det T_{13}>0$, i.e.  I-st and III-rd bases have the same orientation,
or  $\det T_{23}>0$,i.e II-nd and III-rd bases have 
the same orientation.\finish

         \m


{\bf Example} Let  $\{\e_1,\e_2\dots,\e_n\}$ be an arbitrary basis in $n$-dimensional vector space $V$.
 Swap the vectors $\e_1,\e_2$. We come to a new basis:
   $\{\e'_1,\e'_2\dots,\e'_n\}$
   \begin{equation}\label{swapping base vectors}
    \e'_1=\e_2, \e_2'=\e_1,\,\,\,\hbox {all other vectors are the same: $\e_3=\e'_3,\dots,\e_n=\e'_n$}
\end{equation}
We have:
 \begin{equation}\label{transitionm4}
\{\e'_1,\e'_2,\e'_3\dots,\e'_n\}=\{\e_2,\e_1,\e_3,\dots,\e_n\}=
      \{\e_1,\e_2,\e_3,\dots,\e_n\}T_{\rm swap}\,,
\end{equation}
where one can easy see that the determinant for transition matrix $T_{\rm swap}$ is equal to $-1$, i.e.
bases $\{\e_1,\e_2\dots,\e_n\}$ and $\{\e_2,\e_1\dots,\e_n\}$ have opposite orientation.

E.g. write down the  transition matrix \eqref{transitionm4} in the case if dimension of vector space is equal to $5$, $n=5$.
  Then we have $\{\e'_1,\e'_2,\e'_3,\e'_4,\e'_5\}=$ $\{\e_2,\e_1,\e_3,\e_4,\e_5\}=
      \{\e_1,\e_2,\e_3,\e_4,\e_5\}T$ where
                           \begin{equation}\label{transitionmatrixforswapping}
       \,\,\, T_{\rm swap}=
    \begin{pmatrix}
   0 &1 &0  &0 &0\cr
   1&0 &0 &0 &0\cr
 0 &0 & 1 &0 &0\cr
 0 &0 & 0 &1 &0\cr
0 &0 & 0 &0 &1\cr
\end{pmatrix}\qquad (\det T_{\rm swap}=-1)\,.
\end{equation}


\m

We see that bases $\{\e_1,\e_2\dots,\e_n\}$ and $\{\e'_1,\e'_2\dots,\e'_n\}$ have opposite orientation.

Hence according to Proposition above an arbitrary basis $\{\e'_1,\dots\e'_n\}$
have the same orientation as the basis  $\{\e_1,\e_2\dots,\e_n\}$, i.e. belongs to the equivalence
class of basis $\{\e_1,\e_2\dots,\e_n\}$, or it has the same orientation as the ``swapped''
 basis  $\{\e_2,\e_1\dots,\e_n\}$, i.e. it belongs to the equivalence
class of the ``swappedd'' basis $\{\e_2,\e_1\dots,\e_n\}$.



  The set of all bases is a union of two disjoint subsets.

  Any two bases which belong to the same subset  have the same orientation.
 Any two bases which belong to different subsets  have opposite orientation.

{\bf Definition} {\it An orientation of a vector  space is an equivalence class of bases in this vector space.}

    Note that fixing any basis we fix orientation, considering the subset of all bases
    which have the same orientation that the given basis.

   There are two orientations. Every basis has the same orientation as a given basis
or orientation opposite to the orientation of the given basis.

If we choose an arbitrary basis then all bases which belong to the equivalence class of this basis may be called
``left'' bases
 and all the bases which do not belong to the equivalence class of
this basis may be called ``right'' bases


\medskip

  {\bf Definition} {\it An oriented vector space is a vector space equipped with orientation.}




\m

Consider examples.

\m


{\bf Example} (Orientation in two-dimensional space).
 Let $\{\e_x,\e_y\}$ be  arbitrary  two bases in $\R^2$ and let
 $\ac,\b$ be  arbitrary two vectors in $\R^2$.
 Consider an ordered pair $\{\ac,\b,\}$.
  The transition matrix from the basis $\{\e_x,\e_y\}$ to the  ordered pair
  $\{\ac,\b\}$
  is $
           T=
                     \begin{pmatrix}
                 a_x   & b_x   \\
                  a_y   & b_y  \\
                  \end{pmatrix}
                      $:
                      $$
\{\ac,\b\}=\{\e_x,\e_y\}T= \{\e_x,\e_y\}   \begin{pmatrix}
                 a_x   & b_x   \\
                  a_y   & b_y  \\
                  \end{pmatrix},\quad
                  \begin{cases}
                  \ac=a_x\e_x+a_y\e_y\cr
                  \b=b_x\e_x+b_y\e_y\cr
                  \end{cases}
                      $$
One can see that the ordered pair  $\{\ac,\b\}$ also is a basis, (i.e. these two vectors
are  linearly independent in $\R^2$)
 if and only if  transition matrix is not degenerate, i.e.
 $\det T\not=0$. The  basis  $\{\ac,\b\}$ has the same orientation as the basis $\{\e_x,\e_y\}$ if
 $\det T>0$ and  the basis $\{\ac,\b\}$ has the  orientation opposite to the orientation
of the basis $\{\e_x,\e_y\}$ if $\det T<0$.

\m

{\bf Example}  Let $\{\e,\f\}$ be a basis in $2$-dimensional vector space.
  Consider bases $\{\e,-\f\}$, $\{\f,-\e\}$ and $\{\f,\e\}$.

1) We come to basis $\{\e,-\f\}$ reflecting the second basis vector.
  Transition matrix from initial basis $\{\e,\f\}$ to the
basis $\{\e,-\f\}$ is $T_{\{\e,-\f\}}=
   \begin{pmatrix} 1&0\cr 0&-1\cr\end{pmatrix}$. Its determinant is $-1$.
   Bases $\{\e,\f\}$ and $\{\e,-\f\}$ have opposite orientation.

\smallskip


2)  Transition matrix from initial basis $\{\e,\f\}$ to the
basis $\{\f,-\e\}$ is $T_{\{\f,-\e\}}=
   \begin{pmatrix} 0&-1\cr 1&0\cr\end{pmatrix}$. Its determinant is $1$.
   Bases $\{\e,\f\}$ and $\{\f,-\e\}$ have same orientation.
 We come to basis $\{\f,-\e\}$ rotating  the initial basis on the angle
$\pi/2$.

\smallskip

3) Transition matrix from initial basis $\{\e,\f\}$ to the
basis $\{\f,\e\}$ is $T_{\{\f,\e\}}=
   \begin{pmatrix} 0&1\cr 1&0\cr\end{pmatrix}$. Its determinant is $-1$.
   Bases $\{\e,\f\}$ and $\{\e,-\f\}$ have opposite orientation.

 We come to basis $\{\f,\e\}$ reflecting the initial basis.

\smallskip

We see that bases $\{\e,\f\}$ and
$\{\f,-\e\}$ have the same orientation; i.e. they belong to the same equivalenceclass. Bases $\{\e,-\f\}$ and
$\{\f,\e\}$ have the same orientation too, they belong to the another
 equivalence class. If we say that bases $\{\e,\f\}$ and $\{\f,-\e\}$
are {\it left} bases then bases  $\{\e,-\f\}$ and $\{\f,\e\}$
are {\it right} bases.

(There are plenty exercises in the Homework 3.)



\bigskip


{\bf Example}(Orientation in three-dimensional euclidean space.)
Let $\{\e_x,\e_y,\e_z\}$ be any basis in $\E^3$ and
 $\ac,\b,\bf \c$ are  arbitrary three vectors in $\E^3$:
                            $$
  \ac=a_x\e_x+a_y\e_y+a_z\e_z\,\,{\bf b}=b_x\e_x+b_y\e_y+b_z\e_z,\,\,\c=c_x\e_x+c_y\e_y+c_z\e_z\,.
                           $$
   Consider ordered triple $\{\ac,\b,\c\}$.
  The transition matrix from the basis $\{\e_x,\e_y,\e_z\}$ to the  ordered triple
  $\{\ac,\b,\bf \c\}$
  is $
           T=
                     \begin{pmatrix}
                 a_x   & b_x  &c_x \\
                  a_y   & b_y  &c_y\\
                   a_z   & b_z  &c_z\\
                  \end{pmatrix}
                      $:
                      $$
\{\ac,\b,\bf \c\}=\{\e_x,\e_y,\e_z\}T= \{\e_x,\e_y,\e_z\}
                    \begin{pmatrix}
                 a_x   & b_x  &c_x \\
                  a_y   & b_y  &c_y\\
                   a_z   & b_z  &c_z\\
                  \end{pmatrix}
                      $$
One can see that the ordered triple  $\{\ac,\b,\bf \c\}$ also is a basis, (i.e. these three vectors
are  linearly independent)
 if and only if  transition matrix is not degenerate $\det T\not=0$.
 The  basis  $\{\ac,\b,\c\}$ has the same orientation as the basis $\{\e_x,\e_y,\e_z\}$ if
              \begin{equation}\label{conditionofsameorientation1}
                      \det T>0\,.
                   \end{equation}
The basis $\{\ac,\b,\c\}$ has the  orientation opposite to the orientation
of the basis $\{\e_x,\e_y,\e_z\}$ if
        \begin{equation}\label{conditionofoppositeorientation1}
                      \det T<0\,.
                    \end{equation}


\m

{\bf Remark} Note that in the example above we considered in $\E^3$
{\it arbitrary bases} not necessarily orthonormal bases.





\m





Relations \eqref{conditionofsameorientation1},\eqref{conditionofoppositeorientation1}
 define equivalence relations in the set of bases.
 Orientation is equivalence class of bases.
There are two orientations, every basis has the same orientation as a given basis
or opposite orientation.

\bigskip

{\small If two bases $\{\e_i\}$, $\{\e_{i'}\}$ have the same
orientation then they can be transformed to each other by continuous
transformation, i.e. there exists one-parametric family of
bases $\{\e_i(t)\}$ such that $0\leq t\leq 1$
and $\{\e_i(t)\}\vert_{t=0}=\{\e_i\}$, $\{\e_i(t)\}\vert_{t=1}=\{\e_{i'}\}$. (All functions $\e_i(t)$ are continuous)
In the case of three-dimensional space the following statement is
true :
{\it Let  $\{\e_{i}\},\{\e_{i'}\}$ $(i=1,2,3)$
be two orthonormal bases in $\E^3$ which have the same orientation. Then there
exists an axis $\n$ such that basis $\{\e_{i}\}$ transforms to the basis $\{\e_{i'}\}$
under rotation around the axis.}(This is Euler Theorem (see it later).}

\m

{\bf Exercise} Show that bases $\{\e,\f,\g\}$ and  $\{\f,\e,\g\}$
have opposite orientation but bases $\{\e,\f,\g\}$ and  $\{\f,\e,-\g\}$
have the same orientation.


\m

{\sl Solution}. Transformation from basis $\{\e,\f,\g\}$ to basis
  $\{\f,\e,\g\}$ is ``swapping'' of  vectors
      ($(\e,\f)\mapsto (\f,\e)$. This is reflection
and this transformation changes orientation. One can see it using transition matrix:
                     $$
   T\colon \{\f,\e,\g\}=\{\e,\f,\g\}T=\{\e,\f,\g\}
                   \begin{pmatrix}
                 0   & 1  &0 \\
                  1   & 0  &0\\
                   0   & 0  &1\\
                  \end{pmatrix}\,. \det T=-1
                     $$
Transformation from basis $\{\e,\f,\g\}$ to basis
  $\{\f,\e,-\g\}$ is composition of two transformations:
 ``swapping'' of  vectors
      ($(\e,\f)\mapsto (\f,\e)$ and changing direction of vector
   $\g$ ($\g\mapsto-\g$). We have two reflections:
                 $$
\{\e,\f,\g\}\quad
    {\buildrel  \hbox {\footnotesize reflection }\over \longrightarrow}
             \quad
              \{\f,\e,\g\}\quad
    {\buildrel  \hbox {\footnotesize reflection }\over \longrightarrow}
             \quad
              \{\f,\e,-\g\}
              $$
Any reflection changes orientation.
Two reflections preserve orinetation.
 One may come to this result  using transition matrix:
                     \begin{equation}\label{examplerotation=1}
   T\colon \{\f,\e,-\g\}=\{\e,\f,\g\}T=\{\e,\f,\g\}
                   \begin{pmatrix}
                 0   & 1  &0 \\
                  1   & 0  &0\\
                   0   & 0  &-1\\
                  \end{pmatrix}\,. \det T=1.\quad
   \hbox{Orientation is not changed.}
                     \end{equation}






(See also exercises in Homework 3)



%\end{document} %19 February








\subsubsection {Orientation of linear operator}.
Let $P$ be invertible linear operator, i.e. $\det P\not=0$.

If a linear operator $P$
 acting on the space $V$ has positive determinant
then under the action of this operator an  arbitrary basis
  $\{\e_1,\dots,\e_n\}$
transforms to the new basis $\{\e'_1,\dots,\e'_n\}$
such that transition matrix
from basis $\{\e_1,\dots,\e_n\}$ to the new basis $\{\e'_1,\dots,\e'_n\}$
has positive determinant, i.e. these bases have the same orientation.
Respectively  if a linear operator $P$
 acting on the space $V$ has negative determinant
then under the action of this operator an  arbitrary basis
  $\{\e_1,\dots,\e_n\}$
transforms to the new basis $\{\e'_1,\dots,\e'_n\}$
such that transition matrix
from basis $\{\e_1,\dots,\e_n\}$ to the new basis $\{\e'_1,\dots,\e'_n\}$
has negative  determinant, i.e. these bases have opposite orientation.
Thus we can  define does the linear operator $P$ acting in the
vector space $V$ change an orientation
or it does not change an orientation of this vector space.



  {\bf Definition}.
    Non-degenerate (invertible) linear operator $P$ ($\det P\not=0$)
    acting in vector space $V$
    preserves  an orientation of the vector space $V$
   if $\det P>0$. It changes the orientation
   if $\det P<0$.


If $\{\e_1,\dots,\e_n\}$ is an arbitrary basis which transforms
to the new basis $\{\e'_1,\dots,\e'_n\}$ under the action of nvertible operator $P$: $\e'_i=P(\e_i)$ then these bases
have the same orientation if and only if operator $P$ preserves
an orientation, i.e.
 $\det P>0$, and these bases have opposite orientation if and only if
 the operator $P$ changes an orientation, i.e. $\det P<0$.


\smallskip


{\footnotesize
  The matrix $P=||p_{ij}||$  is the transition matrix from the basis $\{\e_1,\dots,\e_n\}$
  to the basis $\{\e'_1,\dots,\e'_n\}$. For an arbitrary vector $\x$
             \begin{equation*}
 \forall \x=\sum_{i=1}^n\e_i x^i=(\e_1,\e_2,\dots,\e_n)\cdot
 \begin{pmatrix}
 x^1\cr x^2\cr \dots \cr x^n\cr
 \end{pmatrix}
             \end{equation*}
\begin{equation*}\label{linearoperatorin terms oftransition matrix2}
 P\x=(\e_1,\e_2,\dots,\e_n)\cdot P\cdot
 \begin{pmatrix}
 x^1\cr x^2\cr \dots \cr x^n\cr
 \end{pmatrix}=\sum_{i=1}^n\e'_i x^i=\sum_{i,k=1}^n\e_k p_{ki} x^i\,.
             \end{equation*}




If $x^i$ are components of vector $\x$ at the basis $\{\e_1,\dots,\e_n\}$
and  $x'^i$ are components of the vector $\x$ at the new basis
$\{\e'_i\}$ then $x'^i=\sum_i p_{ik}x^k$.

\m

}



\subsection {Rotations and orthogonal operators preserving 
orientation of $\E^n$ (n=2,3)}

Orthogonal operators preserving  orientation in $\E^2$ and $\E^3$ are rotations.
We try to explain this.

%\end{document}  % 20 February


Let $\E^n$ be oriented vector space. Recall that oriented vector space means
    that it is chosen the equivalence class of bases: all bases in this class have the same orientation.
    We call all bases in the equivalence class defining orientation ``left'' bases.
    All ``left'' bases have the same orientation.
    To define an orientation in vector space $V$ one may consider an arbitrary basis $\{\e^{(0)}_i\}$ in
    $V$ and claim that this basis is ``left''
    basis. The basis $\{\e^{^{(0)}}_i\}$ defines equivalence class of ``left'' bases:
    all bases $\{\e_i\}$ such that $\{\e_i\}\sim \{\e_i^{^{(0)}}$ will be called ``left'' bases.
     We  can say that basis  $\{\e_i^{(0)}\}$ defines the orientation.

    Later on considering oriented vector space we usually
  call all bases defining the orientation (i.e. belonging to the equivalence
class of bases defining orientation) ``left'' bases.

    Now we define rotation of oriented $\E^2$ and oriented $\E^3$.

    {\bf Definition}  Let $\E^2$ be an oriented Euclidean space.
     We say that linear operator $P$ rotates this space on an angle ``$\varphi$''
     if for a given ``left'' orthonormal  basis $\{\e,\f\}$
                           \begin{equation}\label{rotationofn=2}
                                 \begin{cases}
                      \e'=P(\e)=\e\cos\varphi+\f\sin\varphi\cr
                      \f'=P(\f)=-\e\sin\varphi+\f\cos\varphi\cr
                                \end{cases}\quad
                                {\rm i.e.}\quad
                                \{\e',\f'\}=\{\e,\f\}
                                 \begin{pmatrix}
                                \cos\varphi &-\sin\varphi\cr
                               \sin\varphi &\cos\varphi\cr
                              \end {pmatrix}
                                 \end{equation}
i.e. transition matrix from basis $\{\e,\f\}$ to new basis
$\{\e'=P(\e),\f'=P(\f)\}$
is the rotation matrix \eqref{orthogonalrotation} (see also \eqref{rotationofbasisontheangle}).


{\bf Remark}  One can show that the angle of rotation
does not depend on the choice of ``left'' basis.
If we will choose another
left basis ${\tilde\e,\tilde\f}$ then the angle remains the same

%{\footnotesize Indeed according to \eqref{orthogonalrotation}
%there exists an angle $\varphi'$ such that
%$\{\tilde\e,\tilde\f\}=\{\e,\f\}A_{\theta}$.  Hence
%                            $$
%\{\tilde\e',\tilde\f'\}=\{P(\tilde\e),
%P(\tilde\f)\}=\{\tilde\e,\tilde\f\}A^{-1}_{\varphi'}A_{\varphi}A_{\varphi'}
%       =\{\tilde\e,\tilde\f\}A_{-\varphi'}A_{\varphi}A_{\varphi'}=
%       \{\tilde\e,\tilde\f\}A_{-\varphi'\varphi+\varphi'}=
%       \{\tilde\e,\tilde\f\}A_{\varphi}.
%              $$}.
Operator $P$ rotates every vector rotates on the angle $\varphi$.

If we choose a basis with opposite orientation
(``right'' basis) then the angle will change: $\varphi\mapsto-\varphi$.


\m

We see from formula \eqref{rotationofn=2} that
the matrix of operator $P$ is orthogonal matrix
 such that its determinant equals $1$.
 On the other hand we proved that all orthogonal $2\times 2$
 matrices $A$ such that $\det A=1$ have the appearance
\eqref{rotationofn=2}  (see the subsection 1.8).
 Hence in $2$-dimensional case we come
 to the folowing simple

   {\bf Proposition} Let $P$ be an orthogonal operator in oriented $2$-dimensional Euclidean space.
   If  operator $P$ preserves orientation ($\det P=1$) then it is a rotation operator \eqref{rotationofn=2}
   on some angle $\varphi$.
\m

The situation is little bit more tricky in $3$-dimensional case.

%23 February

%\end{document}



Let $\E^3$ be an Euclidean vector space.
(Problem of orientation we will discuss below.)
Let $\N\not=0$ be
 an arbitrary non-zero vector in $\E^3$.
Consider the line $l_\N$, spanned by vector $\N$.
 This is {\it axis} directed along the vector $\N$.
Choose a unit vector
          \begin{equation}\label{unitvectoEuler}
          \n=\pm {\N\over |\N|}
          \end{equation}
{\footnotesize Vector $\n$ fixes an orientation on $l_\N$. Changing
$\n\mapsto-\n $ changes an orientation on opposite)}.




Choose an arbitrary orthonormal basis such that first vector of this basis
is directed along the axis: a basis $\{\n,\f,\g\}$.


{\bf Definition}
We say that a linear operator $P$ rotates the space $\E^3$
on the angle $\varphi$ with respect to
an axis $l_\N$  directed along a vector $\N$ if
the following conditions are satisfied:
\begin{itemize}
\item
      $$
  P(\N)=\N
      $$
vector $\N$ (and all vectors proportional to this vector)
are eigenvectors of operator $P$ with eigenvalue $1$, i.e.
axis remain intact

\item

for a basis $\{\n,\f,\,\g\}$
 such that the first vector of this basis is equal to $\n$,
($\n$ is a unit vector, proportional to $\N$)
    \begin{equation}\label{rotationofn=3}
                                 \begin{cases}
                      \f'=P(\f)=\f\cos\varphi+\g\sin\varphi\cr
                      \g'=P(\f)=-\f\sin\varphi+\g\cos\varphi\cr
                                \end{cases}\quad
                                {\rm i.e.}\quad
                                \{\f',\g'\}=\{\f,\g\}
                                 \begin{pmatrix}
                                \cos\varphi &-\sin\varphi\cr
                               \sin\varphi &\cos\varphi\cr
                              \end {pmatrix}\,.
                                 \end{equation}
  In other words  plane (subspace) orthogonal to axis
rotates on the angle $\varphi$:
        linear operator $P$ rotates every vector orthogonal to axis
         on the angle $\varphi$ in the plane (subspace) orthogonal to the axis.
\end{itemize}




   Linear operator $P$ transforms the basis $\{\n,,\f,\g\}$ to
the new basis
     $\{\n,\f',\g'\}$ $=\{\n,\f\cos\varphi+\g\sin\varphi,-\f\sin\varphi+\g\cos\varphi\}$.
     The matrix of operator $P$, i.e. the transition matrix from the basis $\{\n,,\f,\g\}$ to the basis
     $\{\n,\f',\g'\}$ is defined by the relation:
                         \begin{equation}\label{ttt4}
   \{\n,\f',\g'\}=\{\n,\f\cos\varphi+\g\sin\varphi,
    -\f\sin\varphi+\g\cos\varphi\}=
   \{\n,,\f,\g\}\begin{pmatrix}
   1 &0 &0\cr
   0 &\cos \varphi& -\sin\varphi\cr
   0&\sin \varphi&\cos\varphi\cr
   \end{pmatrix}
                          \end{equation}
Recalling definition \eqref{traceofoperator}
of trace of linear operator we come to
the following relation
       \begin{equation}\label{eulerformula2}
    Tr P=1+2\cos\varphi
       \end{equation}
 where $\varphi$ is angle of rotation. Note that Trace of the operator
 does not depend on the choice of the basis. This formula express
 cosine of the angle of rotation in terms of operator, irrelevant of the choice of the basis.

{\footnotesize
{\bf Remark} If we change orientation then $\varphi\mapsto-\varphi$.
For non-oriented Euclidean
   space rotation is defined up to a sign\footnote{ Does it recall
you expressions such as ``clockwise'', ``anticlock-wise'' rotation?}

Careful reader maybe already noted that even fixing the orientation of
$\E^3$ does not fix the ``sign'' of the angle:
If we change the orientation of the axis (changing $\n\mapsto-\n$)
then changing the corresponding ``left'' basis will imply
that $\varphi\mapsto-\varphi$.
{\footnotesize In fact angle $\varphi$ is the angle of rotation of oriented plane
which is orthogonal to the axis of rotation. Orientation on the
plane is defined by orientation in $\E^3$ and orientation of
the axis which is orthogonal to this plane.}
In the case of $3$-dimensional space sign of the angle
depends not only on orientation of $\E^3$ but on orientation
of axis. In what follows we will ignore this. This means that
we define rotation on the angle $\pm \varphi$ up to a sign....
Rotation is defined for operators preserving orientation.
The difference between angles of rotations $\varphi$ and $-\varphi$
is depending not only on orientation of $\E^3$ but on orientation of
axis too. But we ignore this difference. Note that
$\cos\varphi$ in the formula \label{eulerformula2}
is defined up to a sign

 }
\m

  Rotation operator eviently is orthogonal operator preserving orientation.
  Is it true converse implication?
   We are ready to formulate the following remarkable result.



   {\bf Theorem} (the Euler Theorem)
{\it Let $P$ be an orthogonal operator
preserving an orientation of Euclidean space $\E^3$, i.e. operator
$P$ preserves the scalar product and orientation.
Then it is a
rotation operator with respect to an axis $l$ on the angle $\varphi$.
Every vector $\N$ directed along the axis does not change,
i.e. the axis is $1$-dimensional space of eigenvectors with eigenvalue
$1$, $P(\N)=\N$.
Every vector orthogonal to axis rotates on the angle $\varphi$
 in the plane orthogonal to the axis,
             $$
      {\rm Tr\,}P=1+2\cos\varphi\,.
             $$
The angle $\varphi$ is defined up to a sign. Changing orientation of
the Euclidean space and of the axis change sign of $\varphi$.

}


This Theorem can be restated in the following way:
every orthogonal operator
$P$ preserving orientation, ($\det P\not=0$) has an eigenvector $\N\not=0$
with eigenvalue $1$. This eigenvector defines the axis of rotation.
In an orthonormal basis $\{\n,\f,\g\}$ where $\n$ is a unit vector
along the axis, the transition matrix of operator has an appearance
\eqref{ttt4}. Angle of rotaion can be defined via Trace of operator
by formula $ {\rm Tr\,}P=1+2\cos\varphi$.

{\bf Remark} If $P$ is an identity operator, $P=I$  then `` there is no rotation'', more precisely:
any line can be considered as an axis of rotation (every vector is eigenvector of identity matrix
with eigenvalue $1$) and angle of rotation is equal to zero.
If $P\not=I$ then axis of rotation is defiend uniquely.

  {\footnotesize {\sl Proof of the Euler Theorem}.
The proof of the Euler Theorem has two parts. First and central part
  is to prove the existence of the axis.
  The rest is trivial: we take an arbitrary orthonormal
basis
  ${\n,\f,\g}$ such that $\n$ is eigenvector
and we come to relation  \eqref{rotationofn=3}.
We expose here maybe the most beautiful proof which belongs to Coxeter.

Let $P$ be linear orthogonal operator preserving orientation.
Note that for any two
not-zero distinct vectors $\e,\f$ one can consider
orthogonal operator $R_{\e,\f}$ which changes orientation and
swaps the vectors $\e,\f$: it is reflection with respect to
the plane spanned by the
vectors $\e+\f$ and a vector $\e\times \f$.


Let $\{\e,\f,\g\}$ be an arbitrary  orthonormal basis in $\E^3$ and
let ${\e',\f',\g'}$ be image of this basis under operator $P$
               $$
    P(\e)=\e',\,\,P(\f)=\f'\,\,P(\g)=\g'\,.
                $$
If $\e=\e'$ nothing to prove ($\e$ is eigenvector with eigenvalue $1$).
If this is not the case, apply reflection operator $R_{\e,\e'}$ to the
initial basis $\{\e,\f,\g\}$ we come to the orthonormal basis
$\{\e',\tilde f,\tilde g\}$, Then applying reflection operator
$R_{\tilde f,\f'}$ to this basis we come to the basis
${\e',\f',\tilde {\tilde g}}$. The third vector has no choice it has
to be equal to $\g'$ since in the case if it is equal to $-\g'$
orientation is opposite. Hence we see that
operator $P$ is the product of two reflections operators.
Consider the line $l$, intersection of these planes, we come to
eigenvectors with eigenvalue $1$.\finish

There are many other proofs, for example:

{\sl Another proof:}  Any non-degenerate $3\times 3$
matrix has at least one eigenvector $\x$: $P\x=\lambda \x$,
 since cubic equation $\det(P-\lambda I)=0$ has at lest one real root.
Since $P$ is orthogonal operator, then $\lambda=\pm1$.  If $\lambda=1$,
then $\x$ defines the axis. If $\lambda=-1$, $P\x=-\x$, then
eigenvector with eigenvalue $1$ belongs to the plane orthogonal to $\x$.\finish


   }

\m


{\bf Example}
   Consider linear operator $P$ such that for orthonormal basis
   $\{\e_x,\e_y,\e_z\}$
                    \begin{equation}\label{firstexampleofrotation}
           P(\e_x)=\e_y, P(\e_y)=\e_x, P(\e_z)=-\e_z
                    \end{equation}
     This is obviously orthogonal operator since it transforms orthogonal basis to orthogonal one.
This operator swaps first two vectors and reflects the third one. It preserves orientation: matrix of operator in the basis $\{\e_x,\e_y,\e_z\}$,
 i.e. the transition matrix from the basis $\{\e_x,,\e_y,\e_z\}$ to the basis
     $\{P(\e_x),P(\e_y),P(\e_z)\}$ is defined by the relation:
                         \begin{equation*}\label{transitionmatrixforrotation4}
   \{P(\e_x),P(\e_y),P(\e_z)\}=\{\e_y,\e_x,-\e_z\}=
   \{\e_x,,\e_y,\e_z\}\begin{pmatrix}
   0 &1 &0\cr
   1 &0& 0\cr
   0&0&-1\cr
   \end{pmatrix}
                          \end{equation*}
  $\det P=1$.
  This operator preserves orientation. Hence by Euler Theorem it is a rotation.
  Find first axis of rotation.
  It is easy to see from \eqref{firstexampleofrotation} that
  $\N=\lambda (\e_x+\e_y)$ is eigenvector with eigenvalue $1$:
                   $$
                  P(\N)=P(\e_x+\e_y)=\e_y+\e_x=\N\,.
                   $$
Hence axis of rotation is directed along the vector $\e_x+\e_y$.
${\rm Tr\,}P=1+2\cos\varphi=0$. hence angle of rotation $\varphi=\pm {\pi\over 2}$.

{\footnotesize One can calculate explicitly angle of rotation:
  Consider orthonormal basis $\{\n,\,f,\g\}$ adjusted to the axis
  ($\n||\N$). We have that $\n={\e_x+\e_y\over \sqrt 2}$
 since $\n$ is proportional to $\N$ and it is unit vector.
  Choose $\f={-\e_x+\e_y\over \sqrt 2}$ and $\g=\e_z$.
Then it is easy to see that
                  $$
  \{\n,\f,\g\}=\left\{{\e_x+\e_y\over \sqrt 2},{-\e_x+\e_y\over \sqrt 2},\g\right\}
            $$
   is orthonormal basis.Using \eqref{firstexampleofrotation}one can see
       that
                  $$
P(\n)=P\left({\e_x+\e_y\over \sqrt 2}\right)={\e_y+\e_x\over \sqrt 2}=\n\,,
                  $$
                  $$
   P(\f)=P\left({-\e_x+\e_y\over \sqrt 2}\right)={-\e_y+\e_x\over \sqrt 2}=-\f,\quad
                P(\g)=-\g
                  $$
We see that
          $$
   \{\n,\f,\g\}{\buildrel P\over \longrightarrow}\{\n,-\f,-\g\}\,.
          $$
         Comparing with \eqref{rotationofn=3} and \eqref{ttt4}
 we see that the operator $P$ is rotation of $\E^3$ on the angle $\pi$ with respect to the axis
         directed along the vector $\e_x+\e_y$.



    }

%\end{document}


  \subsection{ Vector product in oriented $\E^3$}


Now we give a definition of vector product of vectors in $3$-dimensional
Euclidean space equipped with orientation.

\m

 Let $\E^3$ be three-dimensional oriented Euclidean space, i.e. Euclidean space equipped with an equivalence class
 of bases with the same orientation. To define the orientation it suffices to consider just one
 orthonormal basis
 $\{\e,\f,\g\}$ which is claimed to be left basis. Then the equivalence class of the left bases is a
 set of all bases which have the same orientation as the basis $\{\e,\f,\g\}$.



\m
  {\bf Definition}  Vector product  $L(\x,\y)=\x\times \y$ is a  function of two vectors
   which takes vector values
   such that the following axioms (conditions) hold
   \begin{itemize}
   \item
     The vector $L(\x,\y)=\x\times\y$ is orthogonal to vector $\x$ and vector $\y$:
      \begin{equation}\label{vectorproductaxiom1}
   (\x\times \y) \perp \x\,,\quad (\x\times \y)\perp \y
\end{equation}
   In particular it is orthogonal to the
     the plane spanned by the vectors
     $\x,\y$ (in the case if vectors $\x,\y$ are  linearly independent)

\item

\begin{equation}\label{vectorproductaxiom2}
    \x\times \y=-\y\times \x , \qquad \hbox{(anticommutativity condition)}
\end{equation}
     \item
\begin{equation}\label{vectorproductaxiom3}
    (\lambda\x+\mu \y)\times {\bf z}=\lambda (\x\times {\bf z})+\mu (\y\times {\bf z}) \, ,
     \qquad \hbox{(linearity condition)}
\end{equation}
     \item
If vectors $\x,\y$ are perpendicular each other then the magnitude of the vector $\x\times \y$
is equal to the area of the rectangle formed by the vectors  $\x$ and $\y$:
\begin{equation}\label{vectorproductaxiom4}
    \vert\x\times \y\vert=\vert\x\vert\cdot\vert\ \y\vert\,,\qquad {\rm if}\,\, \x\perp \y\,, i.e. (\x,\y)=0\,.
\end{equation}
\item
 If the  ordered triple of the vectors $\{\x,\y, {\bf z}\}$,
 where ${\bf z}=\x\times \y$ is a basis,
  then this basis and an orthonormal basis $\{\e,\f,\g\}$ defining
orientation of $\E^3$ have the same orientation:
\begin{equation}\label{vectorproductaxiom5}
\{\x,\y, {\bf z}\}=\{\e,\f,\g\}T, \,\, \hbox {where for transition matrix $T$, $\det T>0$.}
\end{equation}




  Vector product depends on orientation in Euclidean space.

  \end{itemize}

{\it Comments on conditions (axioms) \eqref{vectorproductaxiom1}---\eqref{vectorproductaxiom5}}:

\m



1. The condition \eqref{vectorproductaxiom3} of linearity of vector product with respect to the first argument
   and the condition \eqref{vectorproductaxiom2} of anticommutativity
   imply that vector product is an operation which is
  linear with respect to the
  second argument too. Show it:
        \begin{equation*}\label{}
 {\bf z}\times(\lambda\x+\mu \y)=-(\lambda\x+\mu \y)\times {\bf z}=
 -\lambda (\x\times {\bf z})-\mu (\y\times {\bf z})=
  \lambda ({\bf z}\times \x)+\mu({\bf z}\times \y)\,.
\end{equation*}

Hence vector product is bilinear operation. Comparing with scalar product we see that
  vector product is bilinear anticommutative (antisymmetric) operation which takes vector values,
   while scalar product is bilinear symmetric operation which takes real values.

\m

2. The condition of anticommutativity immediately implies that
      vector product of two colinear (proportional) vectors $\x,\y$ ($\y=\lambda \x$) is equal to zero.
  It follows from linearity and anticommuativity conditions. Show it:   Indeed
                 \begin{equation}\label{colinearvectors}
   \x\times \y=\x\times (\lambda \x)=\lambda (\x\times \x)=-\lambda (\x\times\x)=-\x\times (\lambda \x)=-\x\times \y.
\end{equation}
 Hence $\x\times \y=0$, if $\y=\lambda \x$\finish.



3. It is very important to emphasize again that vector product depends on orientation.
According the condition \eqref{vectorproductaxiom5} if $\z=\x\times \y$ and
we change the orientation of Euclidean space, then $\z\to -\z$ since the basis $\{\x,\y,-\z\}$
as an orientation opposite to the orientation of the basis  $\{\x,\y,\z\}$.
\m




{\footnotesize   You may ask a question:
Does this operation (taking the vector product) which obeys all the conditions (axioms)
\eqref{vectorproductaxiom1}---\eqref{vectorproductaxiom5} exist? And if it exists is it unique?
  We will show that the vector product is well-defined by the axioms
  \eqref{vectorproductaxiom1}---\eqref{vectorproductaxiom5}, i.e. there exists an operation
  $\x\times y$ which obeys the axioms  \eqref{vectorproductaxiom1}---\eqref{vectorproductaxiom5}
  and these axioms define the operation uniquely.}

\m





 We will   assume  first that there exists an  operation $L(\x,\y)=\x\times \y$
 which obeys all the axioms \eqref{vectorproductaxiom1}---\eqref{vectorproductaxiom5}.
 Under this assumption we will construct explicitly this operation (if it exists!). We will see that
 the operation that we constructed indeed obeys
 all the axioms \eqref{vectorproductaxiom1}---\eqref{vectorproductaxiom5}.


Let $\{\e_x,\e_y,\e_z\}$ be an {\it arbitrary } left
orthonormal basis of oriented Euclidean space $\E^3$, i.e. a basis
which belongs to the equivalence class of the basis $\{\e,\f,\g\}$
  defining orientation of $\E^3$.
Then it follows from the considerations above for vector product that
   \begin{equation}\label{vectorproductofbasisvectors}
   \begin{matrix}
    &\e_x\times \e_x=0,\,&\e_x\times \e_y=\e_z,\,&\e_x\times \e_z=-\e_y \\
&\e_y\times \e_x=-\e_z,\,&\e_y\times \e_y=0,\,&\e_y\times \e_z=\e_x \\
&\e_z\times \e_x=\e_y,\,&\e_z\times \e_y=-\e_x,\,&\e_z\times \e_z=0
          \end{matrix}
   \end{equation}
  E.g. $\e_x\times \e_x=0$, because of \eqref{vectorproductaxiom2},
    $\e_x\times \e_y$ is equal to $\e_z$ or to $-\e_z$ according to
    \eqref{vectorproductaxiom4}, and according to orientation arguments \eqref{vectorproductaxiom5}
      $\e_x\times \e_y=\e_z$.

      \m


 Now it follows from linearity and \eqref{vectorproductofbasisvectors} that
 for two arbitrary vectors
 $\ac=a_x\e_x+a_y\e_y+a_z\e_z$, $\b=b_x\e_x+b_y\e_y+b_z\e_z$
              $$
   \ac\times \b=(a_x\e_x+a_y\e_y+a_z\e_z)\times (b_x\e_x+b_y\e_y+b_z\e_z)=
   a_xb_y\e_x\times\e_y+a_xb_z\e_x\times \e_z+
                 $$
                 $$
   a_yb_x\e_y\times\e_x+a_yb_z\e_y\times\e_z+a_zb_x\e_z\times \e_x+a_zb_y\e_z\times\e_y=
              $$
            \begin{equation}\label{defofvectorproduct}
            (a_yb_z-a_zb_y)\e_x+(a_zb_x-a_xb_z)\e_y+(a_xb_y-a_yb_x)\e_z\,.
            \end{equation}
        It is convenient to represent this formula in the following very familiar way:
         \begin{equation}\label{defofvectorproduct}
           L(\ac,\b)=\ac\times \b=
                \det
                \begin{pmatrix}
                \e_x  &\e_y  &\e_z \\
                  a_x   & a_y  &a_z \\
                  b_x   & b_y  &b_z
                  \end{pmatrix}
            \end{equation}
\m



{\footnotesize  We see that the operation $L(\x,\y)=\x\times \y$ which obeys all the axioms
     \eqref{vectorproductaxiom1}---\eqref{vectorproductaxiom5}, if it exists,  has
     an appearance \eqref{defofvectorproduct}, where $\{\e_x,\e_y,\e_z\}$
     is an arbitrary orthonormal basis  (with rightly chosen orientation).
On the other hand  using the properties of determinant and the fact that
  vectors are orthogonal if and only if their scalar product equals to zero
  one can easy see that the vector product defined by this formula
  indeed obeys all the conditions \eqref{vectorproductaxiom1}---\eqref{vectorproductaxiom5}.

  Thus we proved that the vector product is well-defined by the axioms
  \eqref{vectorproductaxiom1}---\eqref{vectorproductaxiom5} and it is given
  by the formula \eqref{defofvectorproduct} in  an arbitrary
  orthonormal basis  (with rightly chosen orientation).}



   {\bf Remark} In the formula above we have chosen an arbitrary orthonormal basis which belongs to the equivalence
  class of bases defining the orientation.  What will happen if we
   choose instead the basis $\{\e_x,\e_y,\e_z\}$
  an arbitrary orthonormal basis $\{\f_1,\f_2,\f_3\}$.
  We see that  such that answer does not change if both bases
  $\{\e_x,\e_y,\e_z\}$ and $\{\f_1,\f_2,\f_3\}$ have the same orientation,
  Formulae \eqref{vectorproductofbasisvectors} are valid for an arbitrary orthonormal basis which have the same orientation as
the orthonormal basis $\{\e_x,\e_y,\e_z\}$.--- In oriented Euclidean space $\E^3$ we may take an arbitrary basis
from the equivalence class of bases defining orientation.
   On the other hand if we will consider the basis with opposite orientation then according to the axiom
   \eqref{vectorproductaxiom5} vector product will change
  the sign. (See also the question 6 in Homework 4)


\m
  \subsubsection{Vector product---area of parallelogram}

 The following Proposition states that vector product can be considered as area of parallelogram:

   {\bf Proposition 2}  {\it The modulus of the vector $\z=\x\times \y$ is equal to the area of
   parallelogram formed by the vectors $\x$ and $\y$.}:
   \begin{equation}\label{areaofparallelogram}
    S(\x,\y)=|\x\times \y|\,,
\end{equation}
where we denote by  $S(\x,\y)$ the area of parallelogram formed by the vectors $\x,\y$.
   \m


{\it Proof:}  Consider the expansion $\y=\y_{||}+\y_\perp$, where the vector $\y_\perp$ is orthogonal to the vector
     $\x$ and the vector  $\y_{||}$ is parallel to to vector $\x$.  The area of the parallelogram formed by
     vectors $\x$ and $\y$  is equal to the product of the length of
       of the vector $\x$ on the height. The height is equal to the length of the vector $\y_\perp$.
       We have $S(\x,\y)=|\x||\y_\perp|$. On the other
       $\z=\x\times \y=\x\times (\y_{||}+\y_\perp)$ $=\x\times \y_{||}+\x\times \y_\perp$.
       But $\x\times \y_{||}=0$, because these vectors are colinear.
       Hence  $\z=\x\times \y_\perp$ and $|\z|=|\x||\y_{\perp}|=S(\x,\y)$ because vectors $\x,\y_{\perp}$ are
       orthogonal to each other.


  This Proposition is very important to understand the meaning of vector product.
   Shortly speaking {\it vector product of two vectors is a vector which is orthogonal to the
   plane spanned by these vectors, such that its magnitude is equal to the area of the parallelogram
   formed by these vectors. The direction is defined by orientation.}

{\footnotesize {\bf Remark}  It is useful sometimes to consider area of parallelogram not as a positive number but
as an real number positive or negative (see the next subsubsection.)
}


   It is not worthless to recall the formula which we know from the school that
   area of parallelogram formed by vectors $\x,\y$
   equals to the product of the base on the height. Hence
                  \begin{equation}\label{defofvectorproduct2}
                    |\x\times \y|=|\x|\cdot |\y||\sin\theta|\,,
                  \end{equation}
                 where $\theta$ is an angle between vectors $\x,\y$.
\m




Finally I would like again to stress:

  Vector product of two vectors is equal to zero if these vectors are colinear (parallel).
  Scalar product of two vectors is equal to zero if these vector are orthogonal.


\m

{\bf Exercise$^\dagger$}{\footnotesize Show that the vector product obeys to the following identity:
     \begin{equation}\label{vectorproductproperties}
  \left( (\ac\times \b)\times \c\right)+
  \left( (\b\times \c)\times \ac\right)+\left( (\c\times \ac)\times \b\right)=0\,.\quad
    \hbox{(Jacoby identity)}
 \end{equation}
  This identity is related with the fact that heights of the triangle intersect in the one point.

\m
{\bf Exercise$^\dagger$} Show that  $\ac\times (\b\times \c)=\b(\ac,\c)-\c(\ac,\b)$.

}
\m






\subsubsection {Area of parallelogram in $\E^2$ and determinant of $2\times 2$ matrices}.

  Let $\ac,\b$ be two vectors in $2$-dimensional vector space $\E^2$.

  One can consider $\E^2$ as a plane in $3$-dimensional Euclidean space $\E^3$.
   Let $\n$ be a unit vector in $\E^3$ which is orthogonal to $\E^2$.
   One can see $\ac\times \b$ is proportional to the normal vector $\n$ to the plane $\E^2$:
             \begin{equation}\label{defofvectorproduct3}
                 \ac\times \b=A(\ac,\b)\n
                      \end{equation}
  and area of parallelogram equals to the modulus of the coefficient $A(\c,\b)$:
           \begin{equation}\label{defofvectorproductmodularea}
                 S(\ac,\b)=|\ac\times \b|=|A(\ac,\b)|
                      \end{equation}
{\small The normal unit vector $\n$ and coefficient $A(\ac,\b)$ are defined up to a sign: $\n\to-\n$, $A\to-A$.
On the other hand the vector product $\ac\times \b$ is defined up to a sign too: vector
product depends on orientation.
The answer for
$\ac\times \b$ is not changed if we perform calculations for vector product in
an arbitrary basis $\{\e'_x,\e'_y,\e'_z\}$ which have the same orientation as the
the basis $\{\e,\f,\n\}$ and  $\ac\times \b\mapsto \to -\ac\times \b$.
If we consider an arbitrary basis $\{\e'_x,\e'_y,\e'_z\}$ which have the orientation opposite to the orientation
of the basis $\{\e,\f,\n\}$ (e.g. the basis $\{\e,\f,-\n\}$) then $A(\ac,\b)\to -A(\ac,\b)$.
 The magnitude $A(\ac,\b)$ is so called algebraic area of parallelogram. It can positive and negative.
}

   If $(a_1,a_2)$, $(b_1,b_2)$ are coordinates of the vectors $\ac,\b$ in the basis $\{\e,\f\}$:
   $\ac=a_1\e+a_2\f$, $\b=b_1\e+b_2\f$ and according to \eqref{defofvectorproduct}
                  \begin{equation}\label{defofvectorproduct5}
            \ac\times \b=\det \begin{pmatrix}
                \e  &\f  &\n \cr
                  a_1   & a_2  &0 \cr
                  b_1   & b_2  &0\cr
                  \end{pmatrix}=\n\det\begin{pmatrix}
                  a_x   & a_y \cr
                  b_x   & b_y \cr
                  \end{pmatrix}
                  \end{equation}

Hence\begin{equation}\label{areaoftwodimeparal1}
    A(\ac,\b)=\det \det\begin{pmatrix}
                  a_x   & a_y \cr
                  b_x   & b_y \cr
                  \end{pmatrix}\,\, {\rm and}
\,\,
S(\ac,\b)=
                    \left|
              \det\begin{pmatrix}
                  a_x   & a_y \cr
                  b_x   & b_y \cr
                  \end{pmatrix}
                  \right|\,.
\end{equation}
We come to very beautiful formulae for relation between determinant of $2\times 2$ matrix, area of parallelogram and vector product.

\m


{\footnotesize One can deduce this relation in other way:

  Let $\E^2$ be a $2$-dimensional Euclidean space. The function $A(\ac,\b)$
  defined by the relation  \eqref{areaoftwodimeparal1} obeys the following conditions:

 \begin{itemize}

\item
   It is anticommutative:
 \begin{equation}\label{condit1}
   A(\ac,\b)=-A(\ac,\b)
\end{equation}

\item It is bilinear
  \begin{equation}\label{condit2}
A(\lambda\ac+\mu \b,\c)=\lambda A(\ac,\c)+\mu A(\b,\c); \,\,
A(\c,\lambda\ac+\mu \b)=\lambda A(\c,\ac)+\mu A(\c,\b)\,.
\end{equation}
\item and it obeys normalisation condition:
    \begin{equation}\label{condit3}
   A(\e,\f)=\pm 1
\end{equation}

for an arbitrary orthonormal basis.



(Compare with conditions \eqref{vectorproductaxiom1}---\eqref{vectorproductaxiom5}.)

  \end{itemize}
One can see that these conditions define uniquely
$A(\ac,\b)$ and these are the conditions which define the determinant of the $2\times 2$ matrix.
}

\subsubsection {Volume of parallelepiped $^*$}



The vector product of two vectors is related with area of parallelogram.
What about a volume of parallelepiped formed by three vectors $\{\ac,\b,\c\}$?

Consider  parallelepiped formed by vectors  $\{\ac,\b,\c\}$. The parallelogram
formed by vectors $\b,\c$ is considered as a base of this parallelepiped.

Let $\theta$ be an angle between height and vector $\ac$.
It is just the angle between the vector $\b\times\c$ and the vector $\ac$.
Then the
volume is equal to the length  of the height multiplied on the area of the parallelogram,
$V=Sh=S|\ac|\cos\theta$, i.e.
volume  is equal to scalar product of the vectors $\ac$ on the vector product of vectors $\b$ and $\c$:
\begin{equation*}\label{volumeofparallelepiped}
    V(\{\ac,\b,\c\})=\left(\ac,\b\times \c\right)=
    \left(a_x\e_x+a_y\e_y+a_z\e_z, \det
                \begin{pmatrix}
                \e_x  &\e_y  &\e_z \\
                  b_x   & b_y  &b_z \\
                  c_x   & c_y  &c_z
                  \end{pmatrix}
                     \right)
\end{equation*}
             $$
= \left(a_x\e_x+a_y\e_y+a_z\e_z,  (b_yc_z-b_zc_y)\e_x+(b_zc_x-b_xc_z)\e_y+(b_xc_y-b_yc_x)\e_z\right)=
             $$
             $$
     a_x(b_yc_z-b_zc_y)+a_y(b_zc_x-b_xc_z)+a_z(b_xc_y-b_yc_x)=
      \begin{pmatrix}
                a_x  &a_y  &a_z \\
                  b_x   & b_y  &b_z \\
                  c_x   & c_y  &c_z
                  \end{pmatrix}
             $$
We come to beautiful and  useful formula:
 \begin{equation}\label{mixedproduct}
\left(\ac,\left[\b\times \c\right]\right)=\begin{pmatrix}
                a_x  &a_y  &a_z \\
                  b_x   & b_y  &b_z \\
                  c_x   & c_y  &c_z
                  \end{pmatrix}\,.
\end{equation}

{\footnotesize {\bf Remark} The volume of the parallelepiped if considered as a positive number
 equals to the modulus of the number  $\left(\ac,\left[\b\times \c\right]\right)$.
 On the other hand often it is very useful to consider the volume as a real number
  (it could be positive and negative).

{\bf Exercise}
Consider the function  $F(\ac,\b,\c)=\left(\ac,\b\times \c\right)$.

\noindent 1.   Show that  $F(\ac,\b,\c)=0$ if and only if vectors $\ac,\b,\c$ are linear dependent.

2. Show that for an arbitrary vector $\ac$, $F(\ac,\ac,\c)=0$.

3. Show that  for arbitrary vectors $\ac,\b$, $F(\ac,\b,\c)=-F(\ac,\b,\c)$.
Can you  deduce 3) from the 2)?
}

%\end{document} %28  February

\section {Differential forms }

\subsection {Tangent vectors, curves, velocity vectors on the curve}

Tangent vector is a vector $\v$ applied at the given point $\pt\in \E^n$.

 The set of all tangent vectors at the given point $\pt$ is a vector space.
 It is called tangent space of $\E^3$ at the point $\pt$ and it is denoted
 $T_\pt(\E^n)$.

  One can consider {\it vector field} on $\E^n$, i.e.a function which assigns
  to every point $\pt$ vector $\v(\pt)\in T_\pt(\E^n)$.


It is instructive to study the conception of tangent vectors and vector fields
on the curves and surfaces embedded in $\E^n$. We begin with curves.


  A curve in $\E^n$ with parameter $t\in (a,b)$ is a continuous map
\begin{equation}\label{defofcurve}
  C\colon\qquad (a,b)\rightarrow \E^n\:\qquad {\bf r}(t)=(x^1(t),\dots,x^n(t)),\quad
    a<t<b
\end{equation}

   For example consider in $\E^2$ the curve
\begin{equation*}\label{exofcurve}
  C\colon\qquad (0,2\pi)\rightarrow \E^2\:\qquad {\bf r}(t)=(R\cos t,R\sin t),\,\,
  0\leq t<2\pi\,.
\end{equation*}

The image of this curve is the circle  of the radius $R$. It can be defined by the
equation:
\begin{equation*}\label{defofcurvebyequation}
  x^2+y^2=R^2\,.
\end{equation*}

To distinguish between curve and its image we say that curve $C$ in
\eqref{defofcurve} is {\it parameterised}
 curve  or {\it path}.
  We will call  the image of the curve {\it unparameterised curve}
   (see for details the next subsection).
  It is very useful to think about parameter $t$ as a "time" and consider
  parameterised curve like {\it point moving along a curve}.
  Unparameterised curve is the trajectory of the moving point.
The using of word "curve" without adjective "parameterised" or "nonparameterised"
   sometimes is ambiguous.

\m


\medskip



\m

{\it Vectors tangent to curve---velocity vector}

\m

 Let $\r(t)\quad{\bf r}={\bf r}(t)$ be a curve in $\E^n$.

 {\it Velocity $\v(t)$} it is the vector
\begin{equation*}\label{tangentvector}
 {\bf v}(t)={d\r \over dt}=\left(\dot x^1(t),\dots,\dots \dot x^n(t)\right)=
 \left(v^1(t),\dots,v^n(t)\right)
\end{equation*}
  in $\E^n$.  Velocity vector is {\it tangent vector
 to the curve}.



  Let $C\colon \r=\r(t)$ be  a curve
 and $\r_0=\r(t_0)$ any given point on it.  Then
   the set of all vectors tangent to the curve at the  point $\r_0=\r(t_0)$ is one-dimensional  vector space
 $T_{\r_0}C$. It is linear subspace in vector space $T_{\r_0}C$.
 The points of the tangent space  $T_{\r_0}C$ are the points of tangent line.

In the next section we will return to curves and consider them in more details.

{\bf Remark}
  We consider by default only {\it smooth, regular} and {\it simple} curves.
  Curve ${\bf r}(t)$ $=(x^1(t),\dots,x^n(t))$ is called smooth
  if all functions $x^i(t)$, ($i=1,2,\dots,n$) are smooth functions
  (Function is called smooth if it has derivatives of arbitrary order.)
   Curve $\r(t)$ is called regular if velocity vector $\v(t)={d\r(t)\over dt}$ is not equal to zero at all $t$.
  Simple curves are curves which have no intersection points.



 \subsection {Reparameterisation}

  One can move along trajectory  with different velocities, i.e.
  one can consider different parameterisation. E.g. consider

\begin{equation*}\label{eqcurves}
    C_1\colon\qquad
  \begin{cases}
  x(t)=t\\
  y(t)=t^2\\
  \end{cases}
  0<t<1\,,
 \qquad\
  C_2\colon\qquad
 \begin{cases}
  x(t)=\sin t\\
  y(t)=\sin^2 t\\
  \end{cases}
    0<t<{\pi\over 2}
\end{equation*}

Images of these two  parameterised curves are the same. In both cases point moves
along a piece of the same parabola but with different velocities.

\medskip

{\bf Definition}

  Two smooth curves
  $C_1\colon\quad \r_1(t)\colon (a_1,b_1)\rightarrow \E^n$ and
  $C_2\colon\quad \r_2(\tau)\colon (a_2,b_2)\rightarrow \E^n$
      are called equivalent if there exists
      reparameterisation map:
                            $$
             t(\tau)\colon (a_2,b_2)\rightarrow (a_1,b_1),
                        $$
                     such that
\begin{equation}\label{repardef}
     r_2(\tau)=r_1(t(\tau))
\end{equation}
Reparameterisation $t(\tau)$ is diffeomorphism, i.e.
function $t(\tau)$ has derivatives of all orders
and first derivative $t'(\tau)$ is not equal to zero.

E.g. curves in \eqref{eqcurves} are equivalent because a map
$\varphi (t)=\sin t$ transforms first curve to the second.

\medskip

 {\it Equivalence class of equivalent parameterised curves is called non-parameterised curve}.

\smallskip

It is useful sometimes to distinguish curves in the same equivalence class which differ by orientation.

{\bf Definition}  Let curves $C_1,C_2$ be two equivalent curves.
We say that they have same orientation (parameterisations $\r_1(t$  and $\r_(\tau)$
have the same orientation) if reparameterisation
$t=t(\tau)$ has positive derivative, $t'(\tau)>0$.
We say that they have opposite orientation (parameterisations $\r_1(t$  and $\r_(\tau)$
have the opposite orientation) if reparameterisation
$t=t(\tau)$ has negative  derivative, $t'(\tau)<0$.


 Changing orientation means changing the direction of ''walking'' around the curve.


  Equivalence class of equivalent curves splits on two subclasses with respect to orientation.

\smallskip

{\bf Non-formally:} Two curves are equivalent curves
(belong to the same equivalence class)
if these parameterised curves ( paths) have the
 same images. Two equivalent curves have the same image. They
define the same set of points
in $\E^n$. Different parameters correspond to moving along curve
with different velocity.  Two equivalent curves have opposite orientation
If  two parameterisations  correspond to moving along the curve in different directions
then these parameterisations define opposite orientation.



What happens with velocity vector if we change parameterisation?
It  changes its value,  but it can change its direction only on opposite  (If these parameterisations have opposite
 orientation of the curve):
\begin{equation}\label{changingofvelocity}
       {\v}(\tau)=
       {d\r_2(\tau)\over d\tau}=
         {d\r(t(\tau))\over d\tau}=
           {dt(\tau)\over d\tau}
                     \cdot
            {d\r(t)\over dt}\big\vert_{t=t(\tau)}
\end{equation}
Or shortly:  ${\v}(\tau)\big\vert_{\tau}=t_\tau(\tau)\v(t)\big\vert_{t=t(\tau)}$

We see that velocity vector is multiplied on the coefficient (depending on the point of the curve),
i.e. velocity vectors for different parameterisations are collinear vectors.

\noindent (We call two vectors $\ac,\b$ collinear, if they are proportional each other, i,e,
if $\ac=\lambda\b$.)
\bigskip



  {\bf Example}  Consider following  curves in $\E^2$:

\begin{equation*}\label{threedifparametrisations}
  C_1\colon\quad\begin{cases}
  x=\cos\theta\\
  y=\sin\theta\\
  \end{cases},
0<\theta<{\pi},\qquad
     C_2\colon\quad
\begin{cases}
  x=u\\
  y=\sqrt {1-u^2}\\
  \end{cases},
  -1<u<1,\quad
\end{equation*}
\begin{equation}
\begin{cases}
  x=\tan t\\
  y={\sqrt {\cos 2t}\over \cos t}\\
  \end{cases},
  -{\pi\over 4}<t< {\pi\over 4}
  \end{equation}
 These three parameterised curves,(paths) define the same non-parameterised
 curve: the upper piece of the circle: $x^2+y^2=1, y>0$.
The reparameterisation $u(\theta)=\cos \theta$ transforms
the second curve to the first one.

  The reparameterisation $u(\theta)=\cos \theta$ transforms
the second curve to the first one.

The reparameterisation $u(\theta)=\tan t$ transforms
the second curve to the third one one:
${\sqrt {\cos 2t}\over \cos t}=
{\sqrt {\cos^2t-\sin^2 t}\over \cos t}=\sqrt{1-\tan^2 t}$.

Curves $C_1, C_2$ have opposite orientation because $u'(\theta)<0$.
Curves $C_2,C_3$ have the same orientation, because $u'(t)>0$.
Curves $C_1$ and $C_2$ have opposite orientations too (Why?).


In the first case point moves with constant pace $|\v(\theta)|=1$
anti clock-wise "from right to left" from the  point $A=(1,0)$ to the point $B=(-1,0)$.
In the second case pace is not constant, but $v_x=1$ is constant.
Point moves clock-wise "from left to right", from the  point $B=(-1,0)$ to the point $A=(1,0)$.
In the third case  point also moves clock-wise "from the left to right".

There are other examples in the Homeworks.

%\end{document}

\subsection {$0$-forms and $1$-forms}

{\footnotesize{\it Most of considerations of this and next subsections  can be considered
only for $\E^2$ or $\E^3$.
All examples  for differential forms
is only for $\E^2$, $\E^3$}.}

\bigskip


$0$-form on $\E^n$ it is just function on $\E^n$ (all functions under consideration are differentiable)


\bigskip

 Now we define $1$-forms.

{\bf Definition} Differential $1$-form $\w$ on $\E^n$ is a function  on tangent vectors
of $\E^n$, such that it is linear at each point:
\begin{equation}\label{linearityof1forms}
    \w(\r, \lambda \v_1+\mu\v_2)=\lambda \w(\r,\v_1)+\mu \w(\r,\v_2)\,.
\end{equation}
Here $\v_1,\v_2$ are vectors tangent to $\E^n$ at the point $\r$, ($\v_1,\v_2\in T_x\E^n$)
(We recall that vector tangent at the point $\r$ means vector attached at the point $\r$).
We suppose that $\w$ is smooth function on points $\r$.

If $\X(\r)$ is vector field and $\w$-1-form then evaluating $\w$ on $\X(\r)$ we come to the function
$w(\r,\X(\r))$  on $\E^3$.



 Let ${\e_1,\dots,\e_n}$ be a basis in $\E^n$ and $(x^1,\dots,x^n)$ corresponding coordinates:
 an arbitrary point with coordinates $(x^1,\dots,x^n)$ is assigned to the vector
 $\r=x^1\e_1+x^2\e_2+\dots x^n\e_n$ starting at the origin.

Translating basis vectors  $\e_i$ ($i=1,\dots,n$)
from the origin to other points of $\E^n$ we come to vector field which we also denote $\e_i$ ($i=1,\dots,n$).
 The value of vector field $\e_i$ at the point $(x^1,\dots,x^n)$ is the vector
 $\e_i$ attached at this point (tangent to this point).




   Let $\w$ be an $1$-form on $\E^n$.
   Consider an arbitrary  vector field $\A(\r)=\A(x^1,\dots,x^n)$:
 \begin{equation*}\label{expansionofvectorfield}
    \A(\r)= A^1(\r)\e_1+\dots+A^n(\r)\e_n=\sum_{i=1}^nA^i(\r)\e_i
      \end{equation*}
 Then by linearity
           $$
    \w(\r,\A(\r))=\w(\r, A^1(\r)\e_1+\dots+A^n(\r)\e_n)=A^1\w(\r,\e_1)+\dots+A^n\w(\r,\e_n)
           $$
Consider {\it basic} differential forms $dx^1,dx^2,\dots,dx^n$ such that
                 \begin{equation}\label{basicforms}
    dx^i(\e_j)=\delta^i_j=\begin{cases}
     1\,\,{\rm if}\,\, i=j\cr 0\,\,{\rm if}\,\, i\not =j\cr
      \end{cases}\,.
\end{equation}
Then it is easy to see that
                 $$
  dx^1 (\A)=\A^1,dx^2 (\A)=\A^2,....,{\rm i.e.} dx^i(\A)=A^i
                 $$
Hence
              $$
   \w(\r,\A(\r))=\left(\w_1(\r)dx^1+\w_2(\r)dx^2+\dots+\w_n(\r)dx^n\right)(\A(\r))\,
              $$
where components $\w_i(\r)=\w(\r,\e_i)$.

  In the same way as an arbitrary  vector field on $\E^n$ can be expanded over the basis $\{\e_i\}$
  (see \eqref{expansionofvectorfield}),
  an arbitrary differential $1$-form $\w$ can be expanded over the basis forms\eqref{basicforms}
   $$
   \w=\w_1(x^1,\dots,x^n)dx^1+\w_2(x^1,\dots,x^n)dx^2+\dots+\w_n(x^1,\dots,x^n)dx^n\,.
     $$

\m
 {\bf Example} Consider in $\E^3$ a basis $\e_x,\e_y,\e_z$
 and corresponding coordinates $(x,y,z)$.
  Then
\begin{equation}\label{cartesianbasisinthree}
      \begin{matrix}
    dx(\e_x)=1, dx(\e_y)=0, dx(\e_z)=0 \cr
    dy(\e_x)=0, dy(\e_y)=1, dy(\e_z)=0 \cr
    dz(\e_x)=0, dz(\e_y)=0, dz(\e_z)=1 \cr
    \end{matrix}
\end{equation}
The value of a differential $1$-form
$\w=a(x,y,z)dx+b(x,y,z)dy+c(x,y,z)dz$  on vector field
$\X=A(x,y,z)\e_x+B(x,y,z)\e_y+C(x,y,z)\e_z$ is equal to
               $$
   \w(\r, \X)=a(x,y,z)dx(\X)+b(x,y,z)dx(\X)+c(x,y,z)dx(\X)=
           $$
           $$
  a(x,y,z)A(x,y,z)+b(x,y,z)B(x,y,z)+c(x,y,z)C(x,y,z)
               $$

\bigskip

   It is very useful (see below ) introduce for basic vectors new notations:
                \begin{equation}\label{newnotation}
                \e_i\mapsto {\p \over \p x^i}\,
                \hbox {\,\,\,for basic vectors $\e_x,\e_y,\e_z$ in $\E^3$}\,
                \e_x\mapsto {\p \over\p x}\,\,\,\e_y\mapsto {\p \over\p y}\,\,\,
                \e_z\mapsto {\p \over\p z}\,.
                 \end{equation}
In these new notations the formula \eqref{basicforms} looks like
          \begin{equation*}\label{basicforms}
    dx^i\left({\p\over \p x^j}\right)=\delta^i_j=\begin{cases}
     1\,\,{\rm if}\,\, i=j\cr 0\,\,{\rm if}\,\, i\not =j\cr
      \end{cases}\,.
\end{equation*}
and the formula \eqref{cartesianbasisinthree} looks like
      \begin{equation*}\label{cartesianbasisinthreenewnotations}
      \begin{matrix}
    dx\left({\p\over \p x}\right)=1, dx\left({\p\over \p y}\right)=0, dx\left({\p\over \p z}\right)=0 \cr
    dy\left({\p\over \p x}\right)=0, dy\left({\p\over \p y}\right)=1, dy\left({\p\over \p z}\right)=0 \cr
    dz\left({\p\over \p x}\right)=0, dz\left({\p\over \p y}\right)=0, dz\left({\p\over \p z}\right)=1 \cr
    \end{matrix}
\end{equation*}
%\end{document}

 It is very useful to introduce new notation for vectors $\e_x,\e_y,\e_z$.

 In the next subsection we will consider the directional derivative of function along vector fields.
 The directional derivative will justify our new notations \eqref{newnotation}.





 \subsubsection {Vectors---directional derivatives of functions}






Let $\R$ be a vector in $\E^n$ tangent to the point $\r=\r_0$ (attached at a point $\r=\r_0$).
Define the operation of derivative of an arbitrary
(differentiable) function at the point $\r_0$ along the vector $\R$---
directional derivative of function $f$ along the vector $\R$

\m

{\bf Definition}

Let $\r(t)$ be a curve such that
           \begin{itemize}
   \item   $\r(t)\big\vert_{t=0}=\r_0$

     \item Velocity vector of the curve at the point $\r_0$ is equal to $\R$:
               $
       {d\r(t)\over dt}\big\vert_{t=0}=\R
              $
           \end{itemize}
  Then directional derivative of function $f$ with respect to the vector $\R$ at
  the point $\r_0$ $\p_\R f\big\vert_{\r_0}$
  is defined by the relation
  \begin{equation}\label{dirderivative}
\p_\R f\big\vert_{\r_0}={d\over dt}\left(f\left(\r(t)\right)\right)\big\vert_{t=0}\,.
\end{equation}

\m

 Using chain rule one come from this definition to the following important formula for the directional derivative:
\begin{equation}\label{basicformulaforthisnotation}
    {\rm If}\,\, \R=\sum_{i=1}^n R^i\e_i\,\,{\rm  then}\,\,
\p_\R f\big\vert_{\r_0}=\sum_{i=1}^n R^i{\p\over \p x^i}f(x^1,\dots,x^n)\big\vert_{\r=\r_0}
\end{equation}

It follows form this formula that

{\it One can assign to every vector $\R=\sum_{i=1}^n R^i\e_i$ the operation
$\p_\R=R^1{\p\over \p x^1}+R^2{\p\over \p x^2}+\dots+R^n{\p\over \p x^n}$ of taking directional derivative:}
\begin{equation}\label{assigning to vector derivative}
    \R=\sum_{i=1}^n R^i\e_i \mapsto \p_\R=\sum_{i=1}^n R^i{\p\over \p x^i}
\end{equation}
Thus we come to notations \eqref{newnotation}. The symbols  $\p_x,\p_y, \p_z$ correspond to partial derivative with respect to coordinate
  $x$ or $y$ or $z\,\,$. Later we see that these new notations are very illuminating when we deal
   with arbitrary coordinates, such as polar coordinates or spherical  coordinates,
    The conception of orthonormal basis
 is ill-defined in arbitrary coordinates, but one can still consider the corresponding partial derivatives.
Vector fields $\e_x,\e_y,\e_z$ (or in new notation $\p_x,\p_y,\p_z$) can be considered as a
basis\footnote {
Coefficients of expansion are functions, elements of algebra of functions, not numbers ,elements of field.
To be more careful, these vector fields are basis of the {\it module} of vector
fields on $\E^3$} in the space
of all vector fields on $\E^3\,\,$.

 An arbitrary vector field  \eqref{expansionofvectorfield} can be rewritten in the following way:
 \begin{equation}\label{expansionofvectorfield2}
    \A(\r)= A^1(\r)\e_1+\dots+A^n(\r)\e_n=A^1(\r){\p\over \p x^1}+A^2(\r){\p\over \p x^2}+
    \dots+A^n(\r){\p\over \p x^n}
      \end{equation}


\m

 {\it Differential on $0$-forms}

\m

  Now we introduce very important operation: Differential $d$ which acts on $0$-forms and transforms them
  to $1$ forms.
                   $$
     \begin{tabular}{|p{ 2cm}|}
           \hline
            Differential 0-forms\cr
        \hline
      \end{tabular}
      \,\,\,
     {\buildrel d\over \longrightarrow}\,\,
      \begin{tabular}{|p{ 2cm}|}
           \hline
            Differential 1-forms\cr
        \hline
      \end{tabular}
                   $$
      Later we will learn how differential acts on $1$-forms transforming them to $2$-forms.

\m

  {\bf Definition} Let $f=f(x)$-be $0$-form, i.e. function on $\E^n$.
   Then

   \begin{equation}\label{defofdif01}
    df=\sum_{i=1}^n{\p f(x^1,\dots,x^n)\over\p x^i}dx^i\,.
\end{equation}
   The value of $1$-form $df$
  on an arbitrary vector field \eqref{expansionofvectorfield2} is equal to
           \begin{equation}\label{defofdif02}
            df(\A)=\sum_{i=1}^n{\p f(x^1,\dots,x^n)\over x^i}dx^i(\A)=\sum_{i=1}^n{\p f(x^1,\dots,x^n)\over x^i}A^i=
            \p_{\A}f
           \end{equation}
  We see that {\it value of differential of $0$-form $f$ on an arbitrary vector field $\A$
  is equal to directional derivative of function $f$ with respect to the vector $\A$}.

\m

  {\footnotesize  The formula \eqref{defofdif02} defines $df$ in invariant way without using coordinate
  expansions. Later we  check straightforwardly the coordinate-invariance of the  definition \eqref{defofdif01}. }



{\bf Exercise} Check that
                             \begin{equation}\label{fordifferential1}
     dx^i(\A)=\p_\A x^i
\end{equation}

\m

{\bf Example} If $f=f(x,y)$ is a function ($0-form$) on $\E^2$ then
            $$
            df={\p f(x,y)\over \p x}dx+{\p f(x,y)\over\p y}dy
            $$
and for an arbitrary vector field
 $\A=\A=A_x\e_x+\A_y\e_y=A_x(x,y)\p_x+A_y(x,y)\p_y$
                    $$
            df(\A)={\p f(x,y)\over  \p x}dx(\A)+
            A_y(x,y){\p f(x,y)\over  \p y}dy(\A)=
                     $$
                     $$
            \A_x(x,y){\p f(x,y)\over  \p x}+
            A_y(x,y){\p f(x,y)\over  \p y}=\p_\A f\,.
                  $$


\m

{\bf Example}  Find the value of $1$-form $\w=df$ on the vector field $\A=x\p_x+y\p_y$
if $f=\sin (x^2+y^2)$.

$\w(\A)=df(\A)$.
 One can calculate it using formula \eqref{defofdif01} or using formula  \eqref{defofdif02}.

{\it Solution (using \eqref{defofdif01})}:
               $$
               \w=df={\p f\over\p x}dx+{\p f\over\p y}dy=2x\cos (x^2+y^2)dx+2y\cos (x^2+y^2)dy\,.
                $$
                   $$\w(\A)=2x\cos (x^2+y^2)dx(\A)+2y\cos (x^2+y^2)dy(\A)=
                    $$
                    $$
                   2x\cos (x^2+y^2)A_x+2y\cos (x^2+y^2)dA_y=2(x^2+y^2)\cos(x^2+y^2)\,,
                   $$

{\it Another solution (using \eqref{defofdif02})}
                 $$
df(\A)=\p_\A f=A_x{\p f\over\p x}+A_x{\p f\over\p y}=2(x^2+y^2)\cos(x^2+y^2)\,.
                 $$

 See other examples in Homeworks.

%\end{document} % 7 March


\subsection  {Differential $1$-form in arbitrary coordinates }





 Why differential forms? Why so strange notations for vector fields.

If we use the technique of  differential forms we in fact do not
care about what coordinates we work in:
calculations are the same in arbitrary coordinates.

\m

{\footnotesize
\subsubsection {Calculations in arbitrary coordinates $^*$}


Consider  an arbitrary (local) coordinates $u^1,\dots, u^n$ on $\E^n$:
$u^i=u^i(x^1,\dots,x^n)$, $i=1,\dots,n$. Show first that
       \begin{equation}\label{firstdifferential2}
    du^i=\sum_{k=1}^n{\p u^i(x^1,\dots,x^n)\over\p x^k}dx^k\,.
\end{equation}
  It is enough to check it on basic fields:
             $$
              du^i\left({\p\over \p x^m}\right)=
  \p_{\left({\p\over \p x^m}\right)}u^i=
  {\p u^i(x^1,\dots,x^n)\over x^m}=
   \sum_{k=1}^n{\p u^i(x^1,\dots,x^n)\over\p x^k}dx^k\left(\left({\p\over \p x^m}\right)\right)=
   \left({\p\over \p x^m}\right)\,.
             $$
because (see \eqref{basicforms}):
                 \begin{equation}\label{basicforms1}
    dx^i\left({\p\over \p x^j}\right)=
    \delta^i_j=\begin{cases}
     1\,\,{\rm if}\,\, i=j\cr 0\,\,{\rm if}\,\, i\not =j\cr
      \end{cases}\,.
\end{equation}
(We rewrite the formula \eqref{basicforms} using new notations $\p_i$ instead $\e_i$).
In the previous formula \eqref{basicforms}
we considered {\it cartesian} coordinates.

Show that the formula above is valid in an {\it arbitrary coordinates}.

  One can see using chain rule that
       \begin{equation}\label{changingofderivative}
    {\p\over \p u^i}={\p x^1\over \p u^i}{\p\over\p  x^1}+
    {\p x^2\over \p u^i}{\p\over\p  x^2}+\dots+{\p x^n\over \p u^i}{\p\over \p  x^n}=
    \sum_{k=1}^n{\p x^k\over \p u^i}{\p\over\p  x^k}
\end{equation}
Calculate  the value of differential form $du^i$ on vector field
$\p\over \p u^j$ using \eqref{firstdifferential2} and \eqref{changingofderivative}:
                          \begin{equation}\label{calculationofinvariance}
du^i\left({\p\over \p u^j}\right)=
\sum_{k=1}^n{\p u^i(x^1,\dots,x^n)\over\p x^k}dx^k
\left(\sum_{r=1}^n{\p x^r\over \p u^j}{\p\over\p  x^r}\right)=
  \end{equation}
         $$
\sum_{k,r=1}^n{\p u^i(x^1,\dots,x^n)\over\p x^k}{\p x^r(u^1,\dots,u^n)\over \p u^j}dx^k
\left({\p\over\p  x^r}\right)=
         $$
          $$
   \sum_{k,r=1}^n{\p u^i(x^1,\dots,x^n)\over\p x^k}{\p x^r(u^1,\dots,u^n)\over \p u^j}
      \delta^k_r=\sum_{k=1}^n {\p x^k\over \p u^j}{\p u^i\over \p x^k}=\delta_i^j
          $$
We come to
 \begin{equation}\label{basicforms2}
    du^i\left({\p\over \p u^j}\right)=\delta^i_j=\begin{cases}
     1\,\,{\rm if}\,\, i=j\cr 0\,\,{\rm if}\,\, i\not =j\cr
      \end{cases}\,.
\end{equation}


We see that formula \eqref{basicforms1} has the same appearance in arbitrary coordinates.
In other words it  is invariant with respect to an arbitrary transformation of
coordinates.


\m
{\bf  Exercise} Check straightforwardly the invariance of the definition \eqref{defofdif01}.
In coordinates $(u^1,\dots,u^n)$

{\it Solution}  We have to show that the formula \eqref{defofdif01} does not changed
 under changing of coordinates $u^i=u^i(x^1,\dots,x^n)$.
                 $$
    df=\sum_{i=1}^n{\p f(x^1,\dots,x^n)\over\p x^i}dx^i=
    \sum_{i=1,k}^n{\p f(x^1,\dots,x^n)\over\p x^i}{\p x^i\over \p u^k}du^k=
            =\sum_{i=1}^n{\p f\over\p u^k}du^k\,,
                $$
because  $ \sum_{i=1}^n{\p f(x^1,\dots,x^n)\over\p x^i}{\p x^i\over \p u^k}={\p f\over \p u^k}$


\m

\m

{\bf Example}

Consider  more in detail   $\E^2$.
(For $\E^3$ considerations are the same, just calculations little bit more complicated)
   Let $u,v$ be an arbitrary coordinates in $\E^2$, $u=u(x,y),v=v(x,y)$.
\begin{equation}\label{relationbetweencoordinateformsfor2}
    du={\p u(x,y)\over \p x}dx+{\p u(x,y)\over \p y}dy,\qquad
    dv={\p v(x,y)\over \p x}dx+{\p v(x,y)\over \p y}dy
\end{equation}
and
              \begin{equation}\label{vectorfieldfor2}
\p_u={\p x(u,v)\over\p u}\p_x+{\p y(u,v)\over\p u}\p_y,\,\,\,
\p_v={\p x(u,v)\over\p v}\p_x+{\p y(u,v)\over\p v}\p_y
\end{equation}
(As always sometimes we use notation $\p_u$ instead $\p\over \p u$, $\p_x$ instead $\p\over \p x$ e.t.c.)
Then
\begin{equation}\label{defofbasicformsingeneralcoordinatesfor2}
\begin{array} {cc}
  du(\p_u)=1,  du(\p_v)=0\\
    \phantom{}\\
     dv(\p_w)=0,  dv(\p_v)=1\\
     \end{array}
\end{equation}
This follows from the general formula but it is good exercise to repeat the previous calculations for this case:
                 $$
  du(\p_u)=\left({\p u(x,y)\over \p x}dx+{\p u(x,y)\over \p y}dy\right)
  \left({\p x(u,v)\over \p u}\p_x+{\p y(u,v)\over \p u}\p_y\right)=
                 $$
                 $$
  {\p u(x,y)\over \p x}{\p x(u,v)\over \p u}+{\p u(x,y)\over \p y}{\p y(u,v)\over \p u}
   ={\p x(u,v)\over \p u}{\p u(x,y)\over \p x}+{\p y(u,v)\over \p u}{\p u(x,y)\over \p y}=1
                    $$
We just apply chain rule to the function $u=u(x,y)=u(x(u,v),y(u,v))$:

Analogously
              $$
  du(\p_v)=\left({\p u(x,y)\over \p x}dx+{\p u(x,y)\over \p y}dy\right)
  \left({\p x(u,v)\over \p v}\p_x+{\p y(u,v)\over \p v}\p_y\right)
                 $$
                 $$
  {\p u(x,y)\over \p x}{\p x(u,v)\over \p v}+{\p u(x,y)\over \p y}{\p y(u,v)\over \p v}
   ={\p x(u,v)\over \p v}{\p u(x,y)\over \p x}+{\p y(u,v)\over \p v}{\p u(x,y)\over \p y}=0
                    $$
The same calculations for $dv$.
}

\subsubsection {Calculations in polar coordinates}

{\bf Example} (Polar coordinates)  Consider polar coordinates in $\E^2$:
%In this example we apply formulae  \eqref{relationbetweencoordinateformsfor2},
%\eqref{vectorfieldfor2} for polar coordinates on $\E^2$. Recall that for polar coordinates $(r,\varphi)$
   \begin{equation*}
               \begin{cases}
                x(r,\varphi)=r\cos\varphi\cr
                y(r,\varphi)=r\sin\varphi\cr
                \end{cases}
                \quad
               ( 0\leq \varphi<2\pi, 0\leq r<\infty),\,\,
              \end{equation*}
  Respectively
      \begin{equation}\label{polarcoordinates1}
               \begin{cases}
                r(x,y)=\sqrt {x^2+y^2}\cr
                \varphi=\arctan {y\over x}\cr
                \end{cases}\,.
              \end{equation}
We have that for basic $1$-forms
          \begin{equation}\label{inpolar1}
    dr=r_xdx+r_ydy={x\over \sqrt{x^2+y^2}}dx+{y\over \sqrt{x^2+y^2}}dy={xdx+ydy\over r}
          \end{equation}
and
        \begin{equation}\label{inpolar2}
    d\varphi=\varphi_xdx+\varphi_ydy=
    {-ydx\over x^2+y^2}+ {xdy\over x^2+y^2}dx={xdy-ydx\over r^2}
          \end{equation}
Respectively
   \begin{equation*}
    dx=x_rdr+x_\varphi d\varphi=\cos\varphi dr-r\sin\varphi d\varphi
          \end{equation*}
and
        \begin{equation}\label{basicformsforpolar}
    dy=y_rdr+y_\varphi d\varphi=\sin\varphi dr+r\cos\varphi d\varphi
          \end{equation}
For basic vector fields
               \begin{equation*}
    {\p_r}={\p x\over \p r}\p_x+{\p y\over \p r}\p_y=\cos\varphi \p_x+\sin\varphi \p_y=
           {x\p_x+y\p_y\over r},
                      \end{equation*}
\begin{equation}\label{vectorinpolar1}
    {\p_\varphi}={\p x\over \p \varphi}\p_x+{\p y\over \p \varphi}\p_y=-r\sin\varphi \p_x+r\cos\varphi \p_y=
           x\p_y-y\p_x,
                      \end{equation}
respectively
                        \begin{equation*}
    {\p_x}={\p r\over \p x}\p_r+{\p \varphi\over \p x}\p_\varphi={x\over r} \p_r-{y\over r^2} \p_\varphi\,\,
                      \end{equation*}
and
                     \begin{equation}\label{vectorinpolar2}
    {\p_y}={\p r\over \p y}\p_r+{\p \varphi\over \p y}\p_\varphi={y\over r} \p_r+{x\over r^2} \p_\varphi\,\,
                      \end{equation}


\m

{\bf Example} Calculate the value of forms $\w=xdx+ydy$ and $\sigma=xdy-ydx$ on
vector fields $\A=x\p_x+y\p_y$, ${\bf B}=x\p_y-y\p_x$. Perform calculations in Cartesian and in polar coordinates.



 In Cartesian coordinates:
                 $$
        \w(\A)=xdx(x\p_x+y\p_y)+ydy(x\p_x+y\p_y)=x^2+y^2, \,\,\w({\bf B})=xdx({\bf B})+ydy({\bf B})=
           0,
                 $$
                  $$
                  \sigma(\A)=xdy(\A)-ydx(\A)=0, \,\,\sigma({\bf B})=xdy({\bf B})-ydx({\bf B})=
           x^2+y^2\,.
                      $$
Now perform calculations in polar coordinates.
According to relation \eqref{inpolar1}
           $$
        \w=xdx+ydy=rdr,\,\, \sigma=xdy-ydx=r^2d\varphi
           $$
and according to relations \eqref{vectorinpolar1} and \eqref{vectorinpolar2}
            $$
         \A=x\p_x+y\p_y=r\p_r,\,\,{\bf B}=x\p_y-y\p_x=\p_\varphi
            $$
Hence   $\w(\A)=rdr(\A)=r^2=x^2+y^2, \w({\bf B}=rdr(\p_\varphi)=0$,
                $$
      \sigma(\A)=r^2d\varphi(r\p_r)=0,\quad \sigma({\bf B})=
                    r^2d\varphi(\p_\varphi)=r^2=x^2+y^2\,.
                $$
                Answers coincide.


{\bf Example}. Let $F=x^4-y^4$ and vector field $\A=r\p_r$. Calculate $\p_\A F$.
 We have to transform form from Cartesian coordinates to polar or vector field from polar to Cartesian.

{\it In Cartesian coordinates}: $\A=r{\p\over \p r}=x{\p\over \p x}+y{\p\over \p y}$. Hence
  $\p_\A F= \left(x{\p\over \p x}+y{\p\over \p y}\right)(x^4-y^4)=4x^4-4y^4$.
  Or using the fact that $\p_\A F=dF$ we have that $\p_\A (x^4-y^4)=d(x^4-y^4)(\A)=$
            $$
        (4x^3dx-4y^3dy)\left(x{\p\over \p x}+y{\p\over \p y}\right)=
          4x^3 dx \left(x{\p\over \p x}+y{\p\over \p y}\right)-4y^3dy\left(x{\p\over \p x}+y{\p\over \p y}\right)=
          4x^4-4y^4\,.
            $$
In polar coordinates $F=x^4-y^4=(x^2-y^2)(x^2+y^2)=r^2(r^2\cos^\varphi-r^2\sin^2\varphi)=r^4\cos 2\varphi$.
$\p_\A F=r{\p\over \p r}(r^4\cos 2\varphi)=4r^4\cos 2\varphi=4(x^4-y^4)$. Or using $1$-forms:
              $$
 \p_\A F=dF(\A)=d(r^4\cos 2\varphi)(\A)=(4r^3\cos 2\varphi-2r^4\sin 2\varphi d\varphi)(r\p_r)=4r^4\cos 2\varphi\,.
              $$
{\footnotesize {\bf Example} Calculate the value of form $\w={xdy-ydx\over x^2+y^2}$ on the vector field $\A=\p_\varphi\,$.
    We have to transform form from Cartesian coordinates to polar or vector field from polar to Cartesian.
            $$
            {xdy-ydx\over x^2+y^2}=d\varphi,\quad  \w(\A)=d\varphi (\p_\varphi)=1
            $$
            or
            $$
      \p_\varphi=x\p_y-y\p_x,\,\,\w(\A)={xdy(x\p_y-y\p_x)-ydx(x\p_y-y\p_x)\over x^2+y^2}=1\,.
            $$
}





\subsection {Integration of differential $1$-forms over curves}

Let $\w=\w_1(x^1,\dots,x^n)dx^1+\dots+\w_1(x^1,\dots,x^n)dx^n=\sum_{i=1}^n \w_idx^i$
be an arbitrary $1$-form in $\E^n$

and $C\colon \r=\r(t), t_1\leq t\leq t_2$ be an arbitrary smooth curve in $\E^n$.

One can consider the value of one form $\w$ on the velocity vector field $\v(t)={d\r(t)\over dt}$ of the curve:
             $$
      \w(\v(t))=\sum_{i=1}^n\w_i\left(x^1(t),\dots,x^n(t))dx^i(\v(t)\right)=
            \sum_{i=1}^n\w_i\left(x^1(t),\dots,x^n(t)\right){d x^i(t)\over dt}
             $$
 We define now integral of $1$-form $\w$ over the curve $C$.


{\bf Definition}  The integral of the form $\w=\w_1(x^1,\dots,x^n)dx^1+\dots+\w_n(x^1,\dots,x^n)dx^n$
 over the curve
$C\colon\quad \r=\r(t)\quad t_1\leq t\leq t_2$ is equal to the integral of the function $\w(\v(t))$
over the interval $t_1\leq t\leq t_2$:
              \begin{equation}\label{definitionofintegral}
    \int_C \w=\int_{t_1}^{t_2}\w(\v(t))dt=
               \int_{t_1}^{t_2}
               \left(
          \sum_{i=1}^n\w_i\left(x^1(t),\dots,x^n(t)\right){d x^i(t)\over dt}
              \right)dt\,.
            \end{equation}


 \m



{\bf Proposition}
{\it The integral $\int_C \w$ does not depend on the choice of coordinates on $\E^n$. It does not depend
(up to a sign) on parameterisation of the curve: if $C\colon\quad \r=\r(t)\quad t_1\leq t\leq t_2$
is a curve and $t=t(\tau)$ is an arbitrary reparameterisation, i.e.
new curve $C'\colon\quad \r'(\tau)=\r(t(\tau))\quad \tau_1\leq \tau\leq \tau_2$, then
  $\int_C \w=\pm\int_C' \w$:
\begin{equation*}\label{changingtheparameterisationforintegral1}
    \int_C \w=\int_{C'} \w,\quad \hbox{if orientaion is not changed, i.e. if $t'(\tau)> 0$}
\end{equation*}
and
\begin{equation*}\label{changingtheparameterisationforintegral2}
    \int_C \w=-\int_{C'} \w,\quad \hbox{if orientaion is  changed, i.e. if $t'(\tau)< 0$}
\end{equation*}
\m

 If reparameterisation changes the orientation then starting point of the curve becomes the ending point and vice versa.}


\m



{\footnotesize {\it Proof of the Proposition}
Show that integral does not depend (up to a sign) on the
parameterisation of the curve. Let $t(\tau)$ ($\tau_1\leq t\leq
\tau_2$) be reparameterisation. We come to the new curve $C'\colon \r'(\tau)=\r(t(\tau))$.
Note that  the new velocity vector  $\v'(\tau)={dr(t(\tau))\over d\tau}=t'(\tau)\v(t(\tau))$.
Hence  $\w(\v'(\tau))=w(\v(t(\tau)))t'(\tau)$.
 For the new curve  $C'$
           $$
  \int_{C^\prime} \w=\int_{\tau_1}^{\tau_2}\w(\v'(\tau))d\tau=
   \int_{\tau_1}^{\tau_2}\w(\v(t(\tau)){dt(\tau)\over d\tau}d\tau=
   \int_{t(\tau_1)}^{t(\tau_2)}\w(\v(t))dt
              $$

  $t(\tau_1)=t_1$, $t(\tau_2)=t_2$ if reparameterisation does not change orientation and
  $t(\tau_1)=t_2$, $t(\tau_2)=t_1$ if reparameterisation changes orientation.

Hence $\int_{C'} w=\int_{t_1}^{t_2)}\w(\v(t))dt=\int_C \w$ if orientation is not changed and
$\int_{C'} w=\int_{t_2}^{t_1)}\w(\v(t))dt=-\int_{t_1}^{t_2)}\w(\v(t))dt=-\int_C \w$
is orientation is changed.
}

\m
{\bf Example}


Let
         $$
       \w=a(x,y)dx+b(x,y)dy
         $$

be $1$-form in $\E^2$ ($x,y$--are usual Cartesian coordinates).
Let
$C\colon\quad \r=\r(t)\quad\begin{cases}x=x(t)\cr y=y(t)\cr \end{cases}$,
 $t_1\leq t\leq t_2$ be a curve in $\E^2$.



  Consider velocity vector field of this curve
\begin{equation}\label{velocityvector}
  \v(t)={d\r(t)\over dt}=\begin{pmatrix}v_x(t)\cr v_y(t)\cr \end{pmatrix}=
   \begin{pmatrix}x_t(t)\cr y_t(t)\cr
    \end{pmatrix}=x_t\p_x+y_t\p_y
 \end{equation}
($x_t={dx(t)\over dt}$, $y_t={dy(t)\over dt}$).

  One can consider the value of one form $\w$ on the velocity vector field $\v(t)$ of the curve:
  $\w(\v)=a(x(t),y(t))dx(\v)+b(x(t),y(t))dy(\v)=$
             $$
      a(x(t),y(t))x_t(t)+b(x(t),y(t))y_t(t)\,.
             $$
The integral of the form $\omega=a(x,y)dx+b(x,y)dy$ over the curve
$C\colon\quad \r=\r(t)\quad t_1\leq t\leq t_2$ is equal to the integral of the function $\w(\v(t))$
over the interval $t_1\leq t\leq t_2$:
              \begin{equation}\label{definitionofintegral}
    \int_C \w=\int_{t_1}^{t_2}\w(\v(t))dt=
               \int_{t_1}^{t_2}
               \left(
    a(x(t),y(t)){dx(t)\over dt}+b(x(t),y(t)){dy(t)\over dt}
              \right)dt\,.
            \end{equation}
\m

{\bf Example} Consider an integral of the form $\w=3dy+3y^2dx$ over the curve
$C\colon \r(t)\,\, \begin{cases}x=\cos t\\ y=\sin t \end{cases}, 0\leq t\leq \pi/2$.
($C$ is the arc of the circle $x^2+y^2=1$ defined by conditions $x,y\geq 0$).

Velocity vector $  \v(t)={d\r(t)\over dt}=\begin{pmatrix}v_x(t)\\ v_y(t)\\\end{pmatrix}=
   \begin{pmatrix}x_t(t)\\ y_t(t)\\\end{pmatrix}=\begin{pmatrix}-\sin t\\ \cos t\\\end{pmatrix}$.
   The value of the form on velocity vector is equal to
   $$
\w(\v(t))=3y^2(t)v_x(t)+3v_y(t)=3\sin^2 t(-\sin t)+3\cos t=3\cos t-3\sin^3 t
   $$
and
   $$
   \int_C \w=\int_0^{\pi\over 2} w(\v(t))dt=
   \int_0^{\pi\over 2}(3\cos t-3\sin^3 t)dt=3\left(\sin t+\cos t-{\cos^3 t\over 3}\right)
   \big\vert^{\pi\over 2}_0
   $$


\m

{\bf Example }
Now consider  of the same form $\w$ over 
the the curve $C$ which is the upper half of the circle $x^2+y^2=1$:
$C\colon\,\,\begin{cases} x^2+y^2=1\cr y\geq 0\end{cases}$.
 Curve is given as an image.
 We have the image of the curve not the parameterised curve. 
We have to define a parameterisation ourself.

We  consider three different parameterisations of this curve.
Sure to calculate the integral it suffices to calculate 
$\int_C w$ in an arbitrary given parameterisation $\r=\r(t)$ of the curve $C$,
then note that for an arbitrary reparameterisation
$t=t(\tau)$, the integral will remain the same or it 
will change a sign depending on the reparameterisation $t=t(\tau)$ 
preserves  orientation or not. 
           $$
           \r_1(t)\colon\,\,
              \begin{cases}
              x=R\cos t\cr
              y=R\sin t\cr
              \end{cases}, 0\leq t\leq \pi\,,
              \,\,\r_2(t)\colon\,\,
              \begin{cases}
              x=R\cos \Omega t\cr
              y=R\sin \Omega t\cr
              \end{cases},
               0\leq t\leq {\pi\over \Omega}\,, (\Omega>0)
              $$
           and
           \begin{equation}\label{diffparamofhalfcircle}
         \r_3(t)\colon\,\,
              \begin{cases}
              x=t\cr
              y=\sqrt {R^2-t^2}\cr
              \end{cases}, -R\leq t\leq R,,\qquad
               \end{equation}

All these curves are the same image. If $\Omega=1$ 
the second curve coincides with the first one.
First and second curve have the same orientation 
(reparameterisation $t\mapsto \Omega t$)
The third curve have orientation opposite to first 
and second (reparameterisation
$t\mapsto \cos t$, the derivative ${d\cos t\over dt}<0$).


Calculate integrals $\int_{C_1} \w$, $\int_{C_2} \w$, $\int_{C_3} \w$
and check straightforwardly that these integrals coincide
if orientation is the same or they have different signs
if orientation is opposite.
              $$
 \int_{C_1} \w=\int_0^\pi(xy_t-yx_t)dt=\int_0^\pi(R^2\cos^2 t+R^2\sin^2 t)dt=\pi R^2
              $$
          $$
 \int_{C_2} \w=\int_0^{\pi\over \Omega}
 (xy_t-yx_t)dt=\int_0^\pi(R^2\Omega\cos^2 \Omega t+R^2\Omega\sin^2 \Omega t)dt=\pi R^2\,.
         $$
These answers coincide: both parameterisation have the same orientation.

 For the third parameterisation:
           $$
     \int_{C_3} \w=\int_0^{R}(xy_t-yx_t)dt=\int_0^{R}\left(
     t\left({-t\over \sqrt {R^2-t^2}}\right)-{\sqrt {R^2-t^2}}
                 \right)dt=
           $$
         $$
         -R^2
         \int_0^{R} {dt\over \sqrt {R^2-t^2}}=-R^2\int_0^{1} {du\over \sqrt {1-u^2}}=-\pi R^2
         $$
We see that the sign is changed.

{\footnotesize Finally consider the integral of the form $\omega=xdy-ydx$ over the semicircle
in polar coordinates instead Cartesian coordinates, We have that in polar coordinates semicircle is
$\begin{cases} r(t)=R\cr \varphi (t)=t\end {cases}$,
$0\leq t\leq \pi$.  The form
$\w=xdy-ydx=r\cos\varphi d(r\sin \varphi)-r\sin\varphi d(r\sin \varphi)=r^2d\varphi$
and $\v(t)=(r_t,\varphi_t)=(0,1)$, i.e. $\v(t)=\p_\varphi$. We have that $\w(\v(t))=
r(t)^2d\varphi(\p_\varphi)=R^2$. Hence
             $
             \int_C \omega=\int_0^{\pi}R^2dt=\pi R^2$.
Answer is the same: The value of integral does not change if we change coordinates in the plane.
}

\smallskip

For other examples see Homeworks.



\subsection {Integral over curve of exact form}


$1$-form $\w$ is called exact if there exists a function $f$ such that $\w=df$.

\bigskip

{\bf Theorem}

{\it Let $\w$ be an exact $1$-form in $\E^n$, $\w=df$.

Then the integral of this form over an arbitrary curve   $C\colon\quad \r=\r(t)\quad t_1\leq t\leq t_2$
     is equal to the difference of the values of the function $f$ at starting and ending points
     of the curve $C$:
             \begin{equation}\label{theorem1}
    \int_C \w=f\big\vert_{\p C}=f(\r_2)-f(\r_1)\,, \quad \r_1=\r(t_1),\r_2=\r(t_2)\,.
\end{equation}
}

\smallskip
\textsl{Proof:}
 $\int_C df=\int_{t_1}^{t_2}df(\v(t))=\int_{t_1}^{t_2}{d\over dt}f(\r(t))dt=f(\r(t))\vert_{t_1}^{t_2}$.



\m

{\bf Example} Calculate an integral of the form $\w=3x^2(1+y)dx+x^3dy$ over the arc  of the semicircle
$x^2+y^2=1, y\geq 0$.

One can calculate the integral naively using just the formula \eqref{definitionofintegral}:
Choose a parameterisation of $C$,e.g., $x=\cos t,y=\sin t$, then $\v(t)=-\sin t\p_x+\cos t\p_x$ and
$\w(\v(t))=(3x^2(1+y)dx+x^3dy)(-\sin t\p_x+\cos t\p_y)=-3\cos^2 t(1+\sin t)\sin t+\cos^3 t\cdot \cos t$ and
                 \begin{equation*}\label{naivecalculofintegral}
    \int_C \w=\int_0^{\pi}(-3\cos^2 t\sin t-3\cos^2t\sin^2 t+\cos^4 t)dt=...
\end{equation*}
Calculations are little bit long.

But  for the form $\w=3x^2(1+y)dx+x^3dy$ one can calculate the integral in
a much more efficient way noting that
it is an exact form:
\begin{equation}\label{thisformisexact}
    \w=3x^2(1+y)dx+x^3dy=d\left(x^3(1+y)\right)
\end{equation}
Hence it follows from the Theorem that
\begin{equation}\label{notnaiveclalculations}
        \int_C \w=f(\r(\pi))-f(\r(0))=x^3(1+y)\big\vert^{x=-1,y=0}_{x=1,y=0}=-2
\end{equation}
{\bf Remark} If  we change the orientation of curve then the starting point becomes the ending point
 and the ending point becomes the starting point.--- The integral changes the sign in accordance
  with general statement, that integral of 1-form over parameterised curve is defined up
  to reparameterisation.

  \bigskip


{\bf Corollary} {\it The integral of an exact form over an arbitrary closed curve is equal to zero.}

{\textsl{} Proof}. According to the Theorem
   $\int_C \w=\int _C df= f\big\vert_{\partial C}=0$, because
the starting and ending points of closed curve coincide.

\m

 {\bf Example}.   Calculate the integral of $1$-form $\w=x^5dy+5x^4ydx$ over the ellipse $x^2+{y^2\over 9}=1$\,.

    The form  $\w=x^5dy+5x^4ydx$ is exact form because $\w=x^5dy+5x^4ydx=d(x^5y)$. Hence the integral over ellipse
    is equal to zero, because it is a closed curve.

    \smallskip

{\bf  Remark}
The remarkable Theorem and Corollary of this section  works only for exact forms.
Of course not any form is an exact form (see exercises in Homeworks and subsection 2.9 below)
E.g. $1$-form $xdy-ydx$ is not an exact form\footnote
{if $xdy-ydx=df=f_xdx+f_ydy$, then $f_y=x$ and $f_x=-y$. We see that on one hand
$f_{xy}=(f_x)_y=-1$ and on the other hand $f_{yx}=(f_y)_x=1$.
Contradiction.}.


{\footnotesize
\subsection {$\dagger$ Differential $2$-forms in $\E^2$}



 We considered detailed definition of $1$-forms. Now we give some formal
 approach to describe $2$-forms.


  Differential forms on $\E^2$ is an expression obtained by adding and multiplying
  functions and differentials $dx,dy$. These operations obey usual associativity and distributivity laws
  but multiplications is not moreover of one-forms on each other is {\it anticommutative}:
               \begin{equation}\label{wedge}
       \w\wedge \w'=-\w'\wedge \w \quad \hbox {if $\w,\w'$ are $1$-forms}
\end{equation}
In particular
 \begin{equation}\label{wedge2}
 dx\wedge dy=-dy\wedge dx, dx\wedge dx=0, dy\wedge dy=0
\end{equation}


{\bf Example} If $\w=xdy+zdx$ and $\rho=dz+ydx$ then
 $$\w\wedge \rho=(xdy+zdx)\wedge (dz+ydx)=xdy\wedge dz+zdx\wedge dz+xy dy\wedge dx$$
and
       $$
       \rho\wedge\w=(dz+ydx)\wedge (xdy+zdx)=xdz\wedge dy+zdz\wedge dx+xy dx\wedge dy=-\w\wedge\rho
       $$
{\footnotesize
{\it Changing of coordinates.} If $\w=a(x,y)dx\wedge dy$ be two form and $x=x(u,v), y=y(u,v)$
new coordinates then $dx=x_udu+x_vdv$, $dy=y_udu+y_vdv$
($x_u={\p x(u,v)\over \p u}$, $x_v={\p x(u,v)\over \p v}$,
$y_u={\p y(u,v)\over \p u}$, $y_v={\p y(u,v)\over \p v}$).
and
  \begin{equation}\label{changingofcoordinfortwoform}
    a(x,y)dx\wedge dy=a\left(x(u,v),y(u,v)\right)\left(x_udu+x_vdv\right)\wedge \left(y_udu+y_vdv\right)=
\end{equation}
  $$
a\left(x(u,v),y(u,v)\right)\left(x_udu+x_vdv\right)(x_uy_vdu\wedge dv+x_vy_u dv\wedge du)=
   $$
   $$
a\left(x(u,v),y(u,v)\right)(x_uy_v-x_vy_u)du\wedge dv
  $$
{\bf Example} Let $\w=dx\wedge dy$
then in polar coordinates   $x=r\cos\varphi,y=r\sin\varphi$
\begin{equation}\label{areaforminpolarcoordinates}
    dx\wedge dy=(\cos\varphi dr-r\sin\varphi d\varphi)\wedge (\sin\varphi dr+r\cos\varphi d\varphi)=rdr\wedge d\varphi
\end{equation}
}




\subsection {$\dagger$ $0$-forms (functions) ${\buildrel d\over \longrightarrow}$
$1$-forms ${\buildrel d\over \longrightarrow}$ $2$-forms }

We introduced differential $d$ of functions ($0$-forms) which transform them to $1$-form.
It obeys the following condition:
\begin {itemize}
\item  $d\colon$ is linear operator: $d(\lambda f+\mu g)=\lambda df+\mu dg$

\item   $d(fg)=df\cdot g+f\cdot dg$
\end {itemize}
Now we introduce differential on $1$-forms such that
\begin {itemize}
\item  $d\colon$ is linear operator on $1$-forms also
  \item  $d(fw)=df\wedge w+f dw$
  \item $d df=0$
\end {itemize}


{\bf Remark} Sometimes differential $d$ is called {\it exterior differential}.

\m

 Perform calculations using this definition and \eqref{wedge}:
     $$
   d\w=d(\w_1 dx+\w_2 dy)=dw_1\wedge dx+ dw_2\wedge dy=
       \left({\p \w_1(x,y)\over \p x}dx+{\p \w_1(x,y)\over \p y}dy\right)\wedge dx+
          $$
          $$
          \left({\p \w_2(x,y)\over \p x}dx+{\p \w_2(x,y)\over \p y}dy\right)\wedge dy=
\left({\p \w_2(x,y)\over \p x}-{\p \w_1(x,y)\over \p y}\right)dx\wedge dy
          $$
{\bf Example} Consider $1$-form $\w=xdy$. Then
$d\w=d(xdy)=dx\wedge dy$.

%It is instructive to perform calculations for $d\w$ in polar coordinates.
% Note that
%  $$
%  \w=xdy={1\over 2}(xdy-ydx)+{1\over 2}(xdy+ydx)={1\over 2}r^2d\varphi+d\left({xy\over 4}\right)=
%  {1\over 2}r^2d\varphi+d\left({r^2\sin 2\varphi\over 4}\right)
%  $$
%  Hence  in polar coordinates
%\begin{equation}\label{claculationofxdyinpolarcoordinates}
%    d\w=d\left({1\over 2}r^2d\varphi+d\left({r^2\sin 2\varphi\over 4}\right)\right)=rdr\wedge d\varphi
%\end{equation}
%because $d^2=0$.  Answers coincide, $dx\wedge dy=rdr\wedge d\varphi$.

%\end{document}
\subsection {$\dagger$Exact and closed forms}

{\small We know that it is very easy to integrate  exact $1$-forms over curves
(see the subsection "Integral over curve of exact form")



How to know  is the $1$-form exact or no?




{\bf Definition} We say that one form $\w$ is {\it closed} if two form $d\w$ is equal to zero.

{\bf Example} One-form $xdy+ydx$ is closed because $d(xdy+ydx)=0$.





\m

It is evident that exact $1$-form is closed:
\begin{equation}\label{exactformisclosed}
    \w=d\rho\Rightarrow d\w=d(d\rho)=d\circ d\rho=0
\end{equation}
We see that the condition that form is closed is necessary condition that form is exact.

So if $d\w\not=0$, i.e. the form is not closed, then it is not exact.}

{\footnotesize Is this condition sufficient? Is it true that a closed form is exact?




In general the answer is: {\it No}.


E.g. we considered differential $2$-form
                 \begin{equation}\label{closednotexactform}
    \w={xdy-ydx\over x^2+y^2}
\end{equation}
defined in $\E^2\backslash 0$. It is closed, but it is not exact (See non-compulsory
exercises 11,12,13 in the Homework 6).


\m


How to recognize for $1$-form $\w$ is it exact or no?





Inverse statement (Poincar\'e lemma) is true if $1$-form is well-defined in $\E^2$:


  {\it A closed $1$-form $\w$ in $\E^n$ is exact if it is well-defined at all points of $\E^n$,
   i.e. if it is differentiable function at all points of $\E^n$}.}




\m

{\footnotesize Sketch a proof for $1$-form in $\E^2$: if $\w$ is defined in whole $\E^2$ then
consider the function
\begin{equation}\label{poincarelemma1}
    F(\r)=\int_{C_{\r}} \w
\end{equation}
where we denote by $C_{\r}$ an arbitrary curve which starts at origin and ends at the point $\r$.
It is easy to see that the integral is well-defined and one can prove that $\w=df$.

The explicit formula for the function \eqref{poincarelemma1} is the following:
If $\w=a(x,y)dx+b(x,y)dy$ then $F(x,y)=\int_0^1 \left(a(tx,ty)x+b(tx,ty)y\right)dt$.

{\bf Exercise}  Check by straightforward calculation that $\w=dF$ (See exercise 14 in Homework 6)}.

}

%\end{document}
{\footnotesize
\subsection {$^\dagger$Integration of two-forms. Area of the domain}

 We know that $1$-form is a linear function on tangent vectors.
\def\B {{\bf B}}
If $\A,\B$
are two vectors attached at the point $\r_0$, i.e. tangent to this point
and $\w,\rho$ are two $1$-forms then one  defines the value of $\w\wedge \rho$
on $\A, B$ by the formula
\begin{equation}\label{valueoftwoformontwovectors1}
    \w\wedge\rho(\A,\B)=\w(\A)\rho(B)-\w(B)\rho(A)
\end{equation}
We come to bilinear anisymmetric function on tangent vectors.
If $\sigma=a(x,y)dx\wedge dy$ is an arbitrary two form then this form defines
bilinear form on pair of tangent vectors:  $\sigma(\A,\B)=$
\begin{equation}\label{valueoftwoformontwovectors1}
    a(x,y)dx\wedge dy(A,B)=a(x,y)\left(dx(\A)dy(\B)-dx(\B)dy(\A)\right)=
    a(x,y)(A_xB_y-A_yB_y)
\end{equation}
One can see that in the case if $a=1$ then
right hand side of this formula is nothing but the area of parallelogram
spanned by the vectors $\A,\B$.



This leads  to the conception of integral of form over domain.


Let $\w=a(x)dx\wedge dy$ be a two form and $D$ be a domain in $\E^2$. Then by definition
 \begin{equation}\label{intoftwoforms}
    \int_D \w=\int_D a(x,y)dx dy
\end{equation}
If $\w=dx\wedge dy$ then
\begin{equation}\label{intoftwoforms2}
    \int_D w=\int_D (x,y)dx dy=\hbox{Area of the domain $D$}
\end{equation}


The advantage of these formulae is that we do not care about coordinates\footnote
{If we consider changing of coordinates then jacobian appears:
If $u,v$ are new coordinates, $x=x(u,v)$, $y=y(u,v)$ are new coordinates then
\begin{equation}\label{jacobia}
    \int a(x,y)dxdy=\int a(x(u,v), y(u,v))\det\begin{pmatrix}x_u & x_v\\ x_u & x_v\\
          \end{pmatrix}dudv
\end{equation}
In formula\eqref{intoftwoforms} it appears under as a part of coefficient of differential form.}


\m

{\bf Example} Let $D$ be a domain defined by the conditions
          \begin{equation}\label{upperhalfellipse}
      \begin{cases}
      x^2+y^2\leq 1\\
      y \geq 0\\
      \end{cases}
\end{equation}
 Calculate $\int_D dx\wedge dy$.

 $\int_D dx\wedge dy=\int_D dxdy=$ area of the $D={\pi\over 2}$.

 If we consider polar coordinates then according \eqref{areaforminpolarcoordinates}
      $$
      dx\wedge dy=rdr\wedge d\varphi
      $$
Hence $\int_D dx\wedge dy=\int_D rdr\wedge d\varphi=\int_D rdrd\varphi=
\int_0^1\left(\int_0^\pi d\varphi\right)rdr$ $=\pi \int_0^1rdr=\pi/2$.



{\footnotesize Another example


{\bf Example}  Let $D$ be a domain in $\E^2$ defined by the conditions
          \begin{equation}\label{upperhalfellipse}
      \begin{cases}
      {(x-c)^2\over a^2}+{y^2\over b^2}\leq 1\\
      y \geq 0
      \end{cases}
\end{equation}

$D$ is domain restricted by upper half of the ellipse and $x$-axis. Ellipse has the centre at the point $(c,0)$.
Its area is equal to  $S=\int_D dx\wedge dy$. Consider new variables $x', y'$: $x=c+ax', y=by'$.
In new variables domain $D$ becomes the domain from the previous example:
                $$
{(x-c)^2\over a^2}+{y^2\over b^2}={x'}^2+{y'}^2
                $$
         and $dx\wedge dy=ab dx'\wedge dy'$.
         Hence
             \begin{equation}\label{areaofhalellipse}
    S=\int_{{(x-c)^2\over a^2}+{y^2\over b^2}\leq 1,y\geq 0} dx\wedge dy=
    ab\int_{{x'}^2+{y'}^2\leq 1,y'\geq 0} dx'\wedge dy'={\pi ab\over 2}
\end{equation}
}
{\footnotesize
{\bf Theorem 2} ( Green formula) Let $\w$ be $2$-form such that $\w=d\w'$
and  $D$ be a domain--interior of the closed curve $C$. Then
  \begin{equation}\label{integralofexactform}
\int_D \w = \int_C \w'
\end{equation}
}}



%12 March

%\end{document}


\section {Curves in Euclidean space. Curvature}

\subsection {Curves. Velocity and acceleration vectors}


We already study velocity vector  of curves. Consider now acceleration vector
$\ac={d^2\r(t)\over dt^2}$. For curve $\r=\r(t)$ in $\E^n$ we have
          $$
          \v={d\r(t)\over dt},\,\, v^i={dx^i(t)\over dt},\,\, (i=1,2,\dots,n)\,\,,
          $$
and
\begin{equation}\label{accelervector}
\ac={d\v(t)\over dt}={d^2\r(t)\over dt^2}, a^i={{d^2x^i(t)\over dt^2}}\,,
(i=1,2,\dots,n)\,\,.
\end{equation}
Velocity vector $\v(t)$ is tangent to the curve.
In general acceleration vector is not tangent to the curve.
One can consider decomposition of acceleration vector $\ac$ on tangential and normal component:
    \begin{equation}\label{decompositontangentandorthogonal}
    \ac=\ac_{tangent}+\ac_{\perp},
\end{equation}
where $\ac_{tangent}$ is the vector tangent to the curve (collinear to velocity vector) and $\ac_{\perp}$
is orthogonal to the tangent vector (orthogonal to the velocity vector).
The vector $\ac_{\perp}$ is called normal acceleration vector of the curve
\footnote{Component of acceleration orthogonal to the velocity vector sometimes is called
also {\it centripetal acceleration}}.

{\bf Example} Consider a curve
       \begin{equation}\label{curve3sandacceleration}
      C\colon\quad\begin{cases}
  x=R\cos\Omega t\\
  y=R\sin\Omega t\\
  \end{cases}, \quad
\end{equation}
If we consider parameter $t$ as a time then we have the point which moves over circle of the radius
$R$ with angular velocity $\Omega$. We see that
             $$
\v=\begin{pmatrix}-R\Omega \sin\Omega t\\ R\Omega \cos\Omega t\end{pmatrix},
\ac=-\begin{pmatrix}R\Omega^2 \cos\Omega t\\ R\Omega^2 \sin\Omega t\end{pmatrix}=-\Omega^2 \r(t)
             $$
 Speed is constant: $|\v|=R\Omega$. Acceleration is perpendicular to the velocity.
 (It is just
 {\it centripetal acceleration}.)


What happens if speed is increasing, or decreasing, i.e. if angular velocity is not constant?
One can see that in this case tangential acceleration is not equal to zero, i.e. the
velocity and acceleration are not orthogonal to each other.


Analyze the meaning of an angle between velocity and acceleration vectors for an arbitrary parameterised
curve $\r=\r(t)$.
For this purpose consider the equation for speed:
          $|\v|^2=(\v,\v)$ and differentiate it:
           \begin{equation}\label{diffbytime}
 {d|\v|^2\over dt}={d\over dt}(\v(t),\v(t))=2(\v(t),\ac(t))=2|\v(t)||\ac(t)|\cos\theta (t)\,,
\end{equation}
where $\theta$ is an angle between velocity vector and  acceleration vector.


\m
We formulate the following

{\bf Proposition}

Suppose that parameter $t$ is just time.
We see from this formula that if point moves along the curve $\r(t)$ then


\begin{itemize}

\item speed is increasing in time if and only if the
angle between velocity and acceleration vector is acute, i.e. tangential acceleration
has the same direction as a velocity vector:
              \begin{equation}\label{anglebetweenvelocityandacceleration1}
    {d|\v|^2\over dt}>0 \Leftrightarrow (\v,\ac)>0\Leftrightarrow
         \cos \theta>0\Leftrightarrow \ac_{tang}=\lambda \v\,\,{\rm with}\,\, \lambda>0\,.
\end{equation}
\item speed is decreasing in time if and only if the
angle between velocity and acceleration vector is obtuse,
i.e. tangential acceleration
has the direction opposite to the direction of a velocity vector.
              \begin{equation}\label{anglebetweenvelocityandacceleration2}
    {d|\v|^2\over dt}<0 \Leftrightarrow (\v,\ac)<0\Leftrightarrow
         \cos \theta< 0\Leftrightarrow \ac_{tang}=\lambda \v\,\,{\rm with}\,\, \lambda<0\,.
\end{equation}
\item speed is constant in time if and only if the
velocity and acceleration vectors are orthogonal to each other, i.e. tangential acceleration
is equal to zero.
              \begin{equation}\label{anglebetweenvelocityandacceleration3}
    {d|\v|^2\over dt}=0 \Leftrightarrow (\v,\ac)=0\Leftrightarrow
         \cos \theta=0\Leftrightarrow \ac_{tang}=0\,.
\end{equation}
\end{itemize}

%\end{document} % 18 March

{\bf Example} Consider the curve $\r(t)\colon \begin{cases}x(t)=v_xt\\ y(t)=v_y t-{gt^2\over 2}\end{cases}$
 It is path of the point moving under the gravity force with initial velocity
 $\v=\begin{pmatrix}v_x\\ v_y\end{pmatrix}$.
One can see that the curve is parabola: $y=\left(v_y\over v_x\right)x-\left(gv_y^2\over v_x^2\right)x^2$.
We have that $\v(t)=\begin{pmatrix}v_x\\ v_y-gt\end{pmatrix}$
and acceleration vector $\ac=\begin{pmatrix}0\\ -g\end{pmatrix}$.
Suppose that $v_y>0$. $(\v,\ac)=-g(v_y-gt)$.
 Then  at the highest point (vertex of the parabola) ($t=v_y/g$)
acceleration is orthogonal to the velocity. For $t< v_y/g$  angle between acceleration and velocity vectors
is obtuse. Speed is decreasing. For $t> v_y/g$  angle between acceleration and velocity vectors
is acute. Speed is increasing.

\bigskip

\subsection { Behaviour of acceleration vector under reparameterisation}

\m

   How acceleration vector changes under changing of parameterisation of the curve?



   Let $C\colon \quad \r=\r(t), t_1\leq t\leq t_2$ be a curve and $t=t(\tau)$ reparametrisation of this curve.
   We know that for new parameterised curve $C'\colon \quad \r'(\tau)=\r(t(\tau)), \tau_1\leq \tau\leq \tau_2$
   velocity vector $\v'(\tau)$ is collinear to the velocity vector $\v(t)$ (see \eqref{changingofvelocity}):
             $$
             \v'(\tau)={d\r'(\tau)\over d\tau}={d\r(t(\tau))\over d\tau}={dt(\tau)\over d\tau}{d\r(t(\tau))\over d t}=
             t_\tau\v(t(\tau))
             $$
Taking second derivative we see that for acceleration vector:
 \begin{equation}\label{changingofaccelerationvectorunderchangingofparametrisation}
\ac'(\tau)={d^2\r'(\tau)\over d\tau^2}={d\v'(\tau)\over d\tau}={d\over d\tau}\left( t_\tau\v(t(\tau))\right)=
    t_{\tau\tau}\v(t(\tau))+t_\tau^2\ac(t(\tau))
\end{equation}

Under reparameterisation acceleration vector in general changes its direction:
new acceleration vector becomes linear combination of old velocity and acceleration vectors:
direction of acceleration vector does not remain unchanged
\footnote{The plane spanned by velocity and acceleration vectors
remains unchanged.(This plane is called osculating plane.)}.




\bigskip
We know that acceleration vector can be decomposed on tangential and normal components
(see \eqref{decompositontangentandorthogonal}). Study how tangential and normal components change under
reparameterisation.

Decompose left and right hand sides of the equation \eqref{changingofaccelerationvectorunderchangingofparametrisation}
on tangential and orthogonal components:
       $$
       \ac'(\tau)_{tangent}+\ac'(\tau)_{\perp}=t_{\tau\tau}\v(t)+
       t_\tau^2\left(\ac(t)_{tangent}+\ac(t)_{\perp}\right)
      $$
Then comparing tangential and orthogonal components we see that new tangential acceleration
is equal to
\begin{equation}\label{changingoftangential}
       \ac'(\tau)_{tangent}=t_{\tau\tau}\v(t)+
       t_\tau^2\ac(t)_{tangent}
\end{equation}
and  normal acceleration is equal to
\begin{equation}\label{changingofcentripetalaccele}
      \ac'(\tau)_{\perp}=
 t_\tau^2\ac(t)_{\perp}
\end{equation}

The magnitude of normal (centripetal) acceleration under changing of parameterisation is multiplied on the $t_\tau^2$.
Now recall that magnitude of velocity vector under reparameterisation is multiplied  on $t_\tau$.
We come to very interesting and important observation:

\bigskip

\centerline {\bf Observation}
                       \begin{equation}\label{observation}
 \hbox {The magnitude}\,\,   {|\ac_{\perp}|\over |\v^2|} \,\,\hbox{remains unchanged under reparameterisation.}
\end{equation}


\m

We come to the expression which is independent of parameterisation: it must have
deep mechanical and geometrical meaning. We see later that it is nothing but curvature.


\subsection {Length of the curve }


 If $\r(t), a\leq t\leq b$
is a parameterisation of the curve $L$ and $\v(t)$ velocity vector
 then length of the curve is equal to the integral of
of $|\v(t)|$  over curve:
          \begin{equation}\label{simpleformulaforlength}
   \hbox{Length of the curve $L$}=\int_a^b |\v(t)|dt=
\end{equation}
$$
\int_a^b\sqrt {\left({dx^1(t)\over dt}\right)^2+
                  \left({dx^2(t)\over dt}\right)^2+\dots+
                \left({dx^n(t)\over dt}\right)^2}dt\,.
$$


Note  that formula above is {\it reparameterisation} invariant. The length of the image of the curve
does not depend on parameterisation. This corresponds to our intuition.


{\footnotesize {\textsl{Proof }}Consider curve
  $\r_1=\r_1(t)$, $a_1\leq t\leq b_1$. Let
      $t=t(\tau)$, $a_2<\tau< b_2$  be another parameterisation of the curve
$\r=\r(t)$, In other words we have two different parameterised curves $\r_1=\r_1(t), a_1\leq t\leq b_1$
and $\r_2=\r_1(t(\tau))$, $a_2\leq \tau\leq \b_2$ such that their images coincide (See \eqref{repardef}).
    Then under reparameterisation velocity vector is multiplied on $t_\tau$
                   $$
    \v_2(\tau)={d\r_2\over d\tau}={dt\over d\tau}{d\r_1 \over dt}=t_\tau(\tau)\v_1(t(\tau))
                $$
Hence
\begin{equation}\label{lenghtreparinvariant}
L_1=\int_{a_1}^{b_1}|\v_1(t)|dt=
\int_{a_2}^{b_2} |\v_1(t)|{dt(\tau)\over d\tau}d\tau=
\int_{a_2}^{b_2} |t_\tau\v_1(t)|d\tau=\int_{a_2}^{b_2} |\v_2(\tau)|d\tau=L_2\,,
 \end{equation}
i.e. length of the curve does not change under reparameterisation.}


\bigskip


If $C\colon\quad \r=\r(t)$ $t_1\leq t\leq t_2$ is a curve in $\E^2$ then its length is equal to
            \begin{equation}\label{lengthofthecurve}
   L_C=\int_{t_1}^{t_2}|\v(t)|dt=\int_{t_1}^{t_2}\sqrt {\left({dx(t)\over dt}\right)^2+
                  \left({dy(t)\over dt}\right)^2}dt
                \end{equation}


\subsection  { Natural parameterisation of the curves}


  Non-parameterised curve can be parameterised in  many different ways.


  Is there any distinguished  parameterisation? Yes, it is.


   {\bf Definition}  A natural parameter $s=s(t)$ on the curve $\r=r(t)$ is a parameter
    which defines the length of the arc of the curve between initial point $\r(t_1)$ and
    the point $\r(t)$.

%\end{document} % 24 March

    If a natural parameter $s$ is chosen we say that a curve $\r=\r(s)$ is given in natural parameterisation.

\m

   Write down explicit formulae for natural parameter.



   Let $C: \r(t),  a<t<b$ be a curve in $\E^n$.
   As always  we suppose that it is smooth and regular curve: (i.e. $\r(t)$ has derivatives of arbitrary order,
   and velocity vector $\v\not=0$.

    Then it follows from \eqref{simpleformulaforlength}
    that
\begin{equation}
  s(t)=\{\hbox {length of the arc of the curve
          between points $\r(a)$ and $\r(t)$}\}
\end{equation}
   $$
  =\int_a^t|\v(t')|dt'=
   $$
\begin{equation}\label{naturalparamdefinition}
=
\int_a^t\sqrt {\left({dx^1(t')\over dt'}\right)^2+
                  \left({dx^2(t')\over dt'}\right)^2+\dots+
                \left({dx^n(t')\over dt'}\right)^2}dt'\,.
\end{equation}
(  As always  we suppose that it is smooth and regular curve: (i.e. $\r(t)$ has derivatives of arbitrary order,
   and velocity vector $\v\not=0$.)

 {\bf Example} Consider circle:  $x=R\cos t, y=R\sin t$ in $\E^2$. Then
 we come to the obvious answer
                  $$
               s(t)=
               \{\hbox {length of the arc of the circle
               between points $\r(0)$ and $\r(t)$}\}=Rt=
               $$
               $$
               \int_0^t\sqrt {\left({dx(t')\over dt'}\right)^2+
               \left({dy(t')\over dt'}\right)^2}dt'=
        \int_0^t\sqrt {R^2\sin^2 t'+
               R^2\cos^2 t'}dt'=
              \int_a^t Rdt'=Rt
              $$
$s=Rt$. Hence in natural parameterisation $x=R\cos{s\over R}$, $y=R\sin{s\over R}$.
\bigskip


\smallskip

{\bf Remark} If we change an initial point then a natural parameter changes on a constant.

\smallskip

For example if we choose as a initial point for the circle above a point $\r(t_1)$ for $t_1=-{\pi\over 2}$,
then the length of the arc between points $\r(-{\pi\over 2})$ and $\r(0)$ is equal to $R{\pi\over 2}$ and

             $$
     s'(t)=s(t)+R{\pi\over 2}\,.
             $$

{\footnotesize
Another

{\bf Example}  Consider arc of the parabola $x=t, y=t^2, 0<t<1$:
\begin{equation}\label{naturalparameterforparabola}
              s(t)=
   \{\hbox {length of the arc of the curve for
          parameter less or equal to $t$}\}=
          \end{equation}
                  $$
              \int_0^t\sqrt {\left({dx(\tau)\over d\tau}\right)^2+
               \left({dy(\tau)\over d\tau}\right)^2}d\tau=
                $$
                $$
        \int_0^t\sqrt {1+
               4\tau^2}d\tau=
               {t\sqrt {1+4t^2}\over 2}+{1\over 4}\log
                  \left(2t+\sqrt {1+4t^2}\right)
                $$
The first example was very simple. The second is harder to calculate
\footnote
{Denote by $I=\int_0^t\sqrt {1+
               4\tau^2}d\tau$. Then integrating by parts we come to:
               $$
I=t\sqrt {1+4t^2}-\int {4\tau^2\over \sqrt {1+4\tau^2}}d\tau=
      t\sqrt {1+4t^2}-I+\int {1\over \sqrt {1+4\tau^2}}d\tau\,.
               $$
   Hence
               $$
         I={t\sqrt {1+4t^2}\over 2}+{1\over 2}\int {1\over \sqrt {1+4\tau^2}}d\tau\,.
               $$
  and we come to the answer.}.
In general case natural parameter is not so easy to calculate.
But its notion is  very important for studying properties of curves.}


\medskip


 Natural parameterisation is distinguished.
 Later we will often use the following very important
  property of natural parameterisation:


\medskip

  {\bf Proposition} {\it If a curve is given in natural parameterisation
  then


   \begin{itemize}
\item  the speed is equal to $1$

    \begin{equation}\label{firstpropertyofnaturalparameter}
  (\v(s),\v(s))\equiv 1,\quad {\rm i.e.}\,\,|\v(s)|\equiv 1\,,
\end{equation}


\item  acceleration is orthogonal to velocity, i.e. tangential acceleration is equal to zero:
\begin{equation}\label{secondcondpropertyofnaturalparameter}
(\v(s),\ac(s))=0\,,\quad {\rm i.e.} \,\,\ac_{tangent}=0\,.
\end{equation}

\end{itemize}
}

   \textsl{Proof:} For an arbitrary parameterisation $|\v(t)|={dL(t)\over dt}$, where $L(t)$ is a length of the curve.
   In the case of natural parameter $L(s)=s$, i.e. $|\v(t)|={dL(t)\over dt}=1$. We come to the first relation.




   The
  second relation means that value of the speed does not change (see \eqref{diffbytime} and
  \eqref{anglebetweenvelocityandacceleration3}).



\subsection {Curvature. Curvature of curves in $\E^2$ and $\E^3$}



{\footnotesize How to find invariants  of non-parameterised
 curve, i.e. magnitudes which depend on the points of non-parameterised
 curve but which do not depend on parameterisation?



 Answer at the first sight looks very simple: Consider the distinguished
 natural parameterisation $\r=\r(s)$ of the curve.
 Then arbitrary functions on $x^i(s)$ and its derivatives do not depend
 on parameterisation.  But the problem is that it is not easy to calculate
  natural parameter explicitly (See e.g. calculations of natural parameter for
  parabola in the previous subsection).
   So it is preferable to know how
  to construct these magnitudes in arbitrary parameterisation, i.e.
  construct functions $f({dx^i\over dt}, {d^2x^i\over dt^2},\dots)$
  such that they {\it do not depend on parameterisation}.}

\bigskip

We  define now curvature. First formulate reasonable conditions on curvature:


   \begin{itemize}
       \item it has to be a function of the points of the curve
     \item it does not depend on parameterisation

          \item
          curvature of the line must be equal to zero
          \item
          curvature of the circle with radius $R$ must be equal to $1/R$
          \end{itemize}

      We first give definition of curvature
              in natural parameterisation. Then study how to calculate it for a curve in an
               arbitrary parameterisation.


        For a given non-parameterised curve consider natural parameterisation
        $\r=\r(s)$.  We know already that velocity vector has length $1$
        and acceleration vector is orthogonal
         to curve in natural parameterisation (see \eqref{firstpropertyofnaturalparameter}
         and \eqref{secondcondpropertyofnaturalparameter}). It is just normal (centripetal) acceleration.

\medskip


        {\bf Definition}. The curvature of the curve in a given point is equal
       to the modulus (length) of acceleration vector (normal acceleration) in natural parameterisation.
    Namely, let $\r(s)$ be natural
         parameterisation of this curve. Then curvature
         at every  point $\r(s)$ of the curve is equal to
         the length of acceleration vector:
\begin{equation}\label{definitionbadofcurvature}
     k=|\ac(s)|,\qquad  \ac (s)={d^2\r(s)\over ds^2}
\end{equation}

   First check that it corresponds to our intuition (see reasonable conditions
   above)

     It does not depend on parameterisation by definition.

     It is evident that for the line in normal parameterisation
     $x^i(s)=x^i_0+b^is$  ($\sum b^ib^i=1$)  the acceleration is equal to zero.

     Now check that the formula \eqref{definitionbadofcurvature}
     gives a natural answer for circle.

         \noindent For circle
    of radius $R$ in natural parameterisation
               $$\r=\r(s)=(x(s),y(s)),\quad
               {\rm where}\quad x(s)=R\cos {s\over R},\quad y(s)=R\sin {s\over R}$$
(length of the arc of the angle $\theta$ of the
circle is equal to $s=R\theta$.) Then
                    $$
 \ac(s)={d\r^2(s)\over ds^2}=\left( -{1\over R}\cos {s\over R},  -{1\over R}\sin {s\over R}\right)$$
 and  for curvature
\begin{equation}\label{curvature}
  k=|\ac(s)|={1\over R}
\end{equation}
we come to the answer which agrees with our intuition.

\medskip


{ \footnotesize
{\bf Geometrical meaning of curvature}:
%We will consider this question later in more detail. But even now it is easy to
One can see from this example
 that $1\over k$ is {\it just a radius of
 the circle which has second order
 touching to curve}.(See the subsection "Second order contact" (this is not compulsory))
}


% 22 April


%\end{document}

\subsection {Curvature of curve in an arbitrary parameterisation.}

Let curve be given in an arbitrary parameterisation. How to calculate curvature.
One way is to go to natural parameterisation. But in general it is very difficult
(see the example of parabola in the subsection "Natural parameterisation").

We do it in another more elegant way.
\m

{\bf Proposition} {\it Curvature of the curve
in terms of an arbitrary parameterisation $\r=\r(t)$ is given by the formula:
\begin{equation}\label{generalformulaforcurvature2}
    k={{|\ac_{\perp}(t)|\over |\v(t)|^2}}={\hbox{Area of parallelogram $\Pi_{\v,\ac}$
     formed by the vectors $\ac,\v$}\over |\v|^3},
\end{equation}
where $\v(t)={d\r(t)/dt}$ is velocity vector and $\ac_{\perp}(t)$ is normal acceleration.}

\m


{\it Proof of the Proposition}

Prove first that  $k={{|\ac_{\perp}(t)|\over |\v(t)|^2}}$.
Note that in natural parameterisation
speed is equal to $1$ and acceleration is orthogonal to
curve: $\ac=\ac_{\perp}$, $|\v|=1$ (see \eqref{firstpropertyofnaturalparameter},
\eqref{secondcondpropertyofnaturalparameter}). Hence in natural parameterisation
 the ration ${{|\ac_{\perp}|\over |\v|^2}}$ is equal just to modulus of acceleration vector, i.e. to the curvature
\eqref{definitionbadofcurvature}.  On the other hand according to the
observation \eqref{observation} (see the end of the
subsection "Velocity and acceleration vectors")
the ratio
                    $
  {{|\ac_{\perp}|\over |\v|^2}}={{|\ac_{\perp}|\over (\v,\v)}}
                    $
{\it does not} depend on parameterisation.  Hence curvature is defined by the formula
$k={{|\ac_{\perp}(t)|\over |\v(t)|^2}}$ in an arbitrary parameterisation.





Advantage of the formula $ k={{|\ac_{\perp}(t)|\over |\v(t)|^2}}$
is that it is given in an arbitrary parameterisation.
Disadvantage of this formula is that we still do not know how to calculate $\ac_{\perp}(t)$.
Do the next step. Note that
       $$
       {|\ac_{\perp}(t)|\over |\v|^2}={|\ac_{\perp}(t)|\cdot|\v|\over |\v|^3}=
       $$
\begin{equation}\label{modulusofcentripetal}
       {|\ac_{\perp}(t)|\over |\v|^2}={|\ac_{\perp}(t)|\cdot|\v|\over |\v|^3}=
{\hbox{Area of parallelogram $\Pi_{\v,\ac}$ formed by the vectors $\ac,\v$}\over |\v|^3}\,.
\end{equation}
 Thus we proved formula \eqref{generalformulaforcurvature2}.
  We express the curvature in terms of area of the parallelogram $\Pi_{\v,\ac}$
 in an arbitrary parameterisation.  We have that under an arbitrary change of parameterisation $t=t(\tau)$
               \begin{equation}
               \begin{matrix}
               \v \mapsto t_\tau \v \cr
               \ac_{\perp}\mapsto t_\tau ^2 \ac_{\perp}\cr
          \hbox{Area of parallelogram} \Pi_{\v,\ac}\mapsto
          t_\tau^3 \hbox{Area of parallelogram} \Pi_{\v,\ac}\cr
          \end{matrix}
            \end{equation}
 Numerator and denominator of the fraction,
 which is in the RHS of the equation \eqref{modulusofcentripetal}
 are multiplied on $t_\tau^3$. The fraction, i.e. curvature does not change.

 We know how to calculate  area of parallelogram
spanned by the vectors $\ac,\v$. In particularly it is easy to do for $\E^3$ and  $\E^2$,
where this is just the magnitude of vector product
(see the formulae for vector product in the subsections 1.11.1 and 1.11. 2):
\begin{equation}\label{Gebformforcurvn=3}
   k={\hbox{Area of parallelogram $\Pi_{\v,\ac}$ formed by the vectors $\ac,\v$}\over |\v|^3}
   ={{|\v(t)\times \ac(t)|\over |\v(t)|^3}}\,,
\end{equation}
if curve is in $\E^3$.


In the case if curve is in $\E^2$ then formula for curvature is
\begin{equation*}\label{Gebformforcurvn=21}
    k={{|\v(t)\times \ac(t)|\over |\v(t)|^3}}={|v_xa_y-v_ya_x|\over (v_x^2+v_y^2)^{3\over 2}}=
    {|v_xa_y-v_ya_x|\over (v_x^2+v_y^2)^{3\over 2}}=
\end{equation*}
\begin{equation}\label{Gebformforcurvn=22}
    ={|x_{t}y_{tt}-y_{t}x_{tt}|\over (x^2_{t}+y^2_{t})^{3\over 2}} \qquad (\hbox{if curve is in $\E^2$})
\end{equation}

This is workable formula.




\smallskip

   {\footnotesize In general case if curve is in $\E^n$   then to  calculate the area $S$ of parallelogram
    note that $S=|\v||\ac||\sin \theta|$ where $|\v||\ac|\cos \theta=(\v,\ac)$.
    Hence $S=|\v||\ac|\sqrt{1-\cos^2\theta}=\sqrt {\v^2\ac^2-\left(\v\cdot \ac\right)^2}$ and curvature is equal to
\begin{equation}\label{curvaturedefinitionworkableingeneralcase}
k=
   {\hbox {Area of parallelogram formed by the vectors $\v$ and $\ac$}\over
     \hbox {Cube of the speed }}
= {\sqrt {\v^2\ac^2-\left(\v\cdot \ac\right)^2}
  \over |\v|^3
  }
\end{equation}





{\bf Remark }.
 Of course one  can come to formulae \eqref{curvaturedefinitionworkableingeneralcase},
 \eqref{Gebformforcurvn=3} and \eqref{Gebformforcurvn=21}
by "brute force" making straightforward attack. Instead considering explicitly natural parameterisation
of the curve we
just try to rewrite the formula in definition \eqref{definitionbadofcurvature}
in arbitrary parameterisation using chain rule. The calculations are not transparent.
{\footnotesize Try to do it.}

}
\m

Consider examples of calculating curvature for curves in $\E^2$ and $\E^3$.

\m

{\bf Example}   Consider a curve $C_f\colon $
$\r(t)\colon\quad \begin{cases}x=t\\ y=f(t)\end{cases}$
(It is parameterisationn of graph of the  function $f=f(x)$).
Calculate curvature of this curve.
We see that $\v(t)=\begin{pmatrix} 1\\ f'(t)\end{pmatrix}$,
$\ac(t)=\begin{pmatrix} 0\\ f''(t)\end{pmatrix}$ and
we have for the curvature that
\begin{equation}\label{curvatureofgraph}
    k={|x_{t}y_{tt}-y_{t}x_{tt}|\over (x^2_{t}+y^2_{t})^{3\over 2}}=
    k={|f''(t)|\over (1+f'(t)^2)^{3\over 2}}
\end{equation}



\m




{\bf Example}. Consider circle of the radius $R$, $x^2+y^2=R^2$. Take any parameterisation, e.g.
$x=R\cos t,y=R\sin t$. Then $\v=(-R\cos t,R\sin t)$, $\ac=(-R\sin t,-R\cos t)$.
Applying  the formula \eqref{Gebformforcurvn=22} we come to
      $$
    k={|x_{t}y_{tt}-y_{t}x_{tt}|\over (x^2_{t}+y^2_{t})^{3\over 2}}=
    {|R^2\cos^2 t+R^2\sin^2 t|\over (R^2\cos^2 t+R^2\sin^2 t)^{3\over 2}}={R^2\over R^3}={1\over R}
      $$

\m
:

{\bf Example} Consider ellipse
  $\r(t)\colon \begin{cases}x=a\cos t\\ y=b\sin t\end{cases}$,
   $0\leq t< 2\pi$.
Then $\v(t)=\begin{pmatrix}-a\sin t\\ b\cos t\end{pmatrix}$,
$\ac(t)=\begin{pmatrix}-a\cos t\\ -b\sin t\end{pmatrix}$ and for curvature we have
\begin{equation}\label{curvatureofellipse}
k={|x_{t}y_{tt}-y_{t}x_{tt}|\over (x^2_{t}+y^2_{t})^{3\over 2}}=
k={|ab\sin^2 t+ab\cos^2 t|\over (a^2\sin^2 t+b^2\cos^2 t)^{3\over 2}}=
{ab\over (a^2\sin^2 t+b^2\cos^2 t)^{3\over 2}}\,.
\end{equation}
In particular we see that at the points $(\pm a, 0)$ ($t=0,\pi$) curvature is equal to
$k={ab\over b^3}={a\over b^2}$ and at the points  $(0, \pm b)$ ($t=\pm {\pi\over 2}$)
 curvature is equal to
$k={ab\over a^3}={b\over a^2}$.



\m

{\bf Example}  Consider helix
                 \begin{equation}\label{helix4}
                 \r(t)\colon\quad
                 \begin{cases}
                 x=R\cos t\cr
                 y=R\sin t\cr
                 z= ct
                 \end{cases}
                 \end{equation}
We see that velocity and acceleration vectors are equal to
\begin{equation*}
\v(t)={d\r(t)\over dt}=
\begin{pmatrix}-R\sin t\cr R\cos t\cr c\cr\end{pmatrix}\,,\quad
\ac(t)={d\v(t)\over dt}=
\begin{pmatrix}-R\cos t\cr R\sin t\cr 0\cr\end{pmatrix}
\end{equation*}

One can calculate curvature traightworwardly
sing the formula \eqref{Gebformforcurvn=3}:
$$
k={{|\v(t)\times \ac(t)|\over |\v(t)|^3}}=
{\left|\det\begin{pmatrix} &{\bf i} &{\bf j} &{\bf k}\cr
&-R\sin t & R\cos t & c \cr
& -R\cos t & -R\sin t &0\end{pmatrix}\right|\over (R^2+c^2)^{3\over 2}}=
$$
$$
{\sqrt {c^2R^2+R^4}\over \left(\sqrt {R^2+c^2}\right)^3}=
{R\sqrt {c^2+R^2}\over \left(\sqrt {R^2+c^2}\right)^3}={R\over R^2+c^2}\,.
$$
We come to the beautiful answer using `brute force'. Try to come to this answer
in a nicer way.
Speed is constant: $|\v(t)|=\sqrt {R^2\Omega^2+c^2t^2}$. Velocity vector
is orthogonal to acceleration vector. This can be checked directly, but
 it is evident without 
any calculations since
speed is constant.  (In fact acceleration vector is orthogonal not only to 
the velocity vector
but to an arbitrary vector at the surface of the cylinder  $x^2+y^2=R^2$
since it is orthogonal to vertical vectors and to velocity vector. )

How to calculate curvature. We do not need to  use 
the formula \eqref{Gebformforcurvn=3}, since the area of parallelogram is equal
just to the product of speed and length of acceleration vectors,
or in the other way we may just use the formula
 $k={|\ac_{\perp}|\over \v^2|}$. We have
         $$
    k={|\ac_{\perp}|\over \v^2|}={|\ac|\over \v^2}={R\Omega^2\over R^2\Omega^2+c^2t^2}
         $$
 since $|\ac|=R\Omega^2$
Notice that in this formula curvature tends to $1\over R$ if $c\to 0$
(in this case helix tends to the circle) and curvature $k$ tends to $0$
if $\Omega\to 0$ (in this case) helix tends to straight line.

It is useful to consider helix in parameterisation \eqref{helix4}
as a ``point'' moving
 with constant angular velocity $\Omega$ along the circle of radius $R$
and moving  in along $z$ axis with constant velocity $c$.

  See also examples in Homework 8.





% 16 April

%\end{document}


\section {Surfaces in $\E^3$. Curvatures and Shape operator.}







  In this section we study surfaces in $\E^3$. One can define surfaces by equation
  $F(x,y,z)=0$ or by parametric equation
             \begin{equation}\label{generlalsurface}
    \r(u,v)\colon \begin{cases}
    x=x(u,v)\cr
    y=y(u,v)\cr
    z=z(u,v)\cr
    \end{cases}\,,
  \end{equation}
\m

{\bf Example} the equation $x^2+y^2=R^2$ defines cylinder (cylindrical surface). $z$-axis
is the axis of this cylinder, $R$ is radius of this cylinder.
One can define this cylinder by the parametric equation
  \begin{equation}\label{cylinder1}
    \r(\varphi,h)\colon \begin{cases}
    x=R\cos\varphi\cr
    y=R\sin\varphi\cr
  z=h\cr
  \end{cases}\,,
  \end{equation}
  where $\varphi$ is the angle $0\leq \varphi<2\pi$ and $-\infty<h<\infty$ takes arbitrary real values.




{\bf Example }
sphere $x^2+y^2+z^2=R^2$:
\begin{equation}\label{sphere}
    \r(\theta,\varphi)\colon \begin{cases}
    x=R\sin\theta\cos\varphi\cr
    y=R\sin\theta\sin\varphi\cr
     z=R\cos\theta\end{cases}\,,
     0\leq \theta\leq \pi,\,\, 0\leq \varphi\leq 2\pi
  \end{equation}

%\end{document} % 28 March

\m
{\bf Example }
cone $k^2x^2+k^2y^2-z^2=0$:
\begin{equation}\label{sphere}
    \r(h,\varphi)\colon \begin{cases}
    x=kh\cos\varphi\cr
    y=kh\sin\varphi\cr
     z=h\end{cases}\,,
       -\infty<h<\infty\,\,\,0\leq \varphi\leq 2\pi
  \end{equation}

\m
{\bf Example }
saddle  $z-axy=0$:
\begin{equation}\label{saddle1}
    \r(h,\varphi)\colon \begin{cases}
    x=u\cos\varphi\cr
    y=v\sin\varphi\cr
     z=auv\end{cases}\,,
     -\infty<u,v<\infty
  \end{equation}


\m

{\bf Example} graph of the surface $z=F(x,y)$:
\begin{equation}\label{graphofthesurface}
    \r(u,v)\colon \begin{cases}
    x=u\cr
    y=v\cr
     z=F(u,v)\end{cases}\,,
      -\infty<u<\infty, \,-\infty<v<\infty
  \end{equation}

Consider an example when $F=u^2-v^2$ we come to the surface:
\begin{equation}\label{saddle2}
    \r(u,v)\colon \begin{cases}
    x=u\cr
    y=v\cr
     z=u^2-v^2\end{cases}\,,
      -\infty<u<\infty, \,-\infty<v<\infty
  \end{equation}
Do you see that it is a saddle?  Yes it is.  Rotate the space on the angle $\pi\over 4$ with respect to
$z$ axis: $x\to {x-y\over \sqrt 2}$, $y\to {x+y\over \sqrt 2}$.
Then $2xy\to x^2-y^2$. We see that surface \eqref{saddle1} with parameter $a=2$ is the surface
\eqref{saddle2} after rotation on the angle $\pi\over 4$.





\subsection {Coordinate basis, tangent plane to the surface.}

  Coordinate basis vectors are $\r_u=\p_u,\r_v=\p_v$. At the any point $\pt$, $\pt=\r(u,v)$
  these vectors  span
  the plane, (two-dimensional linear space) $T_\pt M$ in three dimensional vector space $T_\pt E^3$.
  \begin{equation}\label{tangentplane}
T_\pt M=\{\lambda \r_u+\mu\r_v, \lambda,\mu\in \R\},\qquad  T_\pt \,\hbox{subspace in}\, T_\pt E^3
  \end{equation}


\noindent E.g. consider the point $\pt=(R,0,0)$ on the  cylinder \eqref{cylinder1}.
Then $\pt=\r(\varphi,h)$ for $\varphi=0,h=0$. Coordinate basis vectors  are
\begin{equation}\label{cylinderbasicvectors1}
    \r_\varphi=\begin{pmatrix}-R\sin\varphi\\ R\cos \varphi\\ 0\end{pmatrix},\qquad
\r_h=\begin{pmatrix} 0\\ 0\\ 1
\end{pmatrix}
\end{equation}
or in other notations
\begin{equation}\label{cylinderbasicvectors2}
\r_\varphi=-R\sin\varphi\p_x+R\cos \varphi\p_y,\qquad
\r_h=\p_z
\end{equation}
At the point $\pt=(R,0,0)$ they are
are equal to the vectors $\p_y$ and $\p_z$ respectively
attached at this point.
Tangent plane at the point $\pt$ is the plane passing through the point $\pt$ spanned by the vectors  $\p_y$ and $\p_z$.


% 28 April

%\end{document}



     \subsection {Curves on surfaces. Length of the curve. Internal and external point of the view.
     First Quadratic Form}

    Let $M\colon \r=\r(u,v)$ be a surface and $C$ curve on this surface, i.e.
    $C\colon \r(t)=\r(u(t,v(t)))$.

    Consider an arbitrary point  $\pt=\r(t)=\r(u(t),v(t))$ at this curve.

{\footnotesize
  \begin {itemize}
\item  $T_\pt E^3$---three-dimensional tangent space to the point $\pt$,

\item   $T_\pt M$---two dimensional linear space tangent to the surface at the point $\pt$,
spanned by the tangent vectors $\p_u,\p_v$

\item   $T_\pt M$---one dimensional linear space tangent to the curve at the point $\pt$
   spanned by the velocity vector   $\v(t)$.
    \begin{equation}\label{expansionofvelocityvector1}
    \v(t)={d\r(u(t),v(t))\over dt}=u_t{\p \r\over \p u}+v_t{\p \r\over \p v}=u_t\r_u+v_t\r_v
   \end{equation}
{\footnotesize These tangent spaces form flag of subspaces $T_\pt  C < T_\pt M< T_\pt E^3$.}

 \end {itemize}
}


   How to calculate the length of the arc of the curve:
                       $$
      C\colon \r(t)=\r(u(t,v(t)))=
      \begin{cases}
      x=x(u(t),v(t))\\
      y=y(u(t),v(t))\\
      z=z(u(t),v(t))\\
      \end{cases}
      \qquad t_1\leq t_2.
                    $$
      External and internal observer do it in different ways.
   External observer just looks at the curve as the curve in ambient space. He uses the formula
\eqref{simpleformulaforlength}:
\begin{equation}\label{lengthforexternalobserver}
    L=\hbox{Length of the curve $L$}=\int_a^b |\v(t)|dt=
\int_a^b\sqrt {\left({dx(t)\over dt}\right)^2+
                  \left({dy(t)\over dt}\right)^2+
                \left({dz(t)\over dt}\right)^2}dt\,.
\end{equation}


What about internal observer?

 Internal observer will perform calculations in coordinates $u,v$. We have $|\v(t)|=\sqrt{(\v,\v)}$.
 We have
         $$
       \v={d\r(t)\over dt}={d\r(u(t),v(t))\over dt}=
   \dot u{\p r(u,v)\over \p u}+\dot v{\p r(u,v)\over \p v}=
  \dot u \r_u+\dot v \r_v\,.
         $$
Hence the scalar product
             $$
  (\v,\v)=(u_t \r_u+v_t \r_v,u_t \r_u+v_t \r_v)=u_t^2 (\r_u,\r_u)+2u_tv_t(\r_u,\r_v)+v_t^2(v_t,v_t)\,.
             $$
 To understand how internal observer can calculate the length of the curve we have to introduce
              \begin{equation}\label{intrFQF}
    G_{uu}=(\r_u,\r_u),\,\,\,G_{uv}=(\r_u,\r_v)\,\,\,
   G_{vu}=(\r_v,\r_u),\,\,G_{vv}=(\r_v,\r_v)
\end{equation}
Of course $G_{uv}=G_{vu}$.  We see that internal observer calculates the length of the curve
 using time derivatives $u_t,v_t$ of internal coordinates $u,v$ and coefficients \eqref{intrFQF}:
 \begin{equation}\label{formforspeedusing FQF}
    (\v,\v)=u_t^2 (\r_u,\r_u)+2u_tv_t(\r_u,\r_v)+v_t^2(v_t,v_t)=G_{11} u_t^2+2G_{12}u_tv_t+G_{22}v_t^2.
\end{equation}

\m

We come to conception of {\it first quadratic form.}


\m

  {\bf Definition}  First quadratic form defines length of the tangent vector  to the surface
   in internal coordinates
    and length of the curves on the surface.


    The first quadratic form
   at the point $\r=\r(u,v)$ is defined by symmetric matrix:
\begin{equation}\label{firstquadraticform}
\begin{pmatrix}
   G_{uu} & G_{uv} \\
   G_{vu}& G_{vv}
   \end{pmatrix}=
   \begin{pmatrix}
   (\r_u,\r_u) & (\r_u,\r_v) \\
   (\r_u,\r_v) & (\r_v,\r_v)
   \end{pmatrix},
\end{equation}
where $(\,,\,)$ is a scalar product.

\m

E.g. calculate the first  quadratic form for the cylinder \eqref{cylinder1}.
Using \eqref{cylinderbasicvectors1}, \eqref{cylinderbasicvectors2} we come to
\begin{equation}\label{cylinderquadraticform1}
\begin{pmatrix}
   G_{hh} & G_{h\varphi} \\
   G_{\varphi h}& G_{\varphi\varphi}
   \end{pmatrix}=
   \begin{pmatrix}
   (\r_h,\r_h) & (\r_h,\r_\varphi) \\
   (\r_\varphi,\r_h) & (\r_\varphi,\r_\varphi)
   \end{pmatrix}=\begin{pmatrix}
   1 & 0 \\
   0& R^2
   \end{pmatrix}
\end{equation}
(See this example and other examples in Homework 9)


Let $\X=a\r_u+b\r_v$ be a vector tangent to the surface $M$ at the point $\r(u,v)$.
Then the length of this vector is defined by  the scalar product $(\bf X,\bf X)$:
\begin{equation}\label{lengthforexternalobserver}
 |\X|^2=(\X,\X)=(a\r_u+b\r_v,a\r_u+b\r_v)=a^2(\r_u,\r_u)+
   2ab(\r_u,\r_v)+b^2(\r_v,\r_v)
\end{equation}
It is just equal to the value of the first quadratic form on this tangent vector:
\begin{equation}\label{thevalueoffirstquadraticform}
  (\X,\X)=G(\X,\X)=
  \begin{pmatrix}
   a, &b
  \end{pmatrix}
   \cdot
   \begin{pmatrix}
   G_{uu} & G_{uv} \\
   G_{vu}& G_{vv} \\
   \end{pmatrix}\cdot
  \begin{pmatrix}
   a\\ b\\
  \end{pmatrix}=
  G_{uu}a^2+2G_{uv}ab+G_{vv}b^2
\end{equation}

External observer (person living in ambient space $\E^3$) calculate the length
of the tangent vector  using formula
\eqref{lengthforexternalobserver}. An ant living on the surface (internal observer)
calculate length of this vector in internal coordinates using formula
\eqref{thevalueoffirstquadraticform}. External observer deals
with external coordinates of the vector, ant on the surface with internal coordinates.

\m


{\footnotesize If ${\X,\Y}$ are two tangent vectors in the tangent plane $T_pC$ then $G(\X,\Y)$
  at the point $p$ is equal to scalar product of vectors $\X,\Y$:
$(\X,\Y)=(X^1\r_1+X^2\r_2, Y^1\r_1+Y^2\r_2)=$
$
X^1 (\r_1,\r_1)Y^1+X^1 (\r_1,\r_2)Y^2+X^2 (\r_2,\r_1)Y^1+X^2 (\r_2,\r_2)Y^2=
$
$X^\a (\r_\a,\r_\beta)Y^\beta=
X^\a G_{\a\beta}Y^\beta=G(\X,\Y)
$. We identify quadratic forms and  corresponding symmetric
bilinear forms. {Bilinear symmetric form $B(\X,\Y)=B(\Y,\X)$
defines quadratic form $Q(\X)=B(\X,\X)$. Quadratic form satisfies the condition
$Q(\lambda\X)=\lambda^2 Q(\X)$  and so called parallelogram condition
\begin{equation}\label{parallelogramcondition}
    Q(\X+\Y)+Q(\X-\Y)=2Q(\X)+2Q(\Y)
\end{equation}
}
}

%\end{document}
  \centerline {\it First quadratic form and length of the curve}

  Let $\r(t)=\r(u(t),v(t))$ $a\leq t\leq b$ be a curve on the surface.

  The first quadratic form  measures the length of velocity vector at every
  point of this curve. Write down again the formula for length of the curve in internal coordinates
  using First Quadratic form (compare with \eqref{formforspeedusing FQF}).

  Velocity of this curve at the  point $\r(u(t),v(t))$ is equal to
  $\v={d\r(t)\over dt}=
    u_t\r_u+v_t\r_v$.
The length of the curve is equal to
\begin{equation}\label{lengthofthecurve}
L=\int_a^b |\v(t)|d t= \int_a^b\sqrt
{\left(\v(t),\v(t)\right)}d t = \int_a^b\sqrt
{\left(u_t\r_u+v_t\r_v,u_t\r_u+v_t\r_v\right)}dt=
\end{equation}
           $$
\int_a^b\sqrt {(\r_u,\r_u)u_t^2+2(\r_u,\r_v)u_tv_t+(\r_v,\r_v)v_t^2}d\tau=
           $$
\begin{equation}\label{lengthofthecurve2}
  \int_a^b\sqrt {G_{11} u_t^2+2G_{12}u_tv_t+G_{22}v_t^2}dt\,.
\end{equation}

An external observer will calculate the length of the curve using
\eqref{lengthforexternalobserver}. An ant living on the surface calculate
length of the curve via first quadratic form using
\eqref{lengthofthecurve2}: first quadratic form defines Riemannian
metric on the surface:
\begin{equation}\label{riem}
  ds^2=G_{11}du^2+2G_{12}dudv+G_{22}dv^2
\end{equation}

{\bf Example} Consider the curve
                     $$
                  \r(t)\quad
                  \begin{cases}
                  x=R\cos t\cr
                  y=R\sin t\cr
                   z=vt
                   \end{cases}, \quad 0\leq t\leq 1
                       $$
                        on the cylinder \eqref{cylinder1}
(helix). The coordinates of this curve on the cylinder (internal coordinates) are
                      $$
         \begin{cases}\varphi(t)=t\cr h(t)=vt\end{cases}.
                      $$
To calculate the length of this curve the external observer will perform the calculations
      $$
      L=\int_0^1\sqrt {x_t^2+y_t^2+z_t^2}dt=
      \int_0^1\sqrt {R^2\sin^2 t+R^2\cos^2 t+v^2}dt=\int_0^1\sqrt {R^2+v^2}dt=\sqrt {R^2+v^2}.
           $$

      An internal observer ("ant") uses quadratic form  \eqref{cylinderquadraticform1} and perform the following calculations:
                          $$
      L=\int_0^1 \sqrt {G_{11}\varphi_t^2+2G_{12}\varphi_t h_t+G_{22}h_t^2}dt
      =\int_0^1 \sqrt {R^2\varphi_t^2+h_t^2}dt=\int_0^1 \sqrt {R^2+v^2}dt=\sqrt {R^2+v^2}\,.
                  $$
       The answer will be the same. (See this and other examples in Homework 8).
\smallskip


\subsection {Unit normal vector to surface}

We define unit normal vector field for surfaces in $\E^3$.

Consider vector field defined on the points of surface.

{\bf Definition} Let $M\colon \,\,\r=\r(u,v)$ be a surface in $\E^3$.
 We say that vector $\n(u,v)$ is {\it normal unit vector } at the point $\pt=\r(u,v)$ of the
 surface $M$
if it has unit length $|\n|=1$, and it is orthogonal to the surface,
i.e. it is orthogonal to the tangent plane  $T_\pt M$. This means that
it is orthogonal to any tangent vector ${\bf \xi}\in T_p M$, i.e. it is orthogonal to the
coordinate vectors $\r_u=\p_u$, $\r_v=\p_v$ at the point $\pt$.
\begin{equation}\label{normalvectorconditions}
    \n\colon (\n,\r_u)=(\n,\r_v)=0, (\n,\n)=1\,.
\end{equation}
  Write down this equation in components:

  If surface is given by equation
  $\r(u,v)\colon \begin{cases}x=x(u,v)\\y=y(u,v)\\z=z(u,v)\end{cases} $
  then
  \begin{equation*}
    \r_u=\begin{pmatrix}x_u\\y_u\\z_u\\\end{pmatrix},\,\,
       \r_v=\begin{pmatrix}x_v\\y_v\\z_v\\\end{pmatrix},\,\,
  \end{equation*}
and  $\n=\begin{pmatrix}n_x\\n_y\\n_z\\\end{pmatrix}$
is unit normal vector.  Then writing the previous conditions in components we come to
\begin{equation*}
 (\n,\r_u)=n_xx_u+n_yy_u+n_zz_v=0,\,\,
 (\n,\r_v)=n_xx_v+n_yy_v+n_zz_v=0,\, (\n,\n)=n_x^2+n_y^2+n_z^2=1
  \end{equation*}
Normal unit vector is defined up to a sign. At any point there are two normal unit vetors:
 the transformation $\n\to-\n$ transforms normal unit vector
to normal unit vector.


  Vector field defined at the points of the surface is called normal unit vector field if any vector
  is normal unit vector.

\m

In simple cases one can guess how to find unit normal vector field using geometrical intuition
and just check that conditions above are satisfied. E.g.
for sphere \eqref{sphere} $\r$ is orthogonal to the surface, hence
   $$
  \n(\theta,\varphi)={\r(\theta,\varphi)\over R}=
  \pm
\begin{pmatrix} \sin\theta\cos\varphi\cr
\sin\theta\sin\varphi\cr
  \cos\theta\cr
  \end{pmatrix}
   $$
For cylinder \eqref{cylinder1}
it is easy to see that  at any point $(\varphi,h)$
\eqref{cylinder1},
$\r\colon x=R\cos\varphi,y=R\sin\varphi,z=h$,
a normal unit vector  is equal to
\begin{equation}\label{cylinderunitnormal}
    \n(\varphi,h)=\pm\begin{pmatrix} \cos\varphi\\\sin\varphi\\0\\\end{pmatrix}
\end{equation}
Indeed it is easy to see that the conditions  \eqref{normalvectorconditions} are satisfied.




In general case one can define $\n(u,v)$ in two steps using vector product formula:
\begin{equation}\label{normalusingvectorporduct}
   \n(u,v)={\N(u,v)\over |\N(u,v)|}\,\,\,\,{\rm where}\,\, \,\,\N=\r_u\times \r_v
\end{equation}
Indeed by definition of vector product vector field $\N(u,v)$
is orthogonal to $\r_u$
and $\r_v$, i.e. it is orthogonal to the surface.
 Dividing $\N$ on the length we come to unit normal vector field
 $\n(u,v)$ at the point $\r(u,v)$.
  (See other examples of calculating normal unit vector in the Homework 9)



%\end{document}  %26 April

{\footnotesize

   \subsection { $^\dagger$ Curves on surfaces---normal acceleration and normal curvature}

  We know already how to measure the length of the curve belonging to the given surface. What about curvature?
   Answering this question we will be able to study curvature of the surface.



   Before we have to introduce {normal acceleration and normal curvature} for curves on the surfaces.




   We know that acceleration vector $\ac$ in general is not tangent to the curve.
   Recall that when studying curvature we consider decomposition of acceleration
   vector on tangential component and the component which is perpendicular to velocity vector:
     $\ac=\ac_{tang}+\ac_{\perp}$. The curvature of curve is nothing but the magnitude of
   normal acceleration $\ac_\perp$ of particle which moves along the curve with  unit speed:
   $k={|\ac_{\perp}|\over |\v|}$.



 Now we consider {\it normal acceleration of the curve on the surface}.


Let  $M\colon \,\, \r=\r(u,v)$ be a surface and $C\colon u=u(t),v=v(t)$, i.e.
   $\r(t)=\r(u(t),v(t))$, be curve on the surface $M$.
  Consider an arbitrary point $\pt=\r(t)=\r(u(t),v(t))$ on this curve
  and  velocity and acceleration vectors  $\v={d\r(t)\over dt}$, $\ac={d^2\r(t)\over dt^2}$ at this point.


 {\bf Definition}
  The component of acceleration vector of the curve on the surface orthogonal to the surface
  is called a normal acceleration of curve on the surface.
 If $\ac$ is acceleration vector  then
  \begin{equation}\label{accelerationnormaltosurface}
    \ac=\ac_{||}+\ac_{n},\,\,,
\end{equation}
where the vector  $\ac_{||}$ is tangent to the surface and the vector $\ac_n$ is orthogonal (perpendicular)
to the surface.  Calculate vector $\ac_n$.

If $\n$ is a normal unit vector to the surface, then vector $\ac_n$ is collinear (proportional) to the vector $\n$
and vector $\ac_{||}$ is orthogonal to this vector:
             $$
            \ac_n=a_n\n, \,\, (\n,\ac_{||})=0\,.
             $$
Take a scalar product of left and right hand sides of the formula \eqref{accelerationnormaltosurface} on the vector
$\n$. We come to:
             $$
   (\n,\ac)=(\n,\ac_{||}+\ac_{n})=(\n,\ac_{||})+(\n,\ac_{n})=0+a_n(\n,\n)=a_n.
             $$
Hence we come to
\begin{equation}\label{formulafornormalcurvature}
    \ac=a_n\n=(\n,\ac)\n\,.
\end{equation}
\m

  Avoid confusion! The normal acceleration vector $\ac_n$ of the curve on the surface is orthogonal to the surface.
    The normal acceleration vector of the curve in $\E^3$ $\ac_\perp$ is orthogonla to the velocity vector of the curve.

   Now we are ready give a definition of normal curvature of the curve on the surface.


  {\bf Definition}  Let $C$ be a curve on the surface $M$. Let $\v$, $\ac$ be velocity and acceleration vectors
  at the given point of this curve and $\n$ be normal unit vector at this point.
  Then
  \begin{equation}\label{normalcurvaturedef}
        \kappa_{n}={a_n\over |\v|^2}={(\n,\ac)\over (\v,\v)}
\end{equation}
is called {\it normal curvature of the curve} $C$ on the surface $M$ at the point $\pt$.
  Or in other words \begin{equation}\label{normalcurvaturedef2}
  |\k_n|={|\ac_n|\over (\v,\v)},
\end{equation}
i.e. up to a sign normal curvature is equal to modulus of normal acceleration divided on the square of speed
  (Compare with formula \eqref{modulusofcentripetal} for usual curvature.)



  {\bf Remark} Avoid confusion: We know that usual curvature $k$ of the curve is defined by the formula
   $k={|\ac_\perp|\over |\v|^2}$, where $\ac_\perp$
   is a magnitude of the acceleration vector orthogonal to the curve (see the formula \eqref{modulusofcentripetal}).
   Normal curvature of the curve on the surface is defined by the analogous formula
   bunt in terms of normal acceleration $\ac_n$ which is orthogonal to the surface, not to the curve!

  In fact one can see that $|\ac_\perp|\leq |\ac_n|$, i.e. modulus of the normal curvature is less or equal to
  the usual curvature of the curve. (See in details the Appendix ''Relations between usual curvature, normal curvature
  and geodesic curvature'')}



\m







\subsection {Shape operator on the surface}

\def\xip {{\boldsymbol \xi}}
{\footnotesize  Let $M\colon \r=\r(u,v)$ be a surface and ${\bf L}(u,v)$ be an arbitrary (not necessarily unit normal)
   vector field at the points of the surface  $M$.
We define at every point $\pt=\r(u,v)$ a linear operator $K_L$ acting on the vectors tangent to the surface $M$
such that its value is equal to the derivative of vector field ${\bf L}(u,v)$ along vector $\xip$
\begin{equation}\label{preshapeoperator}
K_{\bf L}\colon\,\, \xip\in T_\pt M \mapsto
K_{\bf L}(\xip)=\p_\xip L=
\xi_u{\p {\bf L}(u,v)\over \p u}+\xi_v{\p {\bf L}(u,v)\over \p v}\,,
\end{equation}
$\xi_u,\xi_v$ are components of vector $\xip$
\begin{equation}\label{expansioofxip2}
    \xip=\xi_u\r_u+\xi_v\r_v
  \end{equation}
The vector $K_{\bf L}\xip\in T_\pt\E^3$ in general is not a vector tangent to the surface $C$
and $K_{\bf L}$ is linear operator from the space $T_\pt M$ in the space $T_\pt  \E^3$
of all vectors in $\E^3$ attached at the point $\pt$

It turns out that in the case if vector field ${\bf L}(u,v)$ is {\it a unit normal vector field} then
operator $K_{\bf L}$ takes values in vectors tangent to $M$ and it is very important geometric properties.
}


\bigskip
{\bf Definition-Proposition} Let $\n(u,v)$ be a unit normal vector field to the surface  $M$.
Then operator
\begin{equation}\label{defofshapeoperator}
    S\colon\,\,S(\X)=\p_\X (-\n)=
-X_u{\p { \n}(u,v)\over \p u}-X_v{\p {\n}(u,v)\over \p v}
\end{equation}


 maps tangent vectors to the tangent vectors:
\begin{equation}\label{propertyofshapeoperator1}
  S\colon T_\pt M\to T_\pt M\,\, \hbox{for every}\,\, 
 \X=X_u\r_u+X_v\r_v\in T_\pt M,
\qquad S(\X)\in T_\pt M
\end{equation}


This operator is called  {\it shape operator}.






\m
{\footnotesize
{\bf Remark} The sign $"-"$ seems to be senseless: if $\n$ is unit normal vector field then $-\n$
is normal vector field too. Later we will see why it is convenient (see the proof of the Proposition below).
}

   Show that  property  \eqref{propertyofshapeoperator1} is 
  indeed obeyed, i.e.
   vector $\X'=S(\X)$ is tangent to surface.  Consider derivative of scalar product $(\n,\n)$
with respect to the vector field $\X$. We have that $(\n,\n)=1$. Hence
                 $$
   \p_\X(\n,\n)=0=\p_\X (\n,\n)=(\p_\X \n,\n)+(\n,\p_\X \n)=2(\p_\X \n,\n)\,.
              $$
   Hence $(\p_\X \n,\n)=-(S(\X),\n)=-(\X',\n)=0$, i.e. vector
$\p_\X \n=-\X'$ is orthogonal to the vector $\n$. This means that vector $\X'$ is
tangent to the surface.

Write down the action of shape operator on coordinate basis $\r_u=\p_u$, $\p_v=\r_v$ at
the given point $\pt$:
\begin{equation*}
S(\r_u)=-\p_{\r_u}\n(u,v)=-{\p \n(u,v)\over \p u},\quad
S(\r_v)=-\p_{\r_v}\n(u,v)=-{\p \n(u,v)\over \p v}
\end{equation*}


Since the shape operator transforms tangent vectors to tangent vectors, then
\begin{equation*}\label{propertyofshapeoperator-1}
\begin {matrix}
S(\r_u)=-{\p \n(u,v)\over \p u}=a\,\r_u+c \r_v \cr
S(\r_v)=-{\p \n(u,v)\over \p v}=b\r_u+d\r_v\cr
\end{matrix},\quad
  \end{equation*}
  i.e.
  \begin{equation}\label{propertyofshapeoperator2}
                       S
   =\begin{pmatrix}
   a &b\cr c& d\cr
   \end{pmatrix}
      \hbox {in the coordinate basis $\r_u,\r_v$}
\end{equation}
Examples of shape operator see
in the subsection above (Shape operator, Gaussian and mean curvature for sphere and cylinder)
and in the Homework 9.

{\bf Remark}. Shape operator as well as normal unit vector is defined up to a sign:
                $$
   \n(u,v)\to -\n(u,v), \quad {\rm then}\quad  S\to -S\,.
                $$


{\footnotesize
\bigskip We show now that normal acceleration of a curve on the surface and
normal curvature are expressed in terms
of shape operator.

Let $C\colon \r(t)$ be a curve on the surface $M$,
$\r(t)=\r(u(t),v(t))$. Let $\v=\v(t)={d\r(t)\over dt}$, $\ac=\ac(t)={d^2r(t)\over dt^2}$
  be velocity and acceleration vectors respectively. Recall that
 \begin{equation}\label{internandexterncomponentsofvelocityvector2}
    \v(t)={d\r(t)\over dt}=\dot x\e_x+\dot y\e_y+\dot z\e_z=
    {d\r(u(t),v(t))\over dt}=\dot u\r_u+\dot v\r_v
 \end{equation}
  be velocity vector; $\dot u,\dot v$ are internal components of the velocity vector with respect
  to the basis $\{\r_u=\p_u,\r_v=\p_v\}$ and
  $\dot x, \dot y,\dot z$,
  are external components velocity  vectors with respect to the basis $\{\e_x=\p_x,\e_y=\p_y,\e_z=\p_z\}$ .
As always we denote by $\n$ normal unit vector.

\bigskip

  {\bf Proposition} {\it    The normal acceleration
    at an arbitrary point $\pt=\r(u(t_0),v(t_0))$ of the curve $C$ on the surface $M$
    is defined by the scalar product of the velocity vector $\v$ of the curve at the point $\pt$
    on the value of the shape operator on the velocity vector:
\begin{equation}\label{propertyofshapeoperator2}
        \ac_{n}=a_{n}\n=\left(\v, S\v\right)\n\
         \end{equation}
and normal curvature \eqref{normalcurvaturedef} is equal to

\begin{equation}\label{propertyofshpaeoperator3}
    \kappa_{n}=
{(\n,\ac)\over (\v,\v)}=
{\left(\v, S\v\right)\over (\v,\v)}
\end{equation}
}

\bigskip

{\it Proof of the Proposition}.  According to \eqref{normalacceler2} we have
                  $$
  \ac_{n}=(\n,\ac)\n=\n\left(\n,{d\over dt}\v(t)\right)\n=
     \n{d\over dt}\left(\n,\v(t)\right)-
     \n\left({d\over dt}\n(u(t),v(t)),\v(t)\right)
                    $$
                    $$
                    =0+\left(-\p_\v\n,\v\right)\n=(S\v,\v)\n
                  $$
This proves Proposition.
}

%\end{document}





\subsection {Principal curvatures, Gaussian and mean curvatures and shape operator}

Now we introduce on surfaces, principal curvatures, Gaussian curvature and mean curvature.

  Let $\pt$ be an arbitrary point of the surface $M$ and $S$ be shape operator at this point.
  $S$ is symmetric operator: $(S\ac, {\bf b})=({\bf b}, S\ac)$.  Consider
 eigenvalues $\lambda_1,\lambda_2$ and  eigenvectors $\l_1,\l_2$  of the shape operator  $S$
 \begin{equation}\label{eigenvaluesofshapeoperator}
\l_1,\l_2\in T_\pt M,\qquad    S\,\l_1=\kappa_1\l_1,\qquad S\,\l_2=\kappa_2\l_2,
   \end{equation}

\m


{\bf Definition} Eigenvalues of shape operator $\lambda_1,\lambda_2$ are called {\it principal curvatures}:
             $$
   \lambda_1=\k_1,\,\,\,\lambda_2=\k_2
              $$
 Eigenvectors $\l_1,\l_2$ define the two directions such that  curves directed along
  these vectors have normal curvature equal to the principal curvatures $\k_+, \k_-$.

These directions are called principal directions

\m



{\bf Remark} As it was noted above normal unit vector as well as a shape operator are defined up to a sign.
Hence principal curvatures, i.e. eigenvalues of shape operator are defined up to a sign too:
                \begin{equation}\label{princcurvaturearedefinedup to asign}
\n\to -\n, {\rm then}\,\, S\to -S, \,\,\,{\rm then}\,\, (\k_1,\k_2)\to (-\k_1,-\k_2)
                \end{equation}

{\footnotesize
{\bf Remark}. Principal directions are well-defined in the case if principal curvatures (eigenvalues of shape operator)
are different: $\lambda_1=\kappa_1\not=\kappa_2=\lambda_2$.
 In the case if eigenvalues $\lambda_1=\lambda_2=\lambda$ then $S=\lambda E$ is proportional to unity operator.
 In this case  all vectors
are eigenvectors, i.e. all directions are principal directions.
(This happens for the shape operator of the sphere: see the Homework 9.)



{\bf Remark}
Do shape operator have always two eigenvectors? Yes, in fact one can prove that it is symmetrical operator:
$\langle S\ac, \b\rangle=\langle S\b, \ac\rangle$ for arbitrary two vectors $\ac,\b$,
hence it has two eigenvectors.
  This implies that principal directions are orthogonal to each other.
 Indeed one can see that
             $\lambda_2(\l_2,\l_1)=(S\l_2,\l_1)=(\l_2,S\l_1)=\lambda_1(\l_2,\l_1)$.
It follows from this relation that eigenvectors are orthogonal ($(\l_-,\l_+)=0$) if $\lambda_-\not=\lambda_+$
If $\lambda_-=\lambda_+$ then all vectors are eigenvectors. One can choose in this case
$\l_-$, $\l_+$ to be orthogonal.}

  \m

{\bf Definition}
\begin{itemize}
    \item   {\it Gaussian curvature} $K$ of the surface $M$ at a point $\pt$ is equal to
                  the product of principal curvatures.
                  \begin{equation}\label{gaussinacurvaturedef0}
                    K=\k_1\k_2
                    \end{equation}


\item   {\it Mean curvature} $K$ of the surface $M$ at a point $S$ is equal to
                  the sum of the principal curvatures:
                  \begin{equation}\label{meancurvaturedef0}
                    H=\k_1+\k_2
                    \end{equation}

\end{itemize}

  Recall that the product of eigenvalues of a linear operator is determinant of this operator,
  and the sum of eigenvalues of linear operator is {\it trace} of this operator.
Thus we immediately come to the useful formulae for calculating Gaussian and mean curvatures:

 \m

 {\bf Proposition}  Let $S$ be a shape operator at the point $\pt$ on the surface $M$.  Then

\begin{itemize}

 \item  Gaussian curvature $K$ of the  surface $M$ at the point $\pt$ is equal to the determinant of the shape operator:
\begin{equation}\label{gauscurvasdeterm}
    K=\k_1\k_2=\det S
\end{equation}

\item Mean curvature  $H$ of the  surface $M$ at the point $\pt$ is equal to the trace of the shape operator
$S$:

\begin{equation}\label{meancurvasdeterm}
    H=\k_1+\k_2={\rm Tr\,} S
\end{equation}


\end{itemize}


 E.g. if  in a given coordinate basis  a shape operator is given by the matrix
 $\begin{pmatrix}a &b\cr c &d\end{pmatrix}$ (see e.g. equations \eqref{propertyofshapeoperator1}
 and\eqref{propertyofshapeoperator2} ),
then
                  \begin{equation}\label{exampleofcalcugaussian}
    K=\det S=\det \begin{pmatrix}a &b\cr c &d\cr\end{pmatrix}=ad-bc,\,\,\,
    H={\rm Tr\,} S={\rm Tr\,}\begin{pmatrix}a &b\cr c &d\cr\end{pmatrix}=a+d
          \end{equation}

\subsection { Shape operator, Gaussian and mean curvature for sphere and cylinder}

Consider now two examples.
  (These and  other examples see in detail  in the Homework 8.)

{\bf Example}  Calculate mean an Gaussian curvature for sphere $x^2+y^2+z^2=R^2$.

For the sphere of radius $R$  in spherical coordinates (see \ref{sphere})
             $$
    \r(\theta,\varphi)\colon \begin{cases}
    x=R\sin\theta\cos\varphi\cr
    y=R\sin\theta\sin\varphi\cr
     z=R\cos\theta\end{cases}\,,
     0\leq \theta\leq \pi,\,\, 0\leq \varphi\leq 2\pi
              $$
coordinate basis vectors are $\r_\theta={\p \r\over \p \theta}=\begin{pmatrix}
                                                       R\cos\theta\cos\varphi\cr
                                                       R\cos\theta\sin\varphi\cr
                                                      -R\sin\theta\cr
                                         \end{pmatrix}$,
                            $\r_\varphi={\p \r\over \p \varphi}=\begin{pmatrix}
                                                       -R\sin\theta\sin\varphi\cr
                                                       R\sin\theta\cos\varphi\cr
                                                         0\cr
                                         \end{pmatrix}$
and unit normal vector which is orthogonal to sphere equals to
$\n(\theta,\varphi)={\r(\theta,\varphi)\over R}=\begin{pmatrix}
                                                       \sin\theta\cos\varphi\cr
                                                       \sin\theta\sin\varphi\cr
                                                      \cos\theta\cr
                                         \end{pmatrix}$.
                                         
One can see that $\n$ is indeed orthogonal to the sphere.
 This is evident geometrically: the fact that 
$(\n,\r_\theta)=(\n\,,\r_\varphi)=0$ and its length equals to $1$
can be checked by straightforward calculations.
On the other hand one cna prove it noticing that
equation of sphere $x^2+y^2+z^2=R^2$ can be rewritten as
$(\r,\r)=R^2$. Differentiating this equation by $\theta$ and $\varphi$
we come to
           $$
  {\p\over \p \theta}(\r,\r)=0=2(\r_\theta,\r)\,\quad
  {\p\over \p \varphi}(\r,\r)=0=2(\r_\varphi,\r)
          $$
Thus we have proved that vector $\r$ is 
orthogonal to  basic vectors $\r_\theta,\r_\varphi$, 
i.e.to any tangent vector. The lengTH of this vector is equal to $R$.
Hence $\n=\pm{\r\over R}$.

Consider shape operator. By definition $S\v=-\p_\v \n$:
           $$
S\r_\theta=-{\p \n(\theta,\varphi)\over \p \theta}=-{\p \over \p \theta}\left({\r(\theta,\varphi)\over R}\right)=
    -{\r_\theta\over R}
           $$
and
       $$
       S\r_\varphi=-{\p \n(\theta,\varphi)\over \p \varphi}=-{\p \over \p \varphi}\left({\r(\theta,\varphi)\over R}\right)=
    -{\r_\varphi\over R}
       $$
   Hence in the coordinate basis $\r_\theta,\r_\varphi$
   $S=\begin{pmatrix}&-{1\over R} &0 \cr &0 &-{1\over R}\end{pmatrix}$.
   In the case if we choose the opposite direction for unit normal
vector then we will come to the answer just with changing the
signs: if $\n\to-\n$, $S\to-S$.

We see that principal curvatures, i.e. eigenvalues of shape operator
are the same:
             $$
 \lambda_1=\lambda_2=-{1\over R}, {\rm i.e.}\,\,        \kappa_1=\kappa_2=-{1\over R}
             $$
 (if we choose the opposite sign for $\n$ then $\kappa_1=\kappa_2={1\over R}$).
   Thus we can calculate Gaussian and mean curvature:
   Gaussian curvature
                    $$
     K=\kappa_1\cdot \kappa_2=\det S={1\over R^2}\,.
                    $$
 Mean curvature
                    $$
     H=\kappa_1+ \kappa_2={\rm Tr\,} S=-{2\over R}\,.
                    $$
If we choose the opposite sign for $\n$ then $S\to -S$,   principal
curvatures change the sign, Gaussian curvature  $K=\kappa_1\cdot
\kappa_2$ does not change but mean curvature $H=\kappa_1+ \kappa_2$
will change the sign: if $\n\to -\n$ then $H={2\over R}$.

\m

{\bf Example}  Cylindircal surface $x^2+y^2=a^2$

For the cylinder we have (see \ref{cylinder1})
             $$
    \r(h,\varphi)\colon \begin{cases}
    x=a\cos\varphi\cr
    y=a\sin\varphi\cr
     z=h\end{cases}\,,
     0\leq \varphi <2\pi, -\infty<h<\infty\,.
              $$
Coordinate basis vectors are (see \ref{cylinderbasicvectors1})
$\r_\varphi={\p \r\over \p \varphi}=\begin{pmatrix}
                                                       -a\sin\varphi\cr
                                                       a\cos\varphi\cr
                                                      0\cr
                                         \end{pmatrix}$,
                            $\r_h={\p \r\over \p h}=\begin{pmatrix}
                                                       0\cr
                                                       0\cr
                                                         1\cr
                                         \end{pmatrix}$
and unit normal vector which is orthogonal to cylinder equals to
$\n(h,\varphi)=\begin{pmatrix}
                                                       \cos\varphi\cr
                                                       \sin\varphi\cr
                                                           0\cr
                                         \end{pmatrix}$.
                                         One can see that $\n$ is indeed orthogonal to cylinder surface.
                                         This is evident geometrically but one can calculate also that
                                         ($(\n,\r_h)=(\n\,,\r_\varphi)=0$) and its length equals to $1$.
Consider shape operator. By definition $S\v=-\p_\v \n$:
           $$
S\r_h=-{\p \n(h,\varphi)\over \p h}=-{\p \over \p h }\begin{pmatrix}
                                                       \cos\varphi\cr
                                                       \sin\varphi\cr
                                                           0\cr
                                         \end{pmatrix}=0,
                                               $$
                                               and
                                               $$
       S\r_\varphi=-{\p \n(\theta,\varphi)\over \p \varphi}=-{\p \over \p \varphi}\begin{pmatrix}
                                                       \cos\varphi\cr
                                                       \sin\varphi\cr
                                                           0\cr
                                         \end{pmatrix}=
                                                 -\begin{pmatrix}
                                                       \sin\varphi\cr
                                                       -\cos\varphi\cr
                                                           0\cr
                                         \end{pmatrix}=
                                         {1\over a}\begin{pmatrix}
                                                       -a\sin\varphi\cr
                                                       a\cos\varphi\cr
                                                           0\cr
                                         \end{pmatrix}=
                                         -{\r_\varphi\over a}
       $$
   We see that $\r_h,\r_\varphi$ are eigenvectors of Shape oeprator.
   In the coordinate basis $\r_h,\r_\varphi$
   $S=\begin{pmatrix}0 &0 \cr 0 &-{1\over a}\end{pmatrix}$.
   In the case if we choose the opposite direction for unit normal
vector then we will come to the same answer just with changing the
signs: if $\n\to-\n$, $S\to-S$.

We see that principal curvatures, i.e. eigenvalues of shape operator
are:
             $$
 \lambda_1=0, \lambda_2=-{1\over R}, {\rm i.e.}\,\,        \kappa_1=0,\kappa_2=-{1\over R}
             $$
 (if we choose the opposite sign for $\n$ then $\kappa_2={1\over a}$).
   Thus we can calculate Gaussian and mean curvature:
   Gaussian curvature
                    $$
     K=\kappa_1\cdot \kappa_2=\det S=0\,.
                    $$
 Mean curvature
                    $$
     H=\kappa_1+ \kappa_2={\rm Tr\,} S=-{1\over a}\,.
                    $$
If we choose the opposite sign for $\n$ then $S\to -S$,   principal
curvatures change the sign, Gaussian curvature  $K=\kappa_1\cdot
\kappa_2$ does not change but mean curvature $H=\kappa_1+ \kappa_2$
will change the sign: if $\n\to -\n$ then $H={2\over R}$.


%\end{document}


{\footnotesize
\subsection {$^\dagger $Principal curvatures and normal curvature}

In this subsection we principal curvatures, eigenvectors of the shape operator by
$\k_-,\k_+$ and respectively eigenvectors by $\l_-,\l_+$.

    One can consider different curves passing through an arbitrary point
  $\pt$ on the surface $M$.  We know that if $\v$ velocity vector of the curve then
  normal curvature is equal to  $\k_n={(S\v,\v)\over (\v,\v)}$ (see \eqref{propertyofshpaeoperator3}).
   What are the relations between normal curvature of curves and principal curvature?
The following Proposition establishes these relations.

\m

{\bf Proposition}

Let $\k_-,\k_+$, $\k_-\leq \k_+$ be principal curvatures of  the surface $M$ at the point $\pt$
(eigenvalues of shape operator $S$ at the point $\pt$).

 Then normal curvature $\k$ of an arbitrary curve on the surface $M$ at the point $\pt$ takes values in the interval
  $(\k_-,\k_+)$:
    \begin{equation}\label{rangeofnormalcurvature}
    \k_-\leq k_n\leq \k_+
\end{equation}

\m

  {\bf Example} E.g. consider cylinder surface of the radius $R$. One can calculate that
   principal curvatures are equal to $\k_-=0,\k_+={1\over R}$ (see Homework 8). Then for an arbitrary curve
    on the surface normal curvature $\k_n$ takes values in the interval $(0,{1\over R})$ (up to a sign).
    (See Homework 8 and appendix "Normal curvature of curves on cylinder surface")


\m





{\footnotesize Proof of Proposition: If velocity vector $\v$ of curve
is collinear to the eigenvector $\l_+$, $\v=\lambda \l_+$  then normal curvature
 of the curve  $C$ at the point $\pt$ according to  \eqref{propertyofshpaeoperator3} is equal to
                       $$
                    \k_{n}={\left(\v, S\v\right)\over (\v,\v)}=
                    {\left(\lambda\l_+, S\lambda\l_+\right)\over (\lambda\l_+,\lambda\l_+)}=
                    {\lambda^2\left(\l_+, \k_+\l_+\right)\over \lambda^2(\l_+,\l_+)}=
                    {\k_+\left(\l_+, \l_+\right)\over (\l_+,\l_+)}=\k_+\,.
                       $$
Analogously if velocity vector $\v$ is collinear to the eigenvector $\l_-$ then normal curvature
 of the curve  $C$ at the point $\pt$ is equal to
 $\k_{n}={\left(\v, S\v\right)\over (\v,\v)}=
                    {\left(\l_-, S\l_-\right)\over (\l_-,\l_-)}=
                    {\left(\l_-, \k_-\l_-\right)\over (\l_-,\l_-)}=\k_-$.

In the general case if $\v=v_+\l_++v_-\l_-$ is expansion of velocity vector with respect to
the basis of eigenvectors then we have for normal curvature
\begin{equation}\label{}
k_{n}={\left(\v, S\v\right)\over (\v,\v)}=
{\left(v_+\l_+ +v_-\l_-,\lambda_+ v_+\l_+ +\lambda_- v_-\l_-\right)\over
(v_+\l_++v_-\l_-,v_+\l_++v_-\l_-)}={\k_+v_+^2+\k_-v_-^2\over v_+^2+v_-^2}\,.
\end{equation}

Hence we come to the conclusion that

\begin{equation}\label{rangeofnormalcurvaturevalues}
\k_-\leq\k_{normal}={\k_+v_+^2+\k_-v_-^2\over v+^2+v_-^2}\leq  \k_+
\end{equation}

Thus we prove that normal curvature varies in the interval $(\lambda_-, \lambda_+)$.}


Now remember the definition of principal curvatures from the subsection 4.4:
 we see that $\lambda_-,\lambda_+$ are just principal curvatures.



\m

   Summarize all the relations between normal curvature, shape operator and Gaussian and mean curvature.


\begin{itemize}

    \item  {\it Principal curvatures} $\k_-,\k_+$ of the surface $M$ at the given point $\pt$ are eigenvalues
    of shape operator $S$ acting at the tangent space $T_\pt M$ ($\k_-,\k_+$).
     Corresponding eigenvectors $\l_+,\l_-$ define directions which are called {\it principal directions}.
         Principal directions are orthogonal or can be chosen to be orthogonal if $\k_-=\k_+$.
         The normal curvature $\k_n$ for an arbitrary curve on the surface $M$ at the point $\pt$ varies in the interval
         $(\k_-,\k_+)$:
\begin{equation}\label{rangeofnormalcurvaturevalues1}
\k_-\leq\k_{n}\leq  \k_+
\end{equation}

    \item   {\it Gaussian curvature} $K$ of the surface $M$ at a point $S$ is equal to
                  the product of principal curvatures, i.e. determinant of shape operator  $S$:
                  \begin{equation}\label{gaussinacurvaturedef}
                    K=\k_+\cdot\k_-=\det S
                    \end{equation}


\item   {\it Mean curvature} $H$ of the surface $M$ at a point $S$ is equal to
                  the half-sum of the principal curvatures, i.e. half of the trace of shape operator  $S$:
                  \begin{equation}\label{meancurvaturedef}
                    H={\k_++\k_-}={\rm Tr\,\,} S
                    \end{equation}

\end{itemize}
}

\subsection {Geometrical meaning of Gaussian curvature. Theorema Egregium}

  We know that Gaussian curvature of cylinder cone and plane equals to zero
  and Gaussian curvature of sphere equals to $1\over R^2$
  (see the calculations in the end of the subsection 4.7 and  Homework 8.)

  We know that we can form cylindrical surface and cone surface
   bending the sheet of paper  without "shrinking".  On the other hand one can not form a
   part of sphere from the sheet of the paper without "shrinking" it.
   How to express mathematically this fact?

   Consider on the sheet of the paper
     two close points $A,B$    and the segment $AB$.
     The length of this segment is the shortest distance between points $A$ and $B$.
   Any curve starting at the point $A$ and finishing at the point $B$ has the length which is
   greater or equal than the length of the segment $AB$.
    When we form cylindrical (or conic) surface
   bending the sheet of the paper we {\it do not distort this property.}  The segment $AB$
   on the cylindrical surface will become the curve which we will denote also $AB$, but the length
   of this curve will be the same and it will be the shortest curve amongst all the curves connecting the points $A$
   and $B$.
   Internal observer  ("ant" mathematician living on the cylindrical surface) observes
    that the curve $AB$ on the cylinder has the same length as it has before (being the segment on
    flat sheet of the paper).  This is strictly related with the fact that Gaussian curvature of
    the cylinder surface equals to zero.


  {\bf Theorem} (Theorema Eggregium) {\it The Gaussian curvature of surface is defined by first quadratic form.
If  Two surfaces have the same quadratic form then they have the same Gaussian
 curvature.

 In other words if we measure the length of the curves and angles between them on two surfaces
we will come to the same answers, then these surfaces have the same Gaussian curvature.


In particular if a surfaces have vanishing Gaussian curvature then locally one comes to this surface
bending the sheet of the paper without "shrinking".

}
 This  Theorem explains why sphere even locally cannot be transformed to the plane without distorting.

 This remarkable Theorem which belongs to Gauss is the foundation result in differential geometry.

 The proof of Theorem will be given in the course of Riemannian geometry.

 {\footnotesize
 Note that we calculated Gaussian curvature using Shape operator, i.e. in terms of External observer.
 The Theorem says that Gaussian curvature depends only on distances on the surface, hence
 the internal observer can calculate the gaussian curvature. How it can be done?



  We will formulate another Theorem which is strictly related with the {\it Theorema Egregium}
  and explains how internal observer can  calculate Gaussian curvature.


  {\bf Theorem}
  {\it Let  $C$ be  a closed curve on a surface $M$ such that $C$ is a boundary of a compact oriented domain $D\subset M$,
then during the  parallel transport of an arbitrary tangent vector  along the closed curve $C$
the vector rotates through the angle
\begin{equation}\label{theoremofrotationonangle4}
\Delta\Phi=\angle\left({\X, \R_C\X}\right)=\int_D K d\sigma\,,
             \end{equation}
where $K$ is the Gaussian curvature and $d\sigma=\sqrt {\det g}dudv$ is the area element induced by the
First quadratic form on the surface on the surface $M$, i.e.  $d\sigma=\sqrt {\det g}dudv$.

In particular if Gaussian curvature $K$ is constant then
\begin{equation}\label{theoremofrotationonangle5}
\Delta\Phi=\angle\left({\X, \R_C\X}\right)=KS
             \end{equation}

For example consider the sphere $x^2+y^2+z^2=R^2$ and the triangle $ABC$ on it
with vertices $A=(0,0,1)$, $B=(1,0,0)$ and $C=(\cos\varphi, \sin \varphi,0)$.
Then during parallel transport of the vector along the triangle $ABC$ it will rotate
on the angle $\varphi$ (see the Homework 8). On the other hand
the area of this triangle equals to $S=R^2 \varphi$. We see that
 $$
 \varphi={S\over R^2}=KS
 $$
}


  The angle of rotation of tangent vector in fact depends only on the internal geometry of surface.
  Thus the relation above can be used for proving the Theorema Egregium.


The Theorem above has very interesting


{\bf Corollary} Let $ABC$ be triangle on the surface $M$ where $AB,AC,BC$ are shortest curves
 connecting the points $A,B,C$. Let $\a,\beta,\gamma$ be angles of this triangle.
 For usual triangle sum of angles equal to $\pi$. It turns out that for triangel on the surface
 the sum of angles is related with Gaussian curvature:
         \begin{equation}
         \a+\beta+\gamma-\pi=\int_{\triangle ABC} K d\sigma
         \end{equation}

 Internal observer may use this formula for calculating gaussian curvature at any given point:
 He draws the triangle calculate the sum of angles and see that
              $$
           K\approx{\a+\beta+\gamma-\pi\over S}
              $$


%\end{document}

 (See in more detail "A tale on differential geometry" in Appendices.)}

%2 May

%\end{document}




%\section { $^*$Parallel transport. Geometrical meaning of Gaussian curvature}


%\end{document}



\section {$\dagger$Appendices}


{\footnotesize

\subsection { Formulae for vector fields and differentials in
cylindrical and spherical  coordinates}



{\bf Cylindrical and spherical  coordinates}
\begin {itemize}
 \item   Cylindrical coordinates in $\E^3$
        \begin{equation}\label{polarcoordinates}
               \begin{cases}
                x=r\cos\varphi\\
                y=r\sin\varphi\\
                z=h
                \end{cases}\quad
               ( 0\leq \varphi<2\pi, 0\leq r<\infty)
              \end{equation}
  \item   Spherical coordinates in $\E^3$
      \begin{equation}\label{polarcoordinates}
               \begin{cases}
                x=r\sin\theta\cos\varphi\\
                y=r\sin\theta\cos\varphi\\
                z=r\cos\theta
                \end{cases}
                ( 0\leq \varphi<2\pi,   0\leq r<\infty)
                    ---
                    \hbox{cylindrical  coordiantes in $\E^3$}
              \end{equation}
\end{itemize}



 {\bf Example} (Basis vectors and forms for cylindrical coordinates)
\def \pg {{\bf \partial}}


Consider cylindrical coordinates in $\E^3$: $u=r,v=\varphi,w=h$. Then
calculating partial derivatives we come to
   \begin{equation}\label{basisvectorsforcylindricalcoordinates}
           \begin{cases}
            \p_r={\p x\over \p r}\p_x+{\p y\over \p r}\p_y+{\p z\over \p r}\p_z=\cos\varphi \p_x+\sin\varphi\p_y  \\
             \phantom{}\\
            \p_\varphi={\p x\over \p \varphi}\p_x+{\p y\over \p \varphi}\p_y+{\p z\over \p \varphi}\p_z=
               -r\sin\varphi\p_x+r\cos\varphi\p_y  \\
            \phantom{}\\
            \p_h={\p x\over \p h}\p_x+{\p y\over \p h}\p_y+{\p z\over \p h}\p_z=\p_z  \\
            \end{cases}
           \end{equation}
  Basic forms are $dr,d\varphi, dh$ and
\begin{equation}\label{defofbasicformcylindricalcoordinates}
\begin{array} {cc}
  dr(\p_r)=1,  dr(\p_\varphi)=0, dr(\p_h)=0\\
    \phantom{}\\
  d\varphi(\p_r)=0,  d\varphi(\p_\varphi)=1, d\varphi(\p_h)=0\\
\phantom{}\\
  dh(\p_r)=0,  dh(\p_\varphi)=0, dh(\p_h)=1\\
     \end{array}
\end{equation}




\m

{\bf Example} (Basis vectors for spheric coordinates)
\def \pg {{\bf \partial}}


Consider spheric coordinates in $\E^3$: $u=r,v=\theta,w=\varphi$. Then
calculating partial derivatives we come to
   \begin{equation}\label{basisvectorsforcylindricalcoordinates}
           \begin{cases}
            \p_r={\p x\over \p r}\p_x+{\p y\over \p r}\p_y+{\p z\over \p r}\p_z=
            \sin\theta\cos\varphi \p_x+\sin\theta\sin\varphi\p_y+\cos\theta\p_z  \\
             \phantom{}\\
            \p_\theta={\p x\over \p \theta}\p_x+{\p y\over \p \theta}\p_y+{\p z\over \p \theta}\p_z=
           r\cos\theta\cos\varphi\p_x+r\cos\theta\sin\varphi\p_y
                  -r\sin\theta\p_z\\
                       \phantom{}\\
            \p_\varphi={\p x\over \p \varphi}\p_x+{\p y\over \p \varphi}\p_y+{\p z\over \p \varphi}\p_z=
               -r\cos\theta\sin\varphi\p_x+r\sin\theta\cos\varphi\p_y  \\
            \end{cases}
           \end{equation}
  Basic forms are $dr,d\theta, d\varphi$ and
\begin{equation}\label{defofbasicformcylindricalcoordinates}
\begin{array} {cc}
  dr(\p_r)=1,  dr(\p_\theta)=0, dr(\p_\varphi)=0\\
    \phantom{}\\
  d\theta(\p_r)=0,  d\theta(\p_\theta)=1, d\theta(\p_\varphi)=0\\
\phantom{}\\
d\varphi(\p_r)=0,  d\varphi(\p_\theta)=0, d\varphi(\p_\varphi)=1\\
     \end{array}
\end{equation}





 We know that $1$-form is a linear function on tangent vectors.
\def\B {{\bf B}}
If $\A,\B$
are two vectors attached at the point $\r_0$, i.e. tangent to this point
and $\w,\rho$ are two $1$-forms then one  defines the value of $\w\wedge \rho$
on $\A, B$ by the formula
\begin{equation}\label{valueoftwoformontwovectors1}
    \w\wedge\rho(\A,\B)=\w(\A)\rho(B)-\w(B)\rho(A)
\end{equation}
We come to bilinear anisymmetric function on tangent vectors.
If $\sigma=a(x,y)dx\wedge dy$ is an arbitrary two form then this form defines
bilinear form on pair of tangent vectors:  $\sigma(\A,\B)=$
\begin{equation}\label{valueoftwoformontwovectors1}
    a(x,y)dx\wedge dy(A,B)=a(x,y)\left(dx(\A)dy(\B)-dx(\B)dy(\A)\right)=
    a(x,y)(A_xB_y-A_yB_y)
\end{equation}
One can see that in the case if $a=1$ then
right hand side of this formula is nothing but the area of parallelogram
spanned by the vectors $\A,\B$.



This leads  to the conception of integral of form over domain.


Let $\w=a(x)dx\wedge dy$ be a two form and $D$ be a domain in $\E^2$. Then by definition
 \begin{equation}\label{intoftwoforms}
    \int_D \w=\int_D a(x,y)dx dy
\end{equation}
If $\w=dx\wedge dy$ then
\begin{equation}\label{intoftwoforms2}
    \int_D w=\int_D (x,y)dx dy=\hbox{Area of the domain $D$}
\end{equation}


The advantage of these formulae is that we do not care about coordinates\footnote
{If we consider changing of coordinates then jacobian appears:
If $u,v$ are new coordinates, $x=x(u,v)$, $y=y(u,v)$ are new coordinates then
\begin{equation}\label{jacobia}
    \int a(x,y)dxdy=\int a(x(u,v), y(u,v))\det\begin{pmatrix}x_u & x_v\\ x_u & x_v\\
          \end{pmatrix}dudv
\end{equation}
In formula\eqref{intoftwoforms} it appears under as a part of coefficient of differential form.}


\m

{\bf Example} Let $D$ be a domain defined by the conditions
          \begin{equation}\label{upperhalfellipse}
      \begin{cases}
      x^2+y^2\leq 1\\
      y \geq 0\\
      \end{cases}
\end{equation}
 Calculate $\int_D dx\wedge dy$.

 $\int_D dx\wedge dy=\int_D dxdy=$ area of the $D={\pi\over 2}$.

 If we consider polar coordinates then according \eqref{areaforminpolarcoordinates}
      $$
      dx\wedge dy=rdr\wedge d\varphi
      $$
Hence $\int_D dx\wedge dy=\int_D rdr\wedge d\varphi=\int_D rdrd\varphi=
\int_0^1\left(\int_0^\pi d\varphi\right)rdr$ $=\pi \int_0^1rdr=\pi/2$.



{\footnotesize Another example


{\bf Example}  Let $D$ be a domain in $\E^2$ defined by the conditions
          \begin{equation}\label{upperhalfellipse}
      \begin{cases}
      {(x-c)^2\over a^2}+{y^2\over b^2}\leq 1\\
      y \geq 0
      \end{cases}
\end{equation}

$D$ is domain restricted by upper half of the ellipse and $x$-axis. Ellipse has the centre at the point $(c,0)$.
Its area is equal to  $S=\int_D dx\wedge dy$. Consider new variables $x', y'$: $x=c+ax', y=by'$.
In new variables domain $D$ becomes the domain from the previous example:
                $$
{(x-c)^2\over a^2}+{y^2\over b^2}={x'}^2+{y'}^2
                $$
         and $dx\wedge dy=ab dx'\wedge dy'$.
         Hence
             \begin{equation}\label{areaofhalellipse}
    S=\int_{{(x-c)^2\over a^2}+{y^2\over b^2}\leq 1,y\geq 0} dx\wedge dy=
    ab\int_{{x'}^2+{y'}^2\leq 1,y'\geq 0} dx'\wedge dy'={\pi ab\over 2}
\end{equation}

{\footnotesize
{\bf Theorem 2} ( Green formula) Let $\w$ be $2$-form such that $\w=d\w'$
and  $D$ be a domain--interior of the closed curve $C$. Then
  \begin{equation}\label{integralofexactform}
\int_D w = \int_C w'
\end{equation}
}
}
\subsection {Curvature and second order contact (touching) of curves}

Let $C_1, C_2$ be two curves in $\E^2$. For simplicity we here consider only curves in $\E^2$.

\m


 {\bf Definition} Two non-parameterised curves $C_1,C_2$ have second order contact (touching) at the point $\r_0$ if

 \begin{itemize}
\item  They coincide at the point $\r_0$

\item they have the same tangent line at this point

\item they have the same curvature at the point $\r_0$
 \end{itemize}

\m



If $\r_1(t),\r_2(t)$ are an arbitrary parameterisations of these curves such that $\r_1(t_0)=\r_2(t_0)=\r_0$
then the condition that they have the same tangent line means that velocity vectors $\v_1(t),\v_2(t)$
are collinear at the point $t_0$.

(As always we assume that curves under considerations are smooth and regular, i.e. $x(t), y(t)$ are smooth functions
and velocity vector $\v(t)\not=0$.)


\m


{\footnotesize {\bf Example} Consider two curves $C_{f},  C_{g}$---graphs of the functions $f_1,f_2$.
Recall that curvature of the graph of the function $f$ at the point $(x,y=f(x))$ is equal to
(see \eqref{curvatureofgraph})
 \begin{equation}\label{curvatureofthegraph2}
    k(x)={f^{\prime\prime}(x)\over \left(1+f^\prime(x)\right)^{3\over 2}}
\end{equation}
Then condition
of the second order touching at the point $\r_0=(x_0,y_0)$ means that
     $$
 \begin{cases}
\hbox{They coincide at the point $\r_0$: $f(x_0)=g(x_0)$}\\
\hbox{They have the same tangent line at this point: $f'(x_0)=g'(x_0)$}\\
\hbox {They have the same curvature at the point $\r_0$:}
{f^{\prime\prime}(x_0)\over \left(1+f^\prime(x_0)\right)^{3\over 2}}=
{g^{\prime\prime}(x_0)\over \left(1+g^\prime(x_0)\right)^{3\over 2}},
\,\,{\rm i.e.}\,\, f^{\prime\prime}(x_0)=g^{\prime\prime}(x_0)
\end{cases}
 $$
We see that second order touching means that difference of the functions in vicinity of the point  $x_0$
is of order $o((x-x_0)^2)$. Indeed due to Taylor formula
                \begin{equation}\label{diffoffunctions}
                \begin{array}{cc}
           f(x)=f(x_0)+f'(x_0)(x-x_0)+{1\over 2}f''(x_0)(x-x_0)^2+\dots\\
            g(x)=g(x_0)+g'(x_0)(x-x_0)+{1\over 2}g''(x_0)(x-x_0)^2+\dots\\
            \end{array}
\end{equation}
where we denote by dots terms which are $o(x-x_0)^2$. (They say that $f(x)=o(x-x_0)^n$ if
$\lim_{x\to x_0}{f(x)\over (x-x_0)^n}=0$).

Hence
              \begin{equation}\label{diffoffunctionsinthecaseofsecondordertouching}
    f(x)-g(x)=o(x-x_0)^2
\end{equation}
because $f(x_0)=g(x_0),f(x_0)=g(x_0),f'(x_0)=g'(x_0)$ and $f''(x_0)=g''(x_0)$



In general case if two curves have second order contact then in the vicinity of the contact point
one can consider these curves as a graphs of the functions $y=f(x)$ (or $x=f(y)$).}

\m

To clarify geometrical meaning of second order touching consider the case
where one of the curves is a circle.
Then second order touching means
that curvature of one of these curves is equal to $1/R$, where $R$ is a radius of the circle.


{\it We see that to calculate the radius of the circle which has the second order touching with the given curve
 at the given point  we have to calculate the curvature of this curve
     at this given point.}

\m



{\bf Example}. Let $C_1$ be parabola $y=ax^2$ and $C_2$ be a circle. Suppose these curves have second order contact
at the vertex of the parabola: point $0,0$.


Calculate the curvature of the parabola at the vertex.
Curvature at the vertex is equal to
         $k(t)\vert_{t=0}=2a$ (see Homework).
         Hence the radius of the circle which has second order touching
         is equal to
                  $$
                R={1\over 2a}.
                  $$
To find equation of this circle note that the circle which has second order touching to parabola at the vertex
passes trough the vertex (point $(0,0)$) and is tangent to $x$-axis. The radius of this circle is equal to
$R={1\over 2a}$. Hence equation of the circle is
          $$
         (x-R^2)+y^2=0, \,\,{\rm where}\,\,\, R={1\over 2a}
          $$

{\footnotesize One comes to the same answer by the following detailed analysis:

Consider equation of a circle: $(x-x_0)^2+(y-y_0)^2=R^2$. The condition that
 curves coincide at the point $(0,0)$ means that $x_0^2+y_0^2=R^2$. $x$-axis is tangent to parabola at the vertex.
 Hence it is tangent to the circle too. Hence $y_0^2=R^2$ and $x_0=0$. We see that an equation of the circle
 is $x^2+(y-R)^2=R^2$.  The circle $x^2+(y-R)^2=x^2+y^2-2yR=0$
 in the vicinity of the point $(0,0)$ can be considered as a graph of the function
           $
     y=R- \sqrt{R^2-x^2}\,.
           $.
The condition that functions $y=ax^2$ and $y=R- \sqrt{R^2-x^2}$ have second order contact means
  that
           $$
       R- \sqrt{R^2-x^2}=ax^2+\hbox{terms of the order less that $x^2$}\,.
           $$
But
         $$
         R- \sqrt{R^2-x^2}=
         R-R\sqrt{1-{x^2\over 2R^2}}=R-
         R\left(1-{x^2\over 2R^2}+o(x^2)\right)={x^2\over 2R}+o(x^2)\,.
         $$
Comparing we see that $a={1\over 2R}$ and ${1\over R}=2a$.
But curvature of the parabola at the vertex is equal to $k=2a$ (if $a>0$). We see that $k={1\over R}$.}



%\end{document}



\subsection { Integral of curvature over planar curve.}

{\footnotesize We consider here the following problem: Let $C=\r(t)$ be a planar curve,
i.e. a curve in $\E^2$.

Let  $\n(\r(t)$ be a unit normal vector field to the curve, i.e.  $\n$ is orthogonal to the curve
(velocity vector) and it has unit length.

E.g. if $\r(t): x(t)=R\cos t,y(t)=R\sin t$, then $\n(\r(t))=
\begin{pmatrix} \cos t\\ \sin t \\ \end{pmatrix}$


 If point moves along the curve $\r(t), t_1\leq t\leq t_2$ then velocity
 vector and vector field $\n(t)$ rotate on the same angle. It turns out
 that this angle is expressed via integral of curvature over the curve...


  Try to analyze the situation:

\bigskip

    {\bf Proposition}  Let $C\colon \quad \r(t)$ be a curve in $\E^2$,
    $\v(t)={d\r(t)\over dt}$, velocity vector, $k(\r(t))$---curvature  and $\n(t)$ unit normal vector field.
   Denote by $\varphi(t)$ the angle between normal vector $\n(t)$ and $x$-axis.

    Then
     \begin{equation}\label{firstporpertyofshapeoperatorforcurve1}
    {d\n(t)\over dt}=\pm k(\r(t))\v(t)
\end{equation}

 \begin{equation}\label{firstporpertyofshapeoperatorforcurve2}
    {d\varphi(t)\over dt}=\pm k(\r(t))|\v(t)|
\end{equation}

(Sign depends on the orientation of the pair of vectors $(\v,\n)$)



 Note that the second statement of the Proposition has a clear geometrical meaning:
  If $C$ is a circle of the radius $R$ then RHS of \eqref{firstporpertyofshapeoperatorforcurve2}
is equal to $v\over R$. It is just angular velocity $d\varphi/dt$.


\bigskip


{\footnotesize  To prove this Proposition note that $(\n,\n)=1$. Hence
              $$
   0={d\over dt}(\n(t),\n(t))=2\left({d\n(t)\over dt},\n(t)\right)\,,
              $$
i.e. vector ${d\n(t)\over dt}$ is orthogonal to the vector  $\n$. This means
that ${d\n(t)\over dt}$ is collinear to $\v(t)$, because curve is planar.
We have  ${d\n(t)\over dt}=\kappa(\r(t))\v(t)$ where $\kappa$ is a coefficient.
 Show that the coefficient $\kappa$ is just equal to curvature  $k$
(up to a sign). Clearly $(\n,\v)=0$ because these vectors are orthogonal. Hence
              $$
    0={d\over dt}(\n(t),\v(t))=
    \left({d\n(t)\over dt},\v(t)\right)+
    \left(\n(t),{d\v(t)\over dt}\right)=
              $$
               $$
                 \left(\kappa(\r(t))\v(t),\v(t)\right)+
    \left(\n(t),\ac(t)\right)
     =\kappa(\r(t))|\v(t)|^2+(\n,\ac_{\perp})\,,
              $$

because $\left(\n(t),\ac(t)\right)=(\n,\ac_{\perp})$. But $(\n,\ac_{\perp})$
is just centripetal acceleration: $(\n,\ac_{\perp})=\pm |\ac_{\perp}|$ and curvature
is equal to $|\ac_{\perp}|/|\v|^2$. Hence we come to $ \kappa (\r(t))=\pm {|\ac_{\perp}|\over |\v|^2}=\pm k$.
Thus we prove \eqref{firstporpertyofshapeoperatorforcurve1}.

To prove \eqref{firstporpertyofshapeoperatorforcurve2} consider expansion of
vectors $\n(t),\v(t)$ over basis vectors $\p_x,\p_y$. We see that
\begin{equation}\label{expansionof}
       \n(t)=\cos \varphi(t)\p_x+\sin \varphi(t)\p_y \,\,{\rm and}\,\,\v(t)=
       |\v(t)|\left(-\sin \varphi(t)\p_x+\cos \varphi(t)\p_y\right)
\end{equation}
Differentiating $\n(t)$ by $t$ we come to
       $
     {d\n(t)\over dt}={d\varphi(t)\over dt}\left(-\sin \varphi(t)\p_x+\cos \varphi(t)\p_y\right)=
     {d\varphi(t)\over dt}{\v(t)\over |\v(t)|}
        $.
Comparing this equation with equation \eqref{firstporpertyofshapeoperatorforcurve1}
we come to \eqref{firstporpertyofshapeoperatorforcurve2}.


   The appearance of sign factor in previous formulae related with the fact
   that normal vector field is defined up to a sign factor $\n\to -\n$.

   It is useful to write formulae \eqref{firstporpertyofshapeoperatorforcurve1},
   \eqref{firstporpertyofshapeoperatorforcurve2} in explicit way. Let $\r(t)\colon x(t),y(t)$
   be a parameterisation of the curve. Then $\v(t)=\begin{pmatrix}x_t\\y_t\end{pmatrix}$ velocity vector.
   One can define normal vector field as
   \begin{equation}\label{exprfornormalvectorfield1}
 \n(t)={1\over \sqrt{x_t^2+y_t^2}}\begin{pmatrix}-y_t\\x_t\end{pmatrix}
\end{equation}
 or changing the sign as
 \begin{equation}\label{exprfornormalvectorfield2}
 \n(t)={1\over \sqrt{x_t^2+y_t^2}}\begin{pmatrix}y_t\\-x_t\end{pmatrix}
\end{equation}
If we consider \eqref{exprfornormalvectorfield1} for normal vector field then
\begin{equation}\label{therotationformulawithoutsign}
    {d\n (t)\over dt}={x_{tt}y_t-y_{tt}x_t\over (x_t^2+y_t^2)^{3\over 2}}\begin{pmatrix}x_t\\y_t\end{pmatrix}
\end{equation}
Recalling that  $k={|x_{tt}y_t-y_{tt|}x_t\over (x_t^2+y_t^2)^{3\over 2}}$ we come to \eqref{firstporpertyofshapeoperatorforcurve1}.
For the angle we have
     \begin{equation}\label{rotationoftheangle}
    {d\varphi\over dt}={x_{t}y_{tt}-y_{t}x_{tt}\over (x_t^2+y_t^2)^{3\over 2}}\sqrt{x_t^2+y_t^2}=
    {x_{t}y_{tt}-y_{t}x_{tt}\over (x_t^2+y_t^2)}
\end{equation}
This follows from the considerations above but it can be also calculated straightforwardly.


{\bf Remark} Note that last two formulae do not possess indefenity in sign.}

\bigskip

   This Proposition has very important application. Consider just two examples:

Consider upper half part of the ellipse $x^2/a^2+y^2/b^2=1, y\geq 0$. We already know that
 curvature at the point $x=a\cos t, y=b\cos t$ of the ellipse is equal to
            $$
            k={ab\over (a^2\sin t^2 t+b^2\cos^2 t)^{3/2}}
              $$
and speed is equal to $\sqrt {a^2\sin^2 t+b^2\cos^2 t}$
Apply formula \eqref{firstporpertyofshapeoperatorforcurve2} of Proposition.
 The curvature is not equal to zero at all the point. Hence the sign in the
 \eqref{firstporpertyofshapeoperatorforcurve2} is the same for all the points, i.e.
          \begin{equation}\label{}
\pi=    \int_0^{\pi} {d\varphi (t)dt }=
    \pm \int_0^\pi k(\r(t))|\v(t)|=
\end{equation}
       $$
       \int_0^\pi {ab \over (a^2\sin^2 t +b^2\cos^2 t)^{3/2}}\sqrt {a^2\sin^2 t+b^2\cos^2 t}\,dt=
      \int_0^\pi {ab dt\over a^2\sin^2 t +b^2\cos^2 t}\,.
       $$
 We
calculated this integral  using geometrical considerations: left hand
side represents the angle of rotation of normal unit vector and this angle is equal to $\pi$.
Try to calculate the last integral straightforwardly: it is not easy exercise in calculus.

\m

Another example: Let $\r=\r(t), x=x(t),y=y(t), t_1\leq t\leq t_2$ be a closed curve in $\E^2$
($\r(t_1)=\r(t_2)$.) We suppose that
it possesses self-intersections points. We cannot us a formula \eqref{firstporpertyofshapeoperatorforcurve2}
for integration because in general curvature may vanish at some points, but we still can use
the formula \eqref{rotationoftheangle}. The rotation of the angle $\varphi$ is equal to $2\pi n$,
($n$-is called winding number of the curve). Hence according to \eqref{rotationoftheangle} see that
                 $$
                 \int_{t_1}^{t_2}{x_ty_{tt}-x_ty_{tt}\over x_t^2+y_t^2}=2\pi n
                 $$
or \begin{equation}\label{gaussbonetforn=1}
    {1\over 2\pi} \int_{t_1}^{t_2}{x_ty_{tt}-x_ty_{tt}\over x_t^2+y_t^2}= n
\end{equation}
The integrand us equal to the curvature multiplied by the speed (up to a sign).
Left hand side is integral of continuous function
divided by transcendent number $\pi$. The geometry tells us that the answer must to be equal to integer number.}}




\subsection { Relations between usual curvature normal curvature and geodesic curvature.}

{\footnotesize

Consider at any point $\pt$ of the curve the  following basis
                     $\{\v, {\bf f},\,\, \n\}$, where
  \begin {itemize}
\item  velocity vector $\v$ is tangent to the curve
\item the vector ${\bf f}$ is the vector tangent to the surface but orthogonal to the vector $\v$.

\item $\n$ is the unit normal vector to the surface, i.e.it is orthogonal to vectors $\v$ and ${\bf f}$.
  \end {itemize}


   Decompose acceleration vector over three directions, i.e. over three one-dimensional spaces
   spanned by vectors $\v, {\bf f}$ and $\n$:


\begin{equation}\label{decompositionofaccelvector1}
    \ac=\ac_{\hbox {\footnotesize orthogonal to surface}}+
    \ac_{\hbox {\footnotesize tang.to surf. and orthog. to curve}}+
    \ac_{\hbox {\footnotesize tangent to curve}}
\end{equation}



 The vector $\ac_{\hbox {\footnotesize orthogonal to surface}}\,$ which is collinear to normal unit vector $\n$,
 will be called {\it vector of normal  acceleration of the curve on the surface}.
We denote it    by $\ac_{n}$.

The vector  $\ac_{\hbox {\footnotesize tang.to surf. and orthog. to curve}}\,$, collinear to unit vector
${\bf f}_C$ will be called  {\it vector of geodesic acceleration }. We
denote it by $\ac_{geod}$.

The vector  $\ac_{\hbox {\footnotesize tangent to curve}}\,$, collinear to velocity vector $\v$,
 is just {\it  vector of tangential acceleration }.
We denote it  $\ac_{tang}$. We can rewrite \eqref{decompositionofaccelvector1} as

\begin{equation}\label{decompositionofaccelvector2}
    \ac=\ac_n+
    \ac_{geod}+
    \ac_{tang}
\end{equation}

   Study the expansion \eqref{decompositionofaccelvector2}.
   Both vectors $\ac_{n}$ and $\ac_{geod}$ are orthogonal to the curve.
   The vector   $\ac_{geod}$ is orthogonal to the curve but it is tangent to the surface.
   The vector   $\ac_{n}$ is orthogonal not only to the curve. It is orthogonal to the surface.


   The vector $\ac_{geod}+\ac_{n}=\ac_{\perp}$ is orthogonal to the curve. It is the vector
   of  normal acceleration of the curve.

\m

 {\bf Remark} Please note that when
  we consider the curves on the surface it could arise the confusion between
  the vector $\ac_n$---normal acceleration of the curve on the surface and
  the vector $\ac_{\perp}$ of normal acceleration of the curve (see \eqref{decompositontangentandorthogonal}).

 When we  decompose in \eqref{decompositionofaccelvector2}
 the acceleration vector $\ac$ in the sum of
 three vectors $\ac_n,\ac_{geod}$ and $\ac_{tang}$ then
 the vector  $\ac_{n}$, {\it the normal acceleration of the curve on the surface}
   is orthogonal to the surface not only to the curve.
  The vector $$
           \ac_{\perp}=\ac_{n}+\ac_{geod},
           $$

  is orthogonal only to the curve and in general it is not orthogonal to the surface (if $\ac_{geod}\not=0$).
   It is the normal acceleration of the curve. It
   depends only on the curve.
   The normal acceleration $\ac_n$ of the curve on the surface which is orthogonal to the surface
   depends on the surface where the curve lies.

\m

We know that the curvature of the curve is equal to the magnitude of normal acceleration of the curve
divided on the square of the speed (see \eqref{generalformulaforcurvature2}). We have:
     $$
\hbox{curvature of the curve}\,\,k={|\ac_{\perp}|\over |\v|^2}={|\ac_{n}+\ac_{geod}|\over |\v|^2}\,.
   $$
   The vectors $\ac_{n}$ and $\ac_{geod}$ transform under reparameterisation in the same way as a vector
  $\ac_{\perp}$ (see \eqref{observation}). If
$t\to t(\tau)$  then
\begin{equation}\label{changingofcomponentsanderreparameterisation}
   \ac'_{\perp}(\tau)=t_{\tau}^2\ac_{\perp}\,\,\,{\rm and}\,\,\,   \ac'_n(\tau)=
 t_\tau^2\ac_n(t),\,\, \,\, \ac'_{geod}(\tau)=
 t_\tau^2\ac_{geod}(t)
\end{equation}
where $\ac'(\tau)={d^2\over d\tau^2}\r(t(\tau))=t_\tau^2\ac+t_{\tau\tau}\v$
(see  \eqref{changingoftangential}, \eqref{changingofcentripetalaccele},
\eqref{changingofaccelerationvectorunderchangingofparametrisation}).
Hence the magnitudes
              \begin{equation}\label{changingofcomponentsanderreparameterisationinv2}
  {|\ac_{geod}|\over |\v|^2}\,\,{\rm and} \qquad{|\ac_{n}|\over |\v|^2}
              \end{equation}
are reparameterisation invariant as well as magnitude
$k={|\ac_{\perp}|\over |\v|^2}={|\ac_{n}+\ac_{geod}|\over |\v|^2}$.



Multiply left and right hand sides of the equation
\eqref{decompositionofaccelvector2} on unit normal vector $\n$. Then
$(\ac_{tang},\n)=(\ac_{geod},\n)=0$ because vectors $\ac_{geod}$ and $\ac_{tang}$
are orthogonal to the vector $\n$. We come to the relation
       \begin{equation}\label{normalacceler2}
     \ac_{n}=(\n,\ac)\n \,\,\,\,{\rm and}\,\,\\, |\ac_{n}|=|(\ac,\n)|\,.
\end{equation}
Or in other words scalar product $(\n,\ac)$ is equal to $|\ac_n|$ (up to a sign).

Compare the formula
\begin{equation}\label{normalcurvaturedef2}
    \kappa_{n}={(\n,\ac)\over (\v,\v)}
\end{equation}
(see \eqref{normalcurvaturedef}) for normal curvature
with the formula $$k={|\ac_\perp|\over (\v,\v)}$$ for usual curvature (see \eqref{generalformulaforcurvature2}).


It follows from \eqref{changingofcomponentsanderreparameterisation},
\eqref{changingofcomponentsanderreparameterisationinv2}
and \eqref{normalcurvaturedef} (or \eqref{normalcurvaturedef2}) that for any curve on the surface
the modulus of the normal curvature is less or equal than usual curvature.
           \begin{equation}\label{inequalityfornormalcurvature}
     |\kappa_{n}|\leq k
\end{equation}
Indeed we have for usual curvature
\begin{equation}\label{curvatureofarbitrarycurveonthesurface1}
   k={|\ac_{\perp}|\over |\v|^2}={|\ac_{geod}+\ac_{normal}|\over |\v|^2}=
   \sqrt {\ac_{geod}^2+\ac_{norm}^2\over |\v|^2}\geq
       {|\ac_{normal}|\over |\v|^2}=|\kappa_{n}|
    \end{equation}


Normal curvature is a positive or negative real number. (Usual curvature is non-negative real number). Normal curvature
 changes a sign if $\n\to-\n$.





                 {\footnotesize {\bf Remark}
     We obtained in \eqref{changingofcomponentsanderreparameterisationinv2} that the magnitude
     ${|\ac_{geod}|\over |\v|^2}$ is reparameterisation invariant. It defines so called {\it geodesic curvature}
      $\kappa_{geod}={|\ac_{geod}|\over |\v|^2}$. We see that usual curvature  $k$,
  normal curvature $\k$ and geodesic curvature $\k_{geod}$
  are related by the formula
           \begin{equation}\label{geodcurvature2}
           k^2=
    \kappa^2_{geod}+\kappa^2_{normal}
               \end{equation}
              }
}



\subsection { Normal curvature of curves on cylinder surface.}
{\footnotesize
{\bf Example}  Consider an arbitrary curve $C\colon h=h(t), \varphi=\varphi(t)$ on the cylinder
            $$
\r(\varphi,h)\colon \begin{cases}x=R\cos\varphi\\y=R\sin\varphi\\ z=h\\\end{cases}\,
            $$
Pick any point $\pt$ on this curve and
find normal acceleration vector at this point of this curve.



Without loss of generality suppose that point $\pt$ is just a point $(R,0,0)$.
Note that vector $\e_x$ attached at the point $(R,0,0)$ is unit vector orthogonal to the surface of cylinder,
i.e. $\e_x=-\n$ at the point $\pt=(R,0,0)$.

\smallskip

{\bf Remark}Unit vector, as well as normal curvature is defined up to a sign.
It is convenient for us to choose $\n=-\e_x$, not $\n=\e_x$.

\smallskip

 Vectors $\e_y,\e_z$ are tangent to the surface of cylinder.
At the point $\pt=(R,0,0)$ $\varphi=0,h=0$.



 We have
           $$
           \v={d\r(t)\over dt}=
                     {dx(t)\over dt}\e_x+{dy(t)\over dt}\e_y+{dz(t)\over dt}\e_z=
               $$
               $$
                     R{d \cos \varphi(t)\over dt}\e_x+
                     R{d \sin \varphi(t)\over dt}\e_y+
                     {d h(t)\over dt}\e_z=
 -R\sin\varphi\dot \varphi\e_x+
                     R\cos\varphi\dot \varphi\e_y+
                     \dot h\e_z
 $$
\begin{equation}\label{velocityofcylinder}
\hbox{Thus}\,\,\,   \v= R\dot\varphi\e_y+\dot h\e_z\,\, \hbox {at the point $\pt=(R,0,0)$}\,.
 \end{equation}
For acceleration vector
                $$
                     \ac={d^2\r(t)\over dt^2}=
                     {d^2x(t)\over dt^2}\e_x+{d^2y(t)\over dt^2}\e_y+{d^2z(t)\over dt^2}\e_z=
                       R{d^2 \cos \varphi(t)\over dt^2}\e_x
                       +R{d^2 \sin \varphi(t)\over dt^2}\e_y
                       +{d^2 h(t)\over dt^2}\e_z=
                                    $$
                                    $$
              R\left(-(\dot\varphi)^2\cos\varphi-
              {\buildrel ..\over\varphi}\sin\varphi\right)\e_x+
                           R\left(-(\dot\varphi)^2\sin\varphi+
              {\buildrel ..\over\varphi}\cos\varphi\right)\e_y+
              {\buildrel ..\over h}\e_z=
               {\buildrel ..\over\varphi}R\e_y+{\buildrel ..\over h}\e_z-
                     (\dot\varphi)^2 R\e_x
                                    $$
at the point $\pt=(R,0,0)$ where $\cos \varphi=0, \sin \varphi=1$. We see that
\begin{equation}\label{decompoitionofcylinderaccelerationatthegivenpoint}
               \ac=\underbrace{
                  {\buildrel ..\over\varphi}R\e_y+{\buildrel ..\over h}\e_z}_
                  {\hbox{\footnotesize tangent to the surface}}-\underbrace{
                     (\dot\varphi)^2 R\e_x}_
                  {\hbox{\footnotesize normal to the surface}}
      \end{equation}
 We see that   $\ac_n=(\dot\varphi)^2 R\e_x$.  Comparing with velocity vector
\eqref{velocityofcylinder} we see that
\begin{equation}\label{relationbetweenaccelerandvelocityoncylinder}
    \ac_n={\v_{horizontal}^2\over R}\n
\end{equation}


We see that for any curve on the cylinder  $x^2+y^2=R^2$ the normal curvature ${(\ac_{n},\n)\over |\v|^2}$
(see \eqref{normalcurvaturedef}) is equal to
             \begin{equation}\label{normalcurvature}
                {(\ac_{n},\n)\over |\v|^2}=
                {R\dot\varphi^2\over R^2\dot\varphi^2+\dot h^2}
             \end{equation}
and it obeys relations
              $$
            0\leq \kappa_{normal}\leq {1\over R}
              $$
depending of the curve. E.g. if the curve on the cylinder is a straight line
$x=x_0,y=y_0,z=t$ then $\ac=0$ and
normal curvature of this curve is equal to the naught as well as  usual curvature.

If the curve is circle $x=R\cos t,y=R\sin t, z=z_0$ then normal curvature of this curve
as well as  usual curvature is equal to ${1\over R}$.

\m

{\bf Remark} Very important  conclusion from this example is

\smallskip

\noindent {\it normal curvature of the cylinder of the radius $R$
takes values in the interval $\left(0,{1\over R}\right)$. It
cannot be greater
than $1\over R$}

\smallskip

\noindent   Note that we can consider on cylinder very curly curve
of very big curvature. The normal curvature at the points of this curve will be still less than $1\over R$.

\m

At any point of the surface normal curvature in general depends on the curve but it takes values
 in the restricted  interval.

E.g. for the sphere of radius $R$ one can see that normal curvature at any point is equal
to $1\over R$ independent of curve. In spite of this fact the usual curvature of curve can be very big
\footnote
{It is the geodesic curvature of the curves which characterises its curvature with respect to the curve.
The relation between usual geodesic and normal curvature is given by the formula \eqref{geodcurvature2}.}.
If we consider the circle of very small radius $r$ on the sphere then its usual curvature is equal to $k={1\over r}$
and $k\to \infty$ if $r\to 0$
So we see that one can define curvature of surface in terms of normal curvature.

}





{\footnotesize








\subsection {Concept of parallel transport of the vector tangent to the surface}

Parallel transport of the vectors is one of the fundamental concept
 of differential geometry.
Here we just give some preliminary ideas and formulate the concept of parallel transport
 for surfaces embedded in Euclidean space.


  Let $M$ be a surface $\r=\r(u,v)$ in $\E^3$ and $C\colon \r(t)=\r(u(t),v(t))$, $t_1\leq t\leq t_2$
  be a curve on this surface.

  Let $\X_1$ be a vector tangent to the surface at the initial point $p=\r(t_1)$
  of the curve $\r(t)$
  on the surface: $\X_1\in T_pM$. Note that $\X_1$ is a vector tangent to the surface, not necessarily to the curve.
    We define now parallel  transport of the vector along the curve  C:


  {\bf Definition} Let $\X(t)$ be a family of vectors depending on the parameter $t$ $(t_1\leq t\leq t_2)$
  such that following conditions hold

  \begin {itemize}
\item  For every  $t\in [t_1,t_2]$ vector $\X(t)$ is a vector tangent to the surface $M$
at  the point
 $\r(t)=\r(u(t),v(t))$ of the curve $C$.

\item    $\X(t)=\X_1$ for $t=t_1$


\item   ${d\X(t)\over dt}$ is orthogonal to the surface, i.e.
                       \begin{equation}\label{paralleltransportequation}
    {d\X(t)\over dt}\, \hbox {is collinear to the normal vector $\n(t)$},\,\,
   {d\X(t)\over dt}=\lambda(t)\n(t)
\end{equation}
Recall that normal vector $\n(t)$ is a vector attached to the point $\r(t)$ of the curve $C\colon \r(t)$.
 This vector is
orthogonal to the surface  $M$.

 The condition \eqref{paralleltransportequation} means that only orthogonal component of
  vector field $\X(t)$ can  be changed.
  \end {itemize}
  We say that a family $\X(t)$ is a parallel transport of the vector $\X_1$
  along a curve $C\colon \r(t)$ on the surface $M$. The final vector $\X_2=\X(t_2)$
  is the image of the vector $\X_1$ under the parallel transport along the curve $C$.

\smallskip


 Using the relation  \eqref{paralleltransportequation}
 it is easy to see that the scalar product of two vectors remains invariant under parallel transport.
 In particularly it means that  length of the vector does not change. If $\X(t)$, $\Y(t)$ are parallel transports
 of vectors $\X_1,\Y_1$ then
 $$
 {d\over dt}\left(\X(t),\Y(t)\right)=
 \left({d\X(t)\over dt},\Y(t)\right)+
    \left(\X(t),{d\Y(t)\over dt}\right)=0
    $$
    because vector ${d\X(t)\over dt}$ is orthogonal
    to the vector  $\Y(t)$ and  vector ${d\Y(t)\over dt}$  is orthogonal
    to the vector  $\X(t)$. In particularly length does not change:
                 \begin{equation}\label{lengthofthevectordoesnotchange2}
    {d\over dt}|\X(t)|^2={d\over dt}(\X(t),\X(t))=2\left({d\X(t)\over dt},\X(t)\right)=2(\lambda(t)\n(t),\X(t))=0
\end{equation}


  {\bf Remark} The relation \eqref{paralleltransportequation} shows how the surface
  is engaged in the parallel transport. Note that it is non-sense
   to put the right hand side of the equation \eqref{paralleltransportequation}
   equal to zero: In general a tangent vector ceased to be tangent to the surface if it is not changed!
   (E.g. consider the vector which transports along the great circle on the sphere)


  {\bf Example}

    In the case if surface is a plane then everything is easy.
   If vector $\X_1$ is tangent to the plane at the given point, it is tangent at all the points.
   Vector does not change under parallel transport $\X(t)\equiv \X$, ${d\X(t)\over dt}=0$.

{\bf Example}

   Consider the vector $\e_x={\p\over \p x}$ attached at the point $(0,0,R)$.
   It is tangent vector to the sphere $x^2+y^2+z^2=R^2$ at the North Pole.
  Define parallel transport of this vector along the meridian $\varphi=0,\theta=t$:
    $\r(t)\colon x=R\sin t, y=0,z=R\cos t$.

   Consider the vector field $\X(t)=\begin{pmatrix}\cos t\cr 0\cr -\sin t\cr\end{pmatrix}$
   attached at the point $\r(t)$ of the meridian. One can see that
   $\X(t)\vert_{t=0}=\begin{pmatrix}1\cr 0\cr 0\cr\end{pmatrix}$  is the initial vector
   attached at the North pole and
   $$
   {d\X(t)\over dt}={d\over dt}\begin{pmatrix}\cos t\cr 0\cr -\sin t\cr\end{pmatrix}=
   \begin{pmatrix}-\sin t\cr 0\cr -\cos t\cr\end{pmatrix}=-{\r(t)\over R}
   $$
is orthogonal to the surface of the sphere.  Hence $\X(t)$ is the parallel transport of the initial
vector along the meridian on the sphere.
   \m


  {\footnotesize We consider other more sophisticated examples of parallel transport of vectors
  along curves on surfaces
   in next sections.








 \subsection{Parallel transport of vectors tangent to the sphere.}


    1.   Consider now in  a more detail the parallel transport along curves on sphere.


    In the case if surface is a plane then everything is easy.
   If vector $\X_1$ is tangent to the plane at the given point, it is tangent at all the points.
   Vector does not change under parallel transport $\X(t)\equiv \X$.

   Consider a case of parallel transport along curves on the sphere.
   \m

     Consider on the sphere $x^2+y^2+z^2=a^2$ ($a$ is a radius) tangent vectors:
         \begin{equation}\label{c3}
  \r_\theta=\begin{pmatrix}
        a\cos\theta\cos\varphi\\
        a\cos\theta\sin\varphi\\
        -a\sin\theta\\
   \end{pmatrix}
\quad
  \r_\varphi=\begin{pmatrix}
        -a\sin\theta\sin\varphi\\
        a\sin\theta\cos\varphi\\
          0\\
   \end{pmatrix}
 \end{equation}
attached at the point   $\r(\theta,\varphi)=\begin{pmatrix}
        a\sin\theta\cos\varphi\\
        a\sin\theta\sin\varphi\\
        a\cos\theta\\
   \end{pmatrix}$.
   One can see that
             $$
     (\r_\theta,\r_\theta)=a,\quad
     (\r_h,\r_\varphi)=0,\quad
     (\r_\varphi,\r_\varphi)=a^2\sin^2\theta
            $$
It is convenient to introduce vectors which are parallel to these vectors but have unit length:
              \begin{equation}\label{unitvectorsonthesphere}
    \e_\theta={\r_\theta\over a},\quad
 \e_\varphi={\r_\varphi\over a\sin\theta}\qquad (\e_\theta,\e_\theta)=1,
 (\e_\theta,\e_\varphi)=0,(\e_\varphi,\e_\varphi)=1\,.
\end{equation}
         How these vectors change if we move along parallel (i.e. what is the value of ${\p \e_\theta\over \p\varphi}$,
         ${\p \e_\varphi\over \p\varphi}$);
         how these vectors change if we move along meridians
         (i.e. what is the value of ${\p \e_\theta\over \p\theta}$,
         ${\p \e_\varphi\over \p\theta}$). First of all recall that unit normal vector to the sphere
           at the point $\theta,\varphi$ is equal to $\r(\theta,\varphi)\over a$:
                      $$
                      \n(\theta,\varphi)=
                      \begin{pmatrix}
        \sin\theta\cos\varphi\\
        \sin\theta\sin\varphi\\
        \cos\theta\\
   \end{pmatrix}
                      $$
 Now calculate:
                     \begin{equation}\label{derivationofnomrlavectorstosphere}
    {\p\e_\theta\over \p \theta}=
     {\p\over \p\theta}
             \begin{pmatrix}
        \cos\theta\cos\varphi\\
        \cos\theta\sin\varphi\\
        -\sin\theta\\
             \end{pmatrix}=
             \begin{pmatrix}
        -\sin\theta\cos\varphi\\
        -\sin\theta\sin\varphi\\
        -\cos\theta\\
             \end{pmatrix}=-\n
\end{equation},
                     \begin{equation}\label{derivationofnomrlavectorstosphere2}
    {\p\e_\theta\over \p \varphi}=
     {\p\over \p\varphi}
             \begin{pmatrix}
        \cos\theta\cos\varphi\\
        \cos\theta\sin\varphi\\
        -\sin\theta\\
             \end{pmatrix}=
             \begin{pmatrix}
        -\cos\theta\sin\varphi\\
        \cos\theta\cos\varphi\\
           0\\
             \end{pmatrix}=\cos\theta\e_\varphi,
\end{equation},
   \begin{equation}\label{derivationofnomrlavectorstosphere3}
    {\p\e_\varphi\over \p \theta}=
     {\p\over \p\theta}
             \begin{pmatrix}
               -\sin\varphi\\
                  \cos\varphi\\
                       0\\
             \end{pmatrix}=
             \begin{pmatrix}
        -\cos\theta\sin\varphi\\
        \cos\theta\cos\varphi\\
           0\\
             \end{pmatrix}=0,
             \end{equation}

\begin{equation}\label{derivationofnomrlavectorstosphere4}
    {\p\e_\varphi\over \p \varphi}=
     {\p\over \p\varphi}
             \begin{pmatrix}
               -\sin\varphi\\
                  \cos\varphi\\
                       0\\
             \end{pmatrix}=
             \begin{pmatrix}
             -\cos\varphi\\
             -\sin\varphi\\
           0\\
             \end{pmatrix}=-\sin\theta\n-\cos\theta\e_\theta,
             \end{equation}

   Some of these formulaes are intuitively evident: For example formula
    \eqref{derivationofnomrlavectorstosphere} which means that family
    of the vectors $\e_\theta(\theta)$ is just parallel transport along meridian, because
     its derivation is equal to $-\n$.

       Another intuitively evident example: consider the meridian $\theta(t)=t$, $\varphi(t)=\varphi_0$,
       $0\leq t\leq \pi$.
       It is easy to see that the vector field
                 $$
           \X(t)=\e_\theta(\theta(t),\varphi_0)=
           \begin{pmatrix}
        \cos\theta(t)\cos\varphi_0\\
        \cos\theta(t)\sin\varphi_0\\
        -\sin\theta (t)\\
             \end{pmatrix}
                 $$
   attached at the point $(\theta(t),\varphi_0)$
is a parallel transport because for family of vectors $\X(t)$
all the conditions of parallel transport are satisfied.
In particular according to \eqref{derivationofnomrlavectorstosphere}
                $$
                {d\X(t)\over dt}= {d\theta(t)\over dt}{\p\over \p\theta}
             \begin{pmatrix}
        \cos\theta\cos\varphi\\
        \cos\theta\sin\varphi\\
        -\sin\theta\\
             \end{pmatrix}=-\n(\theta(t),\varphi_0)
                $$

   Now consider an example which is intuitively not-evident.

     {\bf Example.} Calculate parallel transport of the vector $\e_\varphi$ along the parallel.
     On the sphere of the radius $a$ consider the parallel
                \begin{equation}\label{londonparallel}
           \theta(t)=\theta_0, \varphi(t)=t, \quad 0\leq t\leq 2\pi
\end{equation}
 In cartesian coordinates equation of parallel will be:
\begin{equation}
\r(t)=
  \begin{pmatrix}
    a\sin \theta(t)\cos \varphi(t) \\
    a\sin \theta(t)\sin \varphi(t) \\
    -a\cos\theta(t)\\
  \end{pmatrix}=
  \begin{pmatrix}
    a\sin \theta_0\cos t \\
    a\sin \theta(t)\sin t \\
    -a\cos\theta_0\\
  \end{pmatrix},\quad 0\leq t\leq 2\pi
\end{equation}
It is easy to see that the family of the vectors $\e_\varphi(\theta_0,\varphi(t))$ on parallel,
is not parallel transport! because ${d\e_\varphi(\theta_0,\varphi(t))\over dt}=
{d\e_\varphi(\theta_0,\varphi)\over d\varphi}$ is not equal to zero
(see \eqref{derivationofnomrlavectorstosphere4}
above). Let a family of vectors $\X(t)$ be a parallel transport of the vector $\e_\varphi$
along the parallel  \eqref{londonparallel}: $\X(t)=a(t)\e_\theta(t)+b(t)\e_\varphi(t)$
where $a(t),b(t)$ are components of the tangent vector $\X(t)$ with respect to the basis $\e_\theta,\e_\varphi$
at the point $\theta=\theta_0,\varphi=t$ on the sphere.
Initial conditions for coefficients are  $a(t)\vert_{t=0}=0,b(t)\vert_{t=0}=1$
According to the definition of parallel transport and formulae \eqref{derivationofnomrlavectorstosphere}---\eqref{derivationofnomrlavectorstosphere4}
we have:
                   $$
{d\X(t)\over dt}={d\left(a(t)\e_\theta(t)+b(t)\e_\varphi(t)\right)\over dt}=
  \left({da(t)\over dt}\right)\e_\theta+a(t)\cos\theta_0\e_\varphi+{db(t)\over dt}\e_\varphi+
             $$
             $$
             b(t)\left(-\sin\theta_0\n-\cos\theta\e_\theta\right)=
             $$
\begin{equation}\label{derlondon}
=\left({da(t)\over dt}-b(t)\cos \theta_0\right)\e_\theta+
\left({db(t)\over dt}+a(t)\cos \theta_0\right)\e_\varphi-b(t)\sin\theta_0\n
\end{equation}
 Under parallel transport only orthogonal component of the vector changes. Hence we come to differential equations
           \begin{equation}\label{diffewquationforpartransport}
     \begin{cases}
       {da(t)\over dt}-wb(t)=0\\
       {db(t)\over dt}+wa(t)
     \end{cases}
       \quad a(0)=0, b(0)=0, w=\cos\theta_0
               \end{equation}
The solution of these equations is $a(t)=\sin wt,b(t)=\cos wt$. We come to the following answer:
parallel transport along parallel $\theta=\theta_0$ of the initial vector $\e_\varphi$
is the family
                         \begin{equation}\label{partransportworltrip}
     \X(t)=\sin wt\,\e_\theta+\cos wt\,\e_\varphi, w=\cos \theta_0
\end{equation}
During traveling along the parallel $\theta=\theta_0$ the $\e_\theta$ component becomes non-zero
   At the end of the traveling the initial vector $\X(t)\vert_{t=0}=\e_\varphi$ becomes
    $\X(t)\vert_{t=2\pi}=\sin 2\pi w\e_\theta+\cos2\pi w\e_\varphi$:
   {\bf the vector $\e_\varphi$ after woldtrip traveling along the parallel $\theta=\theta_0$
   transforms to the vector
$\sin (2\pi \cos\theta_0)\e_\theta+\cos (2\pi \cos\theta_0) \e_\varphi$.
In particularly this means that the vector $\bf e_\varphi$ after parallel transport will rotate on the angle}
               $$
            \hbox{angle of rotation}=2\pi \cos \theta_0
               $$
Compare the angle of rotation with the area of the segment of the sphere above the parallel
    $\theta=\theta_0$. According to the formula \eqref{areaoftheseqment}
    area of this segment is equal to $S=2\pi a h=2\pi a^2(1-\cos \theta_0)$. On the other hand
    Gaussian curvature of the sphere is equal to ${1\over a^2}$. Hence we see that up to the sign
    angle of rotation is equal to area of the seqment divided on the Gaussian curvature:
            \begin{equation}\label{angleofortationonsphere}
    \Delta \varphi=\pm {}{S\over K}=\pm2\pi \cos\theta_0
\end{equation}





\subsection
 {Parallel transport along a closed curve on arbitrary surface.}

The formula above for the parallel transport along parallel on the sphere keeps in the general case.

 {\bf Theorem} Let $M$  be a surface in $\E^3$.
   Let $\r(t)\colon \r(t), t_1\leq t\leq t_2, \r(t_1)=\r(t_2)$
 be a closed curve on the surface $M$ such that it is a boundary of domain $D$ of the surface $M$.
   (We suppose that the domain $D$ is bounded an orientate.)
    Let $\X(t)$
   be a parallel transport of the arbitrary tangent vector along this closed curve.
   Consider initial and final vectors $\X(t_1), \X(t_2)$. They have the same length
   according to \eqref{lengthofthevectordoesnotchange2}.


    {\bf Theorem }{\it The angle $\Delta\varphi$ between these vectors
    is equal to the integral of Gaussian curvature over the domain $D$:
       \begin{equation}\label{theoremonparalleltransport}
          \Delta\varphi=\pm \int_D K d\sigma
\end{equation}
where we denote by $d\sigma$ the element of the area of surface of $M$.}

\bigskip

The calculations above for traveling along the parallel are just example of this Theorem.
The integral of Gaussian curvature over the domain above parallel $\theta=\theta_0$ is equal
to $K\cdot 2\pi a(1-\cos \theta_0)$=${1\over a^2}\cdot 2\pi a^2(1-\cos\theta_0)=2\pi(1-\cos\theta_0)$.
This is equal to the angle of rotation $2\pi \cos\theta_0$ (up to a sign and modulo $2\pi$).
Another simple

 {\bf Example}. Consider on the sphere $x^2+y^2+z^2=a^2$ points $A=(0,0,1)$, $B=(1,0,0)$ and $C=(0,1,0)$.
 Consider arcs of great circles which connect these points. Consider the vector
 ${\e}_x$ attached at the point $A$. This vector is tangent to the sphere. It is easy to see
 that under parallel transport along the arc $AB$ it will transform at the point $B$
 to the vector $-\e_z$. The vector $-\e_z$  under parallel transport along the arc $BC$
 will remain the same vector $-\e_z$. And finally under parallel transport along the arc $CA$
 the vector $-\e_z$ will transform at the point $A$
 to the vector $-\e_y$. We see that under traveling along the curvilinear triangle $ABC$
 vector $\e_x$ becomes the vector $-\e_y$, i.e. it rotates on the angle ${\pi\over 2}$.
It is just  the integral of the curvature ${1\over a^2}$ over the triangle $ABC$:
  $K\cdot S$ $={1\over a^2}\cdot {4\pi a^2\over 8}={\pi\over 2}$.



  We know that for planar triangles sum of the angles is equal to $\pi$. It turns out that


{\bf Corollary} Let $ABC$ be a triangle on the surface formed by geodesics. Then
         \begin{equation}\label{defectoftriange}
            \angle A+\angle B+\angle C=\pi+\int_{\triangle ABC}Kds
         \end{equation}

The Gaussian curvature measures the difference of $\pi$ and  sum of angles.

The corollary evidently follows form the Theorem. It is of great importance:
 It gives us tool to measure curvature. (See the tale about ant.)




  \subsection {Gauss Bonnet Theorem}



  Consider the integral of curvature over whole closed surface $M$. According to the Theorem
  above the answer has to be equal to $0$ (modulo $2\pi$), i.e. $2\pi N$ where $N$ is an integer,
  because this integral is a limit when we consider very small curve. We come to the formula:
         $$
         \int_D Kd\sigma=2\pi N
         $$
(Compare this formula with formula \eqref{gaussbonetforn=1}).


  What is the value of integer $N$?


We present now one remarkable Theorem which answers this question and prove this Theorem using the
formula \eqref{defectoftriange}.


Let $M$ be a closed orientable surface.\footnote{Closed means compact surface without boundaries.
Intuitively orientability means that one can define out and inner side of the surface.
  In terms of normal vectors
orientability means that
one can define the continuous field of normal vectors at all the points of $M$.
The direction of normal vectors at any point defines outward direction.
Orientable surface is called oriented if the direction of normal vector is chosen.}
All these surfaces can be classified up to a diffeomorphism.
Namely arbitrary  closed oriented surface $M$
   is diffeomorphic either to  sphere (zero holes),
   or torus (one hole), or pretzel (two holes),...
"Number k" of holes is intuitively evident characteristic of the surface.
It is related with very important characteristic---Euler characteristic
$\chi(M)$ by the following formula:
\begin{equation}\label{defofeuler00}
  \chi(M)=2(1-g(M)),\quad \hbox {where $g$ is number of holes}
\end{equation}


{\bf Remark} What we have called here "holes" in a surface is often referred
to as "handles" attached o the sphere, so that the sphere itself does not have any handles,
the torus has one handle, the pretzel has two handles and so on. The number of handles is also called genus.

\smallskip

Euler characteristic appears in many different way. The simplest appearance is the following:

Consider on the surface $M$  an arbitrary  set of points (vertices) connected
with edges (graph on the surface) such that surface is divided on
 polygons with (curvilinear sides)---plaquets. ("Map of world")

Denote by $P$ number of plaquets (countries of the map)

Denote by $E$ number of edges (boundaries between countries)

Denote by $V$ number of vertices.

Then it turns out that
\begin{equation}\label{defofeulerchar100}
  P-E+V=\chi(M)
\end{equation}
It does not depend on the graph, it depends only on how much holes has surface.

E.g. for every graph on $M$, $P-E+V=2$ if $M$ is diffeomorphic to sphere.
For every graph on $M$ $P-E+V=0$ if $M$ is diffeomorphic to torus.


\bigskip


Now we formulate Gau\ss\,-Bonnet Theorem.


Let $M$ be closed oriented surface in $\E^3$.

Let $K(p)$ be Gaussian curvature at any point $p$ of this surface.

Recall that sign of Gaussian curvature does not depend on the orientation.
If we change direction of normal vector $\n\to-\n$ then both principal curvatures
change the sign and Gaussian curvature $K=\det A/\det G$ does not change the sign
\footnote{For an arbitrary point $p$ of the surface $M$ one can always choose
cartesian coordinates $(x,y,z)$ such that surface in a vicinity of this spoint is defined by the
equation $z=ax^2+bx^2+\dots$, where dots means terms of the order higher than 2.
Then Gaussian curvature at this point will be equal to $ab$. If $a,b$ have the same sign
then a surfaces looks as paraboloid in the vicinity of the point $p$. If
If $a,b$ have different signs
then a surfaces looks as saddle in the vicinity of the point $p$.
Gaussian curvature is positive if $ab>0$ (case of paraboloid) and
negative if $ab<0$ saddle  }.



\medskip

{\bf  Theorem} (Gau\ss\,\,-Bonnet)
The integral of Gaussian curvature over the
closed compact oriented surface  $M$ is equal to $2\pi$ multiplied
by the Euler characteristic of the surface $M$
\begin{equation}\label{gaussbonnet}
  {1\over 2\pi}\int_M Kd\sigma=\chi(M)=2(1-\hbox{number of holes})
\end{equation}

In particular for the surface $M$ diffeomorphic to the sphere   $\kappa(M)=2$,
for the surface diffeomorphic to the torus it is equal to $0$.

\smallskip

The value of the integral does not change under continuous deformations of
surface! It is integer number (up to the factor $\pi$) which characterises
topology of the surface.

E.g. consider surface $M$ which is diffeomorphic to the sphere.
If it is sphere of the radius $R$ then curvature is equal to $1\over R^2$,
 area of the sphere is equal to $4\pi R^2$ and left hand side
 is equal to ${4\pi \over 2\pi}=2$.

If surface $M$ is an arbitrary surface diffeomorphic to $M$
then  metrics and curvature depend from point to the point,
Gau\ss\,-Bonnet states that integral nevertheless remains unchanged.

\smallskip

Very simple but impressive corollary:

{\it Let $M$ be surface diffeomorphic to sphere in $\E^3$. Then
there exists at least one point where Gaussian curvature is positive.}

Proof: Suppose it is not right. Then $\int_M K\sqrt {\det g}dudv\leq 0$.
On the other hand according to the Theorem it is equal to $4\pi$. Contradiction.



In the first section in the subsection "Integrals of curvature along the plane curve"
we proved that the integral of curvature over closed convex curve is equal to $2\pi$.
This Theorem seems to be "ancestor" of Gau\ss-Bonnet Theorem\footnote {Note that
there is a following deep difference: Gaussian curvature is internal property of the surface:
it does not depend on isometries of surface. Curvature of curve depends on the position
of the curve in ambient space.}.




\bigskip

 {\it Proof of Gau\ss-Bonet Theorem}

 Consider triangulation of the surface $M$. Suppose $M$ is covered by $N$ triangles. Then number
 of edges will be $3N/over 2$. If $V$ number of vertices then according to Euler Theorem
                $$
        N-{3N\over 2}+V=V-{N\over 2}=\chi(M).
                $$
Calculate the sum of the angles of all triangles.
On the one hand it is equal to $2\pi V$. On the other hand
according the formula \eqref{defectoftriange} it is equal to
           $$
   \sum_{i=1}^N\left(\pi+\int_{\triangle_i}K d\sigma\right)=
   \pi N+\sum_{i=1}^N\left(\int_{\triangle_i}K d\sigma\right)=
   N\pi+\int_M Kd\sigma
           $$
We see that $2\pi V=N\pi+\int_M Kd\sigma$, i.e.
            $$
         \int_M Kd\sigma=\pi \left(2V-{N\over 2}\right)=2\pi \chi(M)\hbox{\finish}
            $$

%\end{document}

\subsection { A Tale on Differential Geometry}

  \bigskip



\def \he {He (She)$\,$}

   Once upon a time there was  an ant living on a sphere of radius $R$.
    One day
   he  asked himself some questions:
   What is the structure of the Universe (surface) where he
   lives?
   Is it a sphere? Is it a torus? Or may be something more
   sophisticated, e.g. pretzel (a surface with two holes)


  Three-dimensional human beings do not need to be mathematicians
  to distinguish between a sphere torus or pretzel.  They just have to look on the surface.
  But the ant living on
  two-dimensional surface cannot fly. He cannot look on the surface
   from outside. How can he judge about what surface he lives on
  \footnote{$^*$}{This is not very far from reality: For us human beings
   it is impossible to have a global look on three-dimensional  manifold.
 We need to develop local methods to understand global properties
 of our Universe. {\it Differential Geometry} allows to study
  global properties of manifold with local tools.}?

  Our ant loved mathematics and in particular {\it Differential Geometry}. He liked to draw various triangles,
   calculate their angles
  $\alpha,\beta,\gamma$, area
  $S(\Delta)$.
   He knew from geometry books that the sum of the angles of
   a triangle equals  $\pi$,
    but for triangles which he drew it was not right!!!!

       Finally he understood that the following formula is true:
        For every triangle
               $$
     {\left(\alpha+\beta+\gamma-\pi\right)\over S(\Delta)}=c
     \eqno (1)
               $$
  A constant in the right hand side depended neither on size of triangle
   nor the triangles location.  After hard research he came to conclusion
   that its Universe can be considered as a sphere embedded in three-dimensional
    Euclidean space  and a constant $c$ is related with
    radius of this sphere by the relation
                 $$
                 c={1\over R^2}
                 \eqno (2)
                 $$
  ...Centuries passed. Men have deformed the sphere of our old ant.
    They smashed it. It seized to be round,
  but the ant civilisation survived. Moreover
 old books survived. New ant mathematicians try to understand the
 structure of their Universe.
 They see
 that formula (1) of the Ancient Ant mathematician is not true.
 For triangles at different places  the right hand side
 of the formula above is different. Why? If ants could fly and look
 on the surface from the cosmos they could see how much the sphere has been
 damaged by humans beings,
 how much it has been deformed, But the ants cannot fly. On the other hand
 they adore mathematics and in particular
 {\it Differential Geometry}. One day considering for every point very small triangles they
introduce
 so called curvature for every point $P$ as a limit of right hand
 side of the formula (1) for small triangles:
                     $$
 K(P)=\lim_{S(\Delta)\to 0}{\left(\alpha+\beta+\gamma-\pi\right)\over S(\Delta)}
                     $$
Ants realise that curvature which can be calculated in every point
gives a way to decide where they live on sphere, torus, pretzel...
They  come to following formula \footnote {In human
civilisation this formula is called Gau\ss $\,$-Bonet formula. The
right hand side of this formula is called Euler characteristics of
the surface.} : integral of curvature over the whole Universe (the
sphere) has to equal  $4\pi $,  for torus it must equal  $0$, for
pretzel it equalts $-4\pi$...
             $$
{1\over 2\pi}\int K(P)dP=2\left(1-\hbox{number of holes}\right)
             $$




}



\end{document}


 \subsubsection {Gramm matrix, Gramm determinant}
   This inequality is related with the following construction.
   Let $\{\ac_1,\dots,\ac_m\}$ be $m$ vectors in Euclidean vector space $\E^n$ (where $m,n$ in general are two different positive integers.  Consider so called Gramm matrix (Grammian) of these vectors
        $$
        ||G_{ik}||\colon \quad G_{ik}=(\ac_i,\ac_k)
        $$
     Let $a_i^k$ is a matrix of components of vectors $\{\ac_i\}$ in an orthonormal basis $\{\e_i\}$:
             $$
           \ac_i=\sum_{k=1}^na_i^k\e_k
             $$
   Then the following very important identity takes place
                 \begin{equation}\label{grammindentity}
            \det G=(\det A)^2\,.
                 \end{equation}
         Proof is easy. We have
                 $$
  (\ac_i,\ac_j)=\left(
     \sum_{k=1}^n a_i^k\e_k,
     \sum_{k'=1}^na_j^{k'}\e_{k'}
             \right)=\left(AA^T\right)_{ij} \Rightarrow \det G=\det \left(AA^T\right)=(\det A)^2
                 $$
   Corollary 1.
         Vectors $\{\ac_1,\dots,\ac_m\}$ are linear independent if and only if $\det G>0$..

   \m

   Corollary 2. Take $m=2$. We come to CBS inequality:
               $$
            \det G=(\ac, \ac)(\b,\b)-(\ac,b)^2\geq 0\,.
               $$


\end{document}
